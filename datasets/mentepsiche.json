[
{"title": "Farmaci, sperimentazioni ed incidenti mortali", "text": "Da due giorni a questa parte - da quando cinque volontari sani sottoposti in Francia a uno studio farmacologico di fase 1 sono stati ricoverati con gravi lesioni neurologiche – mi arrivano tramite i social media tante domande: di che farmaco si tratta, come possono accadere incidenti simili e come è possibile prevenirli (se è possibile). Provo a dare qui in pubblico le risposte sulla base delle informazioni finora disponibili, perché magari sono utili anche a chi vuole capire qualcosa di più di quanto è accaduto e non limitarsi a gridare allo scandalo o al complotto di Big Pharma.  Per rispondere mi tocca spiegare, seppure per sommi capi, come si studia un nuovo farmaco. Chi già conosce la ricerca farmacologica può passare alla domanda successiva. Una volta scoperto che un certo disturbo dipende dal funzionamento di determinate vie metaboliche, chi fa ricerca sui farmaci cerca di identificare una sostanza in grado di interferire con un passaggio chiave del processo. Può farlo attraverso banche dati di molecole già disponibili o attraverso un sistema più complesso chiamato drug design, che permette di creare al computer molecole virtuali che poi vengono sintetizzate in laboratorio e testate. In questo modo si cerca di agire sui sintomi o, meglio, direttamente sulla causa di una malattia. La sostanza viene valutata dal punto di vista tossicologico sulla base di informazioni già disponibili: vi sono programmi appositi che fanno un primo screening e che sanno che determinate combinazioni o gruppi chimici possono dare effetti collaterali o essere tossici per organi o tessuti. Le sostanze identificate come potenzialmente tossiche vengono eliminate già in questa fase “virtuale”. Quelle che passano l’esame vengono sintetizzate e testate prima in vitro (per esempio su colture cellulari) poi nel modello animale più adeguato. L’animale è tappa obbligata della ricerca perché una sostanza potrebbe agire in un modo sul tessuto isolato e in un altro in un organismo complesso, magari perché viene metabolizzato dal fegato e dà origine ad altre molecole che sono tossiche, anche se la molecola da cui provengono non lo è. A volte è disponibile un modello animale della malattia o del sintomo che si vuole curare, e in quel caso questa fase fornisce anche informazioni sulla potenziale efficacia come cura. Solo dopo i test sugli animali  si somministra il farmaco a volontari sani (con l’eccezione di alcune molecole che sono tossiche per loro natura, come i chemioterapici che, per evidenti ragioni, vengono testati fin dall'inizio sui malati). È questa la fase 1 in cui sono stati arruolati anche i volontari francesi, il cui scopo non è di valutare l’efficacia della cura ma la velocità con cui la molecola viene metabolizzata dall'organismo (la cosiddetta emivita) e la dose massima oltre la quale compaiono effetti collaterali. In genere la fase 1 coinvolge poche decine di persone. Se tutto va bene, il farmaco viene somministrato ai malati, per valutarne l’efficacia: prima a piccoli gruppi (fase 2) poi, se non compaiono problemi, anche a gruppi più numerosi (fase 3). Un farmaco impiega circa 10 anni per arrivare dal laboratorio al letto del malato e viene tenuto sotto sorveglianza anche per cinque anni dopo la sua commercializzazione, perché effetti collaterali rari possono emergere solo dopo che molte persone ne hanno fatto uso. Questa è la fase di farmacovigilanza (che in realtà non cessa mai, perché i medici devono segnalare gli effetti collaterali inattesi ogni volta che ne incontrano uno). Vi sono stati diversi casi di farmaci ritirati dal commercio in fase di farmacovigilanza per la comparsa di disturbi in chi li usava: non si tratta di fallimenti della ricerca ma di una inevitabile conseguenza della rarità di certi effetti collaterali, a cui la farmacovigilanza stessa cerca di ovviare. In fase 1 gli effetti collaterali vengono messi in conto, sebbene lo studio preliminare accurato abbia lo scopo di ridurli al minimo, andando a vedere con attenzione su quali sistemi agisce la nuova cura. Gli imprevisti sono però sempre possibili e, secondo i dati disponibili, compaiono effetti collaterali gravi nello 0,02 per cento di tutte le sperimentazioni di fase 1 (percentuale che comprende anche eventi gravi accaduti durante la somministrazione ma non necessariamente legati agli effetti del farmaco). 2. Perché queste persone si sottopongono alle sperimentazioni? Partecipare a uno studio di fase 1 è un’attività che in alcuni Paesi (non in Italia) può essere retribuita. Il sito internet francese Breizh Info ha pubblicato in esclusiva la descrizione e il contratto dello studio che ha provocato l’incidente di Rennes: l’autenticità non è stata confermata dalla casa farmaceutica, ma gli esperti si sono pronunciati positivamente. Pare si tratti di una copia consegnata a un volontario che alla fine è stato scartato. La retribuzione per i volontari ammonta a 1900 euro per circa due settimane di ricovero, l’assunzione del farmaco a dosi elevate e l’esecuzione di una serie di esami clinici, tra cui ripetuti prelievi ematici. Anche altri Paesi europei accettano che questo impegno venga retribuito, seppure con tetti piuttosto bassi. La possibilità di retribuire i volontari fa discutere, perché rende la partecipazione alle sperimentazioni una forma di guadagno relativamente facile per persone giovani o senza lavoro, sollevando problemi etici. D’altronde anche in Italia, per incentivare la partecipazione, si ricorre a incentivi di tipo non economico (per esempio la concessione di crediti universitari agli studenti che accettano di sottoporsi a test o esperimenti): la scienza non può fare a meno dei volontari sani, e chiamarli cavie umane non rende del tutto giustizia al loro ruolo essenziale. Nel nostro Paese, comunque, il divieto di retribuire i volontari rende molto difficile il loro reperimento (con l’esclusione di alcune categorie come i malati terminali, per i quali un nuovo farmaco può significare una possibilità in più di vivere un po’ più a lungo), e di fatto gli studi farmacologici di fase 1 sono da noi rarissimi. 3. Perché solo questi cinque stanno male anche se il farmaco è stato assunto da circa 90 persone? La dinamica non è ancora del tutto chiara, ma sembra che il farmaco sia stato testato in dose singola in circa 90 persone, senza alcun effetto collaterale. Sei persone (le quattro che sono ricoverate con lesioni cerebrali da moderate a gravi, la persona in stato di morte cerebrale che, tra qualche ora, terminato il periodo di osservazione legale, verrà dichiarata ufficialmente morta e quella che per ora sta bene) hanno invece assunto più dosi, per vari giorni consecutivi. È possibile quindi che il danno (al momento sembra si tratti di emorragie cerebrali spontanee) sia l’effetto dell’accumulo della sostanza nell’organismo. 4. In questo farmaco c’è un derivato della cannabis. Anche fumare cannabis può provocare effetti simili? Nel farmaco testato, contrassegnato dalla sigla BIA 10-2474, non c’è alcun derivato della cannabis. Si tratta di un inibitore delle idrolasi delle ammidi degli acidi grassi (più note con la sigla FAAH), enzimi che nel nostro cervello degradano l’anandamide, un cannabinoide endogeno, ovvero una molecola che agisce sugli stessi recettori su cui agiscono le sostanze attive contenute nella cannabis e che ha la funzione di combattere naturalmente il dolore, l’ansia e l’inappetenza. Vi sono due diversi tipi di FAAH, l’1 e il 2. La maggior parte dei farmaci antidolorifici in corso di sperimentazione (quindi non solo quello testato in Francia) agiscono su FAAH1, con lo scopo di bloccarne l’azione, evitando così che gli endocannabinoidi vengano eliminati e aumentandone la concentrazione nel cervello. In pratica la speranza di chi sta testando questa categoria di sostanze è quella di utilizzare le molecole antidolorifiche che produciamo naturalmente per combattere i dolori che i farmaci attuali non sono in grado di lenire, in particolare il dolore cronico. In questo modo, tra l’altro, si hanno gli effetti positivi dei cannabinoidi senza avere quelli negativi (per esempio ottundimento e sonnolenza, come accade a chi fa fa ricorso alla cannabis terapeutica). BIA 10-2474 non è il primo inibitore di FAAH testato sull’uomo: la Pfizer ne ha provato uno, noto con la sigla PF – 0445784 su 77 volontari sani senza effetti collaterali, e su 74 pazienti con artrosi, senza effetti collaterali ma anche, purtroppo, senza efficacia contro il dolore. Gli inibitori di FAAH possono essere reversibili (cioè bloccare il recettore solo per un breve lasso di tempo) oppure irreversibili (cioè occuparlo a tempo indefinito). Sembra che il farmaco sotto accusa sia di tipo irreversibile, ma non ci sono conferme dalla casa farmaceutica, la portoghese Bial, né dall’azienda Biotrial incaricata di portare avanti la sperimentazione. Il meccanismo d’azione è quindi del tutto diverso da quella della cannabis e non c’è alcun rischio, per chi ne fa uso, di incorrere in effetti simili. È bene dire che altre molecole della stessa famiglia di BIA 10-2474 hanno superato senza danni la fase 1 (per esempio IW-6118 è in fase 2 per la cura del dolore dentale, SSR – 411298 ha completato uno studio di fase 2 nella depressione nel 2010 rivelandosi inefficace ma innocua, e ora è allo studio per il dolore da cancro). I ricercatori avevano quindi abbastanza precedenti su cui basarsi per somministrare una molecola analoga nell’uomo senza attendersi grossi guai. Che però sono accaduti. 5. Perché il farmaco ha avuto questo effetto inatteso? È difficile dirlo, dal momento che mancano informazioni sulla molecola. Le ipotesi più accreditate sono due: una contaminazione della produzione (ovvero qualcosa di tossico che contamina le compresse in fase di fabbricazione) oppure un’azione off-target, ovvero un effetto della molecola stessa su un bersaglio molecolare diverso da quello previsto. È questa l’ipotesi più accreditata secondo Sir Munir Pirmohamed, vice presidente della sezione clinica della British Pharmacological Society, interpellato dal Science Media Center di Londra. “ È possibile che il farmaco abbia reagito con un altro enzima, recettore od obiettivo all’interno dell’organismo” ha dichiarato. D’altronde non sarebbe la prima volta che un farmaco disegnato a tavolino per colpire un bersaglio ben preciso riesce a interagire con altri organi o tessuti, in modo del tutto imprevedibile. Una terza ipotesi, per ora esclusa dai medici francesi, è che vi sia stato un errore nella somministrazione e che queste persone abbiano assunto molto più farmaco del previsto. 6. Questo farmaco è stato testato su animali, persino su scimpanzé. Vuol dire che la sperimentazione animale è inutile e non affidabile? Il malaugurato incidente di Rennes dimostra esattamente l’opposto. Se casi del genere non accadono più spesso è proprio perché i farmaci vengono testati nel modello animale prima che nell’uomo. La molteplicità dei potenziali bersagli di una nuova sostanza non può emergere se non in un organismo completo, mentre non emerge certo né nei modelli simulati al computer né negli studi su singoli tessuti in vitro. In questo caso specifico sono stati utilizzati addirittura gli scimpanzé. Si tratta di un evento piuttosto raro in quanto la sperimentazione sui primati è severamente regolata e molto costosa. Non sappiamo ancora quali ragioni hanno spinto la casa farmaceutica a ricorrere a un modello tanto complesso invece di accontentarsi di topi o ratti, ma possiamo ipotizzare una spiegazione. I primati hanno una barriera ematoencefalica simile alla nostra (ovvero un sistema di protezione del cervello simile al nostro, che va superato se vogliamo che un farmaco raggiunga l’obiettivo). Inoltre è molto difficile studiare i farmaci per il dolore in animali come i topi, sebbene di norma si faccia, perché il dolore è una sensazione composita, che coinvolge anche strutture cerebrali molto evolute che non tutti gli animali utilizzano come noi. Perché lo studio sugli scimpanzé non sia bastato a evitare il problema, non è possibile dirlo ora: forse il dosaggio usato negli animali era più basso oppure il farmaco agisce in modo inatteso su una via molecolare peculiare, non del tutto sovrapponibile nell’uomo e nel primate."},
{"title": "Aerei, macchine e perdite di controllo", "text": "Nell’ultimo anno ho viaggiato molto, per ragioni familiari e per lavoro. Ho macinato una bella quantità di chilometri, salendo e scendendo dagli aerei. Non ho mai avuto paura di volare ma qualche volta, complice la stanchezza, ammetto di provare un filo di inquietudine. Ecco perché mi è cascato l’occhio su questo articolo del Sidney Morning Herald in cui si racconta la storia del volo QF72 della Qantas, le linee aeree australiane, i cui dettagli sono stati rivelati solo pochi giorni fa, quando il pilota ha lasciato il servizio e ha accettato di farsi intervistare. È l’ottobre del 2008, il volo trasporta 303 passeggeri e 12 membri di equipaggio da Singangapore a Perth quando uno dei computer di bordo impazzisce. All’improvviso l’aereo si butta in piacchiata, muso avanti, in direzione dell’oceano. Il pilota cerca di riprendere il controllo del mezzo ma negli aerei guidati da sistemi di intelligenza artificiale (come sono oramai quasi tutti i grandi aerei) esiste una gerarchia per cui il computer ha il sopravvento sull’uomo. La ragione è semplice: gli studi hanno dimostrato che, in situazioni di pericolo, i computer reagiscono più in fretta e più razionalmente degli esseri umani. Tranne quando impazziscono, come è accaduto su quel volo.  Come racconta Kevin Sullivan, il pilota che con un incredibile sangue freddo è riuscito a rimettere in asse l’aereo non appena il computer ha \"accettato\" di lasciargli il comando (il folle volo in picchiata si è ripetuto per ben due volte), l’incidente si è concluso miracolosamente con 11 feriti gravi, numerosi altri feriti più lievi e nessun morto. I periti che hanno esaminato la scatola nera hanno capito sche i sensori dell’aereo hanno cominciato a inviare al computer informazioni sbagliate sullo stato del velivolo: su quella base, il sistema di intelligenza artificiale ha preso decisioni che avrebbero potuto portare alla morte di quasi 320 persone. E lo ha fatto per due volte di seguito. Perché non si sia accorto dell’errore rimane ancora un mistero. \"La peggior cosa che ti può capitare, quando sei alla guida di un aereo, è non avere il controllo nelle tue mani\" dice Sullivan che, pur avendo ripreso a volare, da quel giorno non si è più sentio sicuro. La storia del volo QF72 - che ha portato a cambiare alcune delle regole per l’uso dell’intelligenza artificiale negli aerei (per esempio adesso i computer sono più di uno e prima di escludere il pilota dalle decisioni devono essere d’accordo tra loro) - incarna il difficile rapporto che abbiamo con le macchine pensanti. Perché la verità è che anche se i computer possono (raramente) \"impazzire\", gli uomini impazziscono molto più spesso o, più semplicemente, fanno più errori. La maggior parte di noi, però, si fida di più degli umani che delle macchine. E anche quando non ci fidiamo, preferiamo morire per mano di un nostro simile piuttosto che per una decisione presa da un computer. Lo sanno bene i costruttori di automobili autoguidate: una tecnologia pronta a entrare sul mercato ma che stenta a decollare perché tendiamo a fidarci poco. Eppure le auto con automazione parziale (ovvero dotate di sistemi di controllo come il bloccasterzo, il parcheggio autonomo o la frenata assistita che intervengono modificando le decisioni del conducente in caso di pericolo o di manovra azzardata) hanno già oggi ridotto, secondo i dati disponibili, il numero degli incidenti gravi. Anche le macchine totalmente automatizzate sembrano essere più sicure di quelle a cui siamo abituati: in un anno, i 48 esemplari immatricolati in California sono stati coinvolti solo in quattro lievissimi incidenti, senza danni alle persone e con lievi danni alle cose. A provocare questi piccoli scontri è stata, secondo gli esperti, l’imprevedibilità umana, dal momento che le auto robot circolano in un ambiente popolato da imperfetti guidatori in carne e ossa. Di cosa ci fidiamo di più? Il successo di una tecnologia dipende però dalla fiducia che la gente ripone nella stessa. Nel caso delle auto automatizzate bisognerà creare un livello di fiducia molto elevato, ancora maggiore di quello necessario all’auto classica. Un sondaggio condotto nel 2014 dall’Associazione dei fabbricanti di auto statunitensi su un campione di 2000 automobilisti ha dimostrato che se il 32 per cento degli intervistati si dichiara entusiasta e comprerebbe subito un’auto automatica, il 25 per cento dichiara che non si farebbe mai trasportare da un robot. Le donne sono più propense a cedere il controllo dei maschi. È però interessante notare che il 75 per cento degli interpellati dichiara di voler comunque avere il controllo del mezzo quando porta i figli a scuola, anche se le statistiche dicono che la guida automatica è più sicura. È questo tipo di pregiudizio cognitivo che gli psicologi dovranno aiutare a comprendere e a superare se davvero vogliono convincerci a comprare le auto che pensano da sole. Un problema che le compagnie aeree non hanno, perché quando saliamo su un velivolo non ci chiediamo mai chi abbia davvero in mano la cloche e se (e quando) il pilota può escludere il computer dalle decisioni (o viceversa, dato il timore di atti di terrorismo che utilizzino gli aerei o di piloti con tendenze suicide, come è accaduto almeno un paio di volte negli ultimi dieci anni). D’altro canto vi sono anche persone portate ad avere troppa fiducia nella tecnologia, come ha dimostrato Nicholas Ward, esperto di psicologia applicata all’ingegneria all’Università del Montana, che in uno studio condotto su 3000 automobilisti  ha scoperto che esiste una fetta di popolazione che tende a fidarsi eccessivamente delle macchine e dei computer di bordo, affidando loro anche compiti che in realtà non sarebbero in grado di fare, oppure prendendo il controllo della macchina in ritardo in caso di problemi. È quanto è accaduto anche sul volo della Qantas perché, racconta il pilota, gli operatori non sono addestrati a \"sfiduciare\" la macchina. Nella costruzione del rapporto con la macchina conta molto anche quanto questa viene umanizzata. Adam Waytz, uno psicologo della Northwestern University, ha iniziato a studiare questo fenomeno quando ha scoperto che i soldati celebravano i funerali dei robot militari andati distrutti in azioni di guerra. Quanto più avevano affidato la loro o l’altrui vita nelle mani della macchina, tanto più intenso era il sentimento di perdita in caso di incidente. Esistono addirittura “tombe” di robot sminatori, una delle macchine a maggior “rischio” di antropomorfizzazione perché addetta a fare del bene col rischio costante di saltare per aria. Più una macchina è dotata di attributi umani, come una voce, un sesso e un nome, maggiore è la tendenza delle persone ad attribuirle intelligenza e persino emozioni, e maggiore la tendenza a scusarla o ad attribuire ad altri la colpa in caso di incidente. Decisioni buone, ma per chi? Da un sistema intelligente ci aspettiamo anche che prenda per noi non solo le decisioni più giuste ma anche quelle più etiche. Questo può voler dire che, in caso di incidente, il veicolo automatico che ci sta trasportando decide di esporre proprio noi che viaggiamo da soli al maggior rischio di morte, se si accorge che l’altro veicolo trasporta più persone oppure dei bambini. Siamo davvero sicuri che, nell’era in cui acquistiamo le auto anche sulla base del numero di air bag e di sistemi di protezione che offre, siamo pronti a cedere a un computer la decisione su chi deve vivere e chi deve morire? Se lo sono chiesti un ingegnere informatico dell’Università di Standford, Chris Gerdes, e il suo collega filosofo ed eticista del Politecnico della California Patrick Lin, che hanno organizzato un workshop multidisciplinare per cercare di risolvere il dilemma, senza riuscirci. Di conseguenza hanno sviluppato insieme una serie di software in grado di indurre la macchina a utilizzare scenari etici diversi, da quello che evita gli esseri umani a costo di andare a sbattere sulle auto parcheggiate, a quello che agisce semplicemente per limitare il numero dei possibili morti e feriti, indipendentemente dal fatto che siano passeggeri dell’auto stessa o di un’altra auto, fino allo scenario “animalista” che evita persino di investire gli scoiattoli, molto frequenti sulle strade degli Stati Uniti. Esiste anche un software che «autorizza» la macchina a fare la manovra che teoricamente provoca meno morti o feriti tranne se sull’auto ci sono i nostri figli: in quel caso deve fare di tutto per salvarli. Comunque la si pensi, perdere il controllo del proprio mezzo è spaventoso. In questo anno di spostamenti mi è anche capitato un problema in autostrada, mentre viaggiavo con mio padre e le mie figlie di ritorno dal mare. Possedevo una di quelle macchine con il cambio automatico e tutte le funzioni gestite da un computer. Solo che questo, da un po’ di tempo, faceva le bizze, segnalando anomalie di ogni genere ai freni, alle sospensioni… Anomalie «fantasma», dovute a qualche strano inghippo del sistema di intelligenza artificiale sufficienti però a mandare in tilt i sistemi di controllo del cambio e di fronte alle quali i numerosi meccanici interpellati si limitavano ad alzare le braccia. Ed è successo anche quel pomeriggio sull’autostrada: all’improvviso la macchina non cambiava più le marce, non accelerava, ma soprattutto non mi permetteva di riprendere il controllo manuale. L’unica cosa che potevo fare era inchiodare, in mezzo a una autostrada trafficata dove ovviamente nessuno rispettava le distanze di sicurezza. Non so come sono riuscita a fermarmi in corsia di emergenza, tenere a bada il panico e chiamare un carro attrezzi. La settimana dopo abbiamo venduto l’auto computerizzata. Ora abbiamo una grossa, solida, confortevole auto familiare. La richiesta che ho fatto al concessionario ? Che fosse assolutamente, totalmente meccanica. Non ha nemmeno il tachimetro digitale. E io mi sento molto più sicura, anche se le statistiche dicono il contrario. (alcune parti di questo articolo sono già state pubblicate su Mente e Cervello)"},
{"title": "Come ti addestro un bambino", "text": "L'argomento in discussione oggi tra i miei amici (molti dei quali sono psicologi e/o hanno figli piccoli) è la presa di posizione dell'Associazione Culturale Pediatri (ACP, una delle società scientifiche più serie, legata allo sviluppo della pediatria di base in Italia) contro la trasmissione SOS Tata. Per chi non la conoscesse, SOS Tata è una trasmissione in cui famiglie in difficoltà con i propri figli, a volte per incapacità di gestione e altre volte per chiara disfunzionalità, ricorrono a una tata esperta che, dopo un paio di giorni di osservazione, fornisce regole e consigli per rendere la convivenza tra genitori e figli meno pesante. La denuncia di ACP tocca due punti importanti: il primo riguarda l'uso di bambini in un reality show nel corso del quale possono emergere situazioni imbarazzanti o conflittuali nell'ambito familiare. Sono storie e immagini che resteranno anche negli anni futuri a disposizione di tutti e che potrebbero danneggiare la privacy di questi bambini una volta diventati adolescenti o adulti. Su questo punto credo che sia difficile dissentire, anche se le riprese sono fatte in modo professionale e attento e, ovviamente, col consenso dei genitori (che però, su questo punto, dimostrano di essere poco attenti).  Il secondo punto critico, però, è quello che interessa di più. La trasmissione esiste da molti anni ma la presa di posizione arriva dopo una puntata in cui la tata suggerisce ai genitori di mettere in pratica il ben noto \"metodo Estivill\", un sistema di rieducazione al sonno basato sulla teoria cognitivo-comportamentale. Il bambino viene lasciato piangere per intervalli sempre più lunghi, cronometrati secondo uno schema che lo stesso Estivill, esperto di sonno barcellonese, ha messo a punto. Il metodo era già stato utilizzato in precednza nella trasmissione, ma mai prima d'ora le telecamere avevano ripreso così a lungo il pianto disperato del piccolo. Il metodo Estivill non è l'unico metodo di riabilitazione comportamentale del sonno del bambino ed è comunque consigliato dal suo inventore solo a partire dai 3 anni di età (anche se, dopo aver venduto milioni di copie del suo libro, il dottor Estivill ha precisato questo limite solo l'anno scorso). Ve ne sono molti altri, meno rigidi, molto usati e proposti anche dai pediatri nostrani, che si basano sull'assunto che il sonno sia un bisogno fisiologico come l'alimentazione. Mentre stiamo molto attenti a insegnare ai nostri figli le norme di una buona alimentazione, vietando alcuni eccessi, non siamo altrettanto abituati a fare la stessa cosa per quel che riguarda il sonno. L'unico modo per rimettere a posto la situazione è quindi, secondo questa tecnica, rieducare sia l'aspetto neurobiologico, garantendo un \"allineamento\" dell'orologio biologico a quello reale, sia quello comportamentale, insegnando ai più piccoli un po' di autonomia anche in questo ambito. Tutta la trasmissione, in realtà, applica (con l'accetta, c'è da dire) tecniche cognitivo-comportamentali: biglietti con norme scritte, elenchi di cose da fare affinché diventino un'abitudine eccetera. Dati i tempi strettissimi di presenza delle \"tate\" nelle case, è effettivamente l'unica strategia con la quale si può sperare di ottenere un cambiamento (quanto duraturo non è dato sapere, poiché la trasmissione si chiude dopo una settimana). Chi usa, con grande serietà e successo, i metodi cognitivi-comportamentali sia nella psicoterapia dell'adulto sia nei bambini sa che bisogna rinforzare continuamente le conquiste, che nulla è acquisito per sempre. Ma soprattutto sa che risolvere con questi sistema i problemi dei bambini (così come quelli degli adulti) è solo un primo passo verso una maggiore consapevolezza delle motivazioni alla base di un comportamento patologico. Se non si lavora sulle cause, la rieducazione è destinata inevitabilmente a fallire. Che tutto ciò vada ben al di là di una trasmissione TV come SOS Tata è evidente. Eppure c'è qualcosa che, nel comunicato di ACP, mi apparso stonato ed è esattamente questo paragrafo: \"Sappiamo ormai dalla ricerca scientifica, se non bastasse il buon senso,  che la fisiologia dell’essere umano prevede che riceva delle cure di  tipo prossimale da parte della madre e degli adulti che se ne prendono  cura, e che la pretesa che un bambino piccolo si addormenti da solo e  dorma per tutta la notte senza richiedere la presenza e il contatto  dell’adulto, oltre ad essere anti-fisiologica ed irrealistica, può  provocare confusione nei genitori e grande stress nei bambini. Questi metodi possono minare fin dalla primissima infanzia la fiducia  negli adulti e quindi in se stessi e interferire con lo stabilirsi di  una sana relazione genitori-figli, oltre ad interferire (se il bambino è  piccolo) con l’allattamento al seno. Esistono altre modalità di accudimento, più rispettose della fisiologia  del sonno e dei bisogni irrinunciabili dei bambini molto piccoli\". Vi leggo infatti una condanna non troppo velata a qualsiasi metodo educativo che pretenda di regolare i bisogni fisiologici (ma forse sbaglio). Che l'accudimento sia necessario è evidente, ma che debba essere per forza di cose determinato dalle richieste del bambino è tutt'altro che evidente. In molti paesi del mondo i bambini non vengono accuditi \"a richiesta\", né allattati \"a richiesta\", né affidati in modo prevalente o esclusivo alle cure materne: eppure crescono lo stesso sani e felici. Se i metodi comportamentali eccedono in un senso, un certo \"naturalismo\" nell'educazione infantile eccede nell'altro e non tiene conto che nella relazione tra madre (o genitore) e bambino, i soggetti sono due, ciascuno con legittime necessità (come per esempio quella di dormire a sufficienza per poter lavorare o di sospendere l'allattamento al seno senza per questo essere additata come madre degenere). Più che scientifica, quindi - e fatto salvo il sacrosanto principio di tutela dei minori - la diatriba tra ACP e SOS Tata mi pare molto culturale e lo dimostra anche una rapida ricerca in Internet: i fan di Estivill e del suo metodo si trovano per lo più su siti con un impianto più \"scientifico\", mentre i detrattori popolano siti che contengono nel loro titolo la parola \"naturale\". Per quel che mi riguarda, l'ho utilizzato per un certo periodo, senza grande successo perché sono un cuore di budino. Ma come dice Elasti, la mia mamma preferita, si può uscirne vivi e (apparentemente) sereni. PS: Nel frattempo un lettore (che ringrazio) mi ha ricordato questo studio uscito su Pediatrics che conferma l'assenza di danni a lungo termine nell'uso di questo tipo di tecniche per l'educazione al sonno"},
{"title": "C'è del marcio nella produzione scientifica?", "text": "A Belgrado - dove mi trovavo fino a ieri per la scuola di giornalismo scientifico dei balcani organizzata dall'Unesco (in compagnia della mia vicina di blog Claudia Di Giorgio) - l'inchiesta, pubblicata da Science (qui la sintesi di Repubblica) sugli studi scientifici del tutto inventati che escono su riviste facenti parte del circuito open access, è ovviamente la notizia del giorno. I colleghi presenti, in particolare quelli provenienti dai paesi anglosassoni e dalla Germania, hanno guadagnato con i loro articoli su questa storia la prima pagina dei loro quotidiani (il che non è frequente per un giornalista scientifico persino a quelle latitudini). Invece in Italia l'argomento è stato rapidamente liquidato come una questione per addetti ai lavori, oppure come l'ennesima dimostrazione che \"c'è del marcio\" (siamo o non siamo un popolo di complottisti?). A me pare importante spiegare a tutti come funziona oggi la produzione scientifica e quindi tutto ciò che ne consegue: possibili applicazioni delle scoperte ma anche valutazioni della qualità di uno scienziato e, infine, l'intero sistema di fiducia che dovrebbe essere garantito alla scienza e di cui abbiamo già parlato il questo blog. Quando un ricercatore scopre qualcosa, lo rende pubblico all'interno della comunità scientifica (e non solo, perché alcune scoperte arrivano sui giornali grazie al lavoro dei giornalisti). Per fare questo scrive un articolo e lo manda alle riviste di settore. Esiste una classifica delle riviste sulla base della loro importanza per la comunità scientifica di riferimento: quindi più interessante e innovativa è la scoperta, maggiori sono le possibilità che venga pubblicata su una rivista importante. Prima di accettare uno studio, la rivista sottopone la bozza a due o tre esperti dello stesso settore che, in modo del tutto gratuito e anonimo, dovrebbero fare le pulci alla pubblicazione, verificarne l'attendibilità e infine decidere se è accettabile o meno. Passate le forche caudine di questo processo chiamato peer-review, o revisione tra pari, lo studio viene pubblicato. Chi lo legge? Tutti coloro che sono abbonati, per lo più attraverso la loro università, alla rivista scientifica in questione.Le riviste sono di proprietà di grandi gruppi editoriali, come Science stesso. Il modello di business si basa sugli abbonamenti: lo scienziato non paga, il revisore non viene pagato, l'editore mantiene la struttura editoriale e distribuisce le riviste grazie ai soldi degli abbonati.Ma col tempo, e soprattutto con il proliferare del numero di riviste scientifiche, è diventato quasi impossibile per le università e le biblioteche mantenere attivi gli abbonamenti, che sono davvero dispendiosi. Non solo: l'accesso a pagamento alle conoscenze scientifiche è considerato da molti alla stessa stregua del \"digital divide\", ovvero qualcosa che permetterà ai ricchi di diventare sempre più ricchi (perché l'informazione è ricchezza) e ai poveri di diventare sempre più poveri (perché l'ignoranza è povertà). È nato così il modello dell'open access: riviste scientifiche digitali, disponibili gratuitamente sul web, che garantiscono lo stesso processo di revisione e di selezione di ciò che è ben fatto nella scienza. Chi paga? In questo caso è lo scienziato, che ha interesse a pubblicare perché da questo dipende la sua carriera, ma che dall'open access ottiene anche un secondo vantaggio: essendo gratuite, queste riviste stanno diventando sempre più importanti nella \"classifica\" delle riviste influenti, dato che tutti possono leggerne il contenuto e, se lavorano nello stesso campo, citarle in bibliografia (il che ne fa aumentare l'importanza nel ranking delle riviste scientifiche di pregio).  Vediamo ora che cosa ha fatto esattamente John Bohannon. Il giornalista di Science ha mandato a oltre 300 riviste open access un articolo inventato, contenente anche diversi errori marchiani, ottenendo ben 157 approvazioni. Ha anche seguito il destino dei soldi versati, scoprendo che spesso finiscono in Paesi come la Nigeria o il Pakistan, anche se la rivista si chiama \"American Journal of...\" Quindi se ne deduce che il meccanismo di controllo e di filtro, la revisione tra pari, non funziona più tanto bene, almeno nel modello open-access, in cui la rivista ha ovviamente interesse ad accettare il lavoro perché viene pagata dall'autore. Le cose, come sa chi fa scienza, non sono così semplici: una rivista open access che pubblica fuffa non salirà mai in graduatoria, quindi quelle che riempiono il loro \"catalogo\" di lavori scadenti finiranno col \"morire\", dopo aver ragranellato un po' di denaro. Il punto, però, è che le riviste con abbonamento non sono affatto esenti da errori e da valutazioni sommarie, come dimostra un collega giornalista, Ivan Oransky, che da alcuni anni pubblica un interessante blog dal titolo Retraction Watch in cui dà conto di tutti i lavori scientifici ritirati dopo la pubblicazione per via di magagne di diverso genere (comprese, seppure in netta minoranza, le frodi scientifiche o gli studi con disegni sperimentali sbagliati a priori). Dal punto di vista sociale, la questione è un'altra: negli ultimi 50 anni la quantità di informazioni che vengono dalla ricerca scientifica è aumentata in modo esponenziale fino a diventare sinceramente ingovernabile. Persino un superesperto non riesce più a seguire tutto ciò che si pubblica nel proprio ambito. Si potrebbe dire: fantastico, vuol dire che stiamo procedendo nella conoscenza! Non è esattamente così: esistono molte ricerche che sono ripetizioni di altre già pubblicate (il che è importante come conferma, ma quando diventa troppo frequente è lavoro inutile), altre di scarso interesse o ricaduta e così via. C'è, insomma, un gran rumore di fondo, dal quale è sempre più difficile far emergere il dato significativo. Come ho detto prima, finché gli scienziati verranno valutati solo sulla base di ciò che pubblicano (e, sinceramente, finora nessuno ha trovato un metodo più efficace benché se ne discuta da tempo), tutti saranno spinti a scrivere più lavori possibili, un fenomeno che gli americani hanno chiamato \"publish or perish\" ovvero \"pubblica o muori\" e che determina la vita di chiunque abbia ambizioni accademiche. Per tornare a Bohannon, ha fatto benissimo a fare la sua inchiesta, anche se meglio avrebbe fatto a sottoporre allo stesso test anche le riviste su abbonamento: sparare contro il modello open acces, che comunque è più \"democratico\" dell'altro, non serve, dal momento che abbiamo sempre meno soldi da investire nelle nostre biblioteche. Il rischio è che gli scienziati finiscano col pubblicare un sacco di lavori che nessuno legge. È evidente, però, che la comunità scientifica deve interrogarsi sul proprio modo di fare scienza e che, come hanno detto i vari esperti riuniti a Belgrado, il ruolo di \"cane da guardia\" della stampa specializzata (che può mettere in luce i migliori e additare i peggiori, come sta cercando di fare negli ultimi anni) sta diventando sempre più importante."},
{"title": "Autismo e vaccini, la saga continua", "text": "Vabbé. Tocca fare e rifare il lavoro sporco. Sì, perché sta storia dei vaccini e dell'autismo continua a riemergere, come un fiume carsico che ogni tanto si rivede in superficie, anche se uno è ormai convinto che sia interrato definitivamente. Quel che è accaduto lo potete leggere in questo pessimo articolo de Il Fatto Quotidiano, ma facciamo un breve riassunto.  Nel 2000 nasce il figlio del signor Antonio Palazzolo, che viene regolarmente vaccinato con tutte le vaccinazioni previste dal protocollo in vigore in Italia (e i molti Paesi del mondo). Il bambino, dopo i vaccini, ha la febbre alta e viene ricoverato per convulsioni febbrili, un'evenienza purtroppo possibile nei piccoli in presenza di temperatura corporea elevata ma che, nella maggior parte dei casi, è una condizione benigna: con la crescita il sistema nervoso diventa meno sensibile e il sintomo scompare. Una piccola percentuale di bambini sviluppa invece una epilessia vera e propria. Il figlio del signor Palazzolo, però, comincia a manifestare, qualche anno dopo, sintomi di regressione delle abilità acquisite, specie nel linguaggio, e poco dopo viene diagnosticato come autistico. I genitori vanno su Internet e trovano molto materiale a sostegno del nesso tra i vaccini (in particolare quello contro il morbillo, parotite e rosolia) e l'insorgenza di autismo. Il web ne è pieno: è una bufala che ha le sue radici in una ben nota frode scientifica. Nel 1998 la rivista medica Lancet pubblica infatti uno studio del medico britannico Andrew Wakefield che, analizzando le cartelle cliniche di 12 bambini autistici stabilisce un nesso di causa-effetto tra vaccini, disturbi intestinali e autismo. Già sei anni dopo, nel 2004, il giornalista del Sunday Times Brian Deer svela i conflitti di interesse economici dietro la ricerca di Wakefield e avanza dubbi sulla sua scientificità, ma siccome è un giornalista (!) la comunità scientifica non lo prende sul serio. Deer impiega altri sei anni, e investe tempo e denaro nella ricerca certosina delle cartelle cliniche all'origine dello studio, arrivando nel gennaio 2011 a pubblicare una controanalisi scientifica sulla rivista medica British Medical Journal: in pratica dimostra che Wakefield ha portato avanti una vera e propria frode scientifica, falsificando i dati, con l'idea di metter su un mercato di test diagnostici per le cause di autismo. Il danno però è  fatto: il web pullula di siti complottisti che continuano a citare lo studio nel frattempo ritrattato da Lancet, mentre Wakefield stesso è stato radiato dall'albo dei medici nel 2010. Non solo: nel marzo 2013 viene pubblicata sul Journal of Pediatrics un'altra grande ricerca condotta da Frank Di Stefano dei CDC di Atalanta, su oltre 1000 bambini con e senza autismo. In questo caso non vengono considerati solo i cicli vaccinali ai quali i piccoli sono stati sottoposti, ma vengono anche contati gli antigeni a cui sono stati esposti (cioè il numero di elementi potenzialmente immunizzanti e quindi legati all'attivazione del sistema immunitario). Ne consegue che non solo non vi è un nesso tra la pratica della vaccinazione, i suoi tempi e le sue modalità e l'insorgenza di autismo, ma non c'è nemmeno un nesso tra numero di antigeni e insorgenza della malattia. In sostanza, viene negata persino la possibile base immunologica del nesso di causalità. Nel caso italiano di cui si parla in questi giorni, oltre agli antigeni,  si è puntato il dito contro i metalli pesanti, e in particolare contro il mercurio contenuto come additivo nei vaccini fino al 2002. Ma i bambini che sono stati vaccinati con prodotti contenenti tiomersale, il composto a base di mercurio, hanno lo stesso tasso di autismo di quelli vaccinati con prodotti senza tiomersale. Una prova ancora più forte viene dalla costatazione che, dopo il bando del mercurio dai vaccini nel 2002 il tasso di autismo è rimasto invariato, quando non è invece aumentato. Qui potete leggere una revisione sulla questione e qui la pagina di Wikipedia, che è fatta molto bene. È vero che anche studi recenti, come questo, dimostrano che nell'organismo dei bambini autistici ci possono essere livelli elevati di metalli pesanti, ma è molto difficile capire che ruolo giocano nella patologia, visto che il più delle volte per essere nocivi devono agire su un substrato di predisposizione o di malattia già conclamata. Come se non bastasse, solo l'inquinamento atmosferico e l'esposizione professionale (per esempio nelle fabbriche) giustifica la presenza di una tale varietà di metalli pesanti in un unico organismo, non certo i vaccini. In seguito a queste ricerche, l'Organizzazione mondiale della sanità ha deciso di porre un ulteriore freno alle bufale che circolano con il seguente pronunciamento, aggiunto all'interno del suo documento ufficiale sull'autismo: \"I dati epidemiologici disponibili non mostrano nessuna evidenza di correlazione tra il vaccino trivalente per morbillo, rosolia e parotite e l'autismo, e lo stesso vale per ogni altro vaccino infantile\". Nel frattempo la “ricerca” di Wakefield ha portato, in Gran Bretagna, a un brusco calo di vaccinazioni: dal 92 per cento di copertura all'87 per cento (nel 2012), con l'esplosione di una vera e propria epidemia e un aumento dei casi di encefalite da morbillo. Torniamo in Italia: come è possibile che, a fronte di una tale mole di prove, un tribunale abbia potuto dichiarare che esiste un nesso di causa-effetto tra la vaccinazione e la malattia? Si aprirebbe qui un discorso troppo ampio che riguarda la relazione tra i tribunali e le prove scientifiche e la grande autonomia che è data al giudice nello scegliere i periti a cui rivolgersi, e anche nell'accogliere come prova un parere indipendentemente da quanto stabilito a livello scientifico. Diciamo che fa parte dell'autonomia dei giudici, che di per sé è un valore da preservare, purché siano dati loro gli strumenti per giudicare la qualità di ciò che viene loro presentato, il che non sempre accade. Ora la frittata è fatta, e il Consiglio di Stato, per non pagare un risarcimento privo di fondamenti scientifici, si appella a ragioni piuttosto meschine, contribuendo a fomentare la teoria del complotto. Ci si mette anche Beppe Grillo e il suo movimento (che in tema di prese di posizione sulla scienza è una vera galleria degli orrori) per dare voce ai genitori che, giustamente dal loro punto di vista, si attendono che una sentenza venga eseguita.Insomma, un bel pasticcio all'italiana, a cui si aggiunge il collega de Il Fatto Quotidiano con il suo articolo pieno di inesattezze e scarso approfondimento dei fatti."},
{"title": "L'ADHD e l'infanzia normale", "text": "È inusuale ritrovarsi al cinema alle 10 di mattina. Mi è accaduto oggi per la proiezione, in anteprima per la stampa, di ADHD Rush Hours, un docu-film di 75 minuti firmato dalla regista Stella Savino, con la consulenza scientifica di Stefano Canali, ricercatore in Storia della scienza e bioetica presso la SISSA di Trieste. Il documentario uscirà nelle sale il 26 giugno prossimo. L'ADHD o disturbo da deficit di attenzione e iperattività è una delle diagnosi più controverse in ambito neuropsichiatrico, anche perché riguarda, nella stragrande maggioranza dei casi, l'età infantile. I sintomi, come spiegano gli esperti intervistati nel film, sono piuttosto comuni: irrequietezza mentale, incapacità di concentrarsi, necessità di muoversi in continuazione, incapacità di rispettare turni o tempi delle conversazione, insoffereza verso le regole eccetera. Niente di tanto diverso dal comune comportamento infantile, se non nella quantità. Ed è proprio nella \"quantità\" dei sintomi che si gioca l'arbitrarietà della diagnosi: dove va posizionata l'asticella oltre la quale si può pensare che tali comportamenti siano la manifestazione di una patologia? La storia dell'ADHD è strettamente legata a quella di un farmaco, il metilfenidato, che ha una struttura analoga a quella delle amfetamine e agisce sul reuptake della dopamina nel cervello; è stato messo a punto nella prima metà degli anni '50 per il trattamento della depressione e della narcolessia. È solo verso la fine degli anni '90, però, che alcuni psichiatri statunitensi associano i disturbi dell'attenzione tipici dell'ADHD a una alterazione del sistema dopaminergico. I primi studi condotti su ragazzini con deficit attentivi mostrano che la sostanza favorisce la capacità di concentrazione e di autocontrollo, e il metilfenidato, col nome di Ritalin, diventa uno dei blockbuster dell'industria farmaceutica. Negli USA viene utilizzato anche a sproposito, da parte di studenti che non hanno alcuna diagnosi neurospsichiatrica, come psicostimolante per potenziare le capacità cognitive (per esempio la memoria). L'ADHD, benché non abbia ancora una chiara spiegazione eziologica, è entrato a far parte delle patologie riconosciute dal DSM, il manuale dei disturbi psichiatrici, già nel 1980: all'epoca figurava con la sigla ADD, poiché si teneva più conto del deficit di concentrazione rispetto all'iperattività. Nel 1970 l'American Psychiatric Association stimava che negli Stati Uniti vi fossero circa 150.000 casi di ADHD, ma vent'anni dopo le diagnosi erano già oltre un milione. Le stime attuali, per quel Paese, indicano una prevalenza dell'8-10 per cento della popolazione. In Italia l'ADHD ha \"attecchito\" pochissimo, forse per via di un sistema scolastico più inclusivo o forse per le basi culturali sulle quali si fonda la nostra scuola neuropsichiatrica infantile che, se deve proprio \"pendere\" a favore di una qualche interpretazione eziologica dei disturbi infantili, pende più facilmente verso l'ipotesi psicodinamica piuttosto che verso quella biologica. È evidente che, come il documentario spiega molto bene, c'è un problema di sovradiagnosi e di eccessiva medicalizzazione, almeno Oltreoceano. Attraverso la storia di Zache, che ha dieci anni e vive a Miami, e di Lindsay, che vive a New York e ha 25 anni, la regista mostra chiaramente l'approccio americano all'ADHD, fatto di cure farmacologiche non prive di effetti collaterali talvolta gravi e di terapie comprotamentali basate sul meccanismo della punizione-ricompensa, che a noi europei ricordano un po' troppo il condizionamento operante di Pavlov per non risultare istintivamente disturbanti. Poi c'è Armando, un ragazzo romano di 19 anni che, con il contributo della madre, racconta il vissuto della malattia dal punto di vista di chi sta nel nostro Paese. Non nasconde i pro e i contro della terapia farmacologica, che lui stesso, con la famiglia, comunque difende. È grazie ai farmaci che Armando ha potuto studiare, e questo è un dato inconfutabile, ma esistono dei lati oscuri della cura che il ragazzo racconta con grande efficacia: una continua focalizzazione sulla performance scolastica e sociale e, soprattutto, la sensazione di non essere davvero se stesso quando è sotto farmaci. La regista del documentario si è chiaramente fatta un'opinione, che traspare nel montaggio e in parte anche nella scelta dei soggetti da intervistare, ma è equilibrata nel presentare tutti i punti di vista, dall'esperto che è convinto della natura genetica del disturbo (pur riconoscendo che si tratta di una predisposizione legata a un gran numero di geni e che l'ambiente gioca ceratmente un ruolo importante nello sviluppo dei sintomi) a chi nega persino che l'ADHD possa esistere. A proposito di pluralismo, se mi si consente un inciso, sarebbe stato utile inserire dei sottopancia per indicare nomi e qualifiche degli intervistati: dal punto di vista scientifico, poter verificare il ruolo e le credenziali scientifiche degli esperti invitati a esprimersi è essenziale, e avrebbe dato ancor più valore a un lodevole tentativo di mantenere una posizione di equilibrio su un argomento controverso. Uscendo dalla sala, però, ho pensato che il documentario, che pure consiglio, sorvola su alcuni elementi che dal punto di vista strettamente scientifico sono invece importanti perché il pubblico possa farsi un'opinione. Per esempio quando afferma che nella maggior parte dei casi la diagnosi di ADHD si fa solo dopo che un bambino che ha assunto il farmaco mostra dei miglioramenti. Questo, però, è un comportamento scientificamente più che legittimo: la diagnosi ex juvantibus (cioè dopo che il paziente ha tratto giovamento da una certa cura) è uno dei possibili metodi diagnostici della medicina. Non il migliore, certo, ma sicuramente utile. Non sono poche le malattie che vengono confermate con lo stesso sistema, prima fra tutte la malattia di Parkinson, la cui diagnosi è certa solo se il paziente risponde alla dopamina Un secondo punto che a mio avviso non viene sufficientememte valorizzato è quello dell'impatto dei sintomi sulla vita quotidiana: è vero che di molti disturbi psichiatrici , come dell'ADHD, non conosciamo l'eziologia o le basi molecolari, ma per decidere che un certo insieme di sintomi costituisce una malattia si va a vedere quanto impatta sulla vita della persona, quanto interferisce con i suoi progetti o con la sua realizzazione negli studi, nella vita di relazione o nel lavoro. E, sinceramente, è difficile dire che questi bambini e ragazzi hanno una vita scolastica o sociale normale in assenza di un qualche intervento. E qui affrontiamo il punto più spinoso della questione: quanto conta l'ambiente in cui li facciamo vivere? Conta tantissimo, e su questo concordano sia i sostenitori sia i dettrattori dell'esistenza dell'ADHD. E non è un caso che gli Stati Uniti, col loro sistema scolastico tutto imperniato sulla performance piuttosto che sui contenuti o sulle abilità acquisite, siano anche il Paese con il maggior numero di diagnosi. È anche vero, e il documentario lo mostra bene, che l'ambiente familiare è importantissimo e che determinati modelli educativi possono essere più adatti a contenere i sintomi di altri, ovviamente nel caso di un bambino predisposto a certi disturbi del comportamento. Anche su questo, però, gli studi scientifici non danno una risposta univoca e, al momento, non è possibile dire se esiste un modello educativo che consenta di evitare lo sviluppo dell'ADHD. In conclusione, c'è una sola intervista, nel documentario, che mi ha davvero infastidito. Credo che a parlare sia un rappresentante del movimento antipsichiatrico negli USA, oppure un militante per le libertà civili (mancava appunto il sottopancia!). In una delle ultime scene dice: \"Le vere malattie sono quelle fisiche, quelle che si vedono e che i medici possono diagnosticare con gli occhi\". E con questo seppellisce la psichiatria tutta, l'ADHD, i pazienti e i loro disturbi nel novero delle malattie inventate, quelle causate da una struttura sociale iniqua oppure dal complotto di Big Pharma. Ecco, io sono istintivamente allergica alle teorie complottistiche, pur riconoscendo che nell'ADHD e nella sua gestione ci sono spesso più ombre che luci. Ma ci sono anche studi seri sulle possibili origini biologiche di certi sintomi, modelli cognitivi dell'attenzione che potrebbero spiegarne le manifestazioni e così via. Non me la sento di dire che si tratta di una \"malattia inventata\" o che siccome il metilfenidato è un farmaco decisamente sovraprescritto, allora è sempre inutile o, peggio \"è un veleno\", come dice il signore di cui sopra. Anche il cervello è un organo come gli altri e i nostri comportamenti sono comunque sempre riconducibili a segnali elettrochimici.  Ovviamente non siamo monadi e uno studio serio sull'incidenza dell'ADHD deve mettere in discussione anche i modelli sociali ed educativi, fermo restando che anche l'ambiente, come è ormai più che dimostrato, plasma i nostri circuiti cerebrali. Per concludere, credo che il punto nodale della questione si nasconda nella frase di un altro intervistato (che ha tutta l'aria di essere un neuropsichiatra o comunque un esperto del settore): \"è sempre più difficile, man mano che cambia la società in cui viviamo e in parallelo si sviluppano nuovi strumenti neuroscientifici, decidere chi è normale\". E sul concetto di normalità dovremmo avere tutti qualcosa da dire. Il trailer del documentario: "},
{"title": "Bufera nel mondo delle neuroscienze", "text": "Una decina di giorni fa ho ricevuto una mail. Il mittente era uno dei direttori dello Human Brain Project (HBP), il grande progetto europeo che riceverà (se non cambierà qualcosa nei prossimi mesi) un megafinanziamento per simulare in un computer un cervello umano. Il testo diceva più o meno: \"mandami il tuo cellulare, devo parlarti dello scandalo che sta per scoppiare\". Poco dopo ci siamo parlati e nella mia mailbox è arrivata la petizione che potete leggere qui. Promettere uno scandalo a un giornalista è come fare annusare la cocaina a un cane antidroga: va su di giri immediatamente. E così ho atteso impaziente che mi arrivasse il testo della lettera, firmata da un gran numero di neuroscienziati importanti (che crescono di giorno in giorno, perché la petizione è ancora aperta): un vero e proprio j'accuse contro lo Human Brain Project, gli scienziati che vi sono coinvolti e l'intero sistema di finanziamento alla ricerca messo in piedi dall'Unione Europea. C'è da dire che qualche avvisaglia di crisi si era intravista già un paio di mesi fa, quando un nutrito gruppo di neuroscienziati cognitivisti (cioè quelli che studiano le funzioni cognitive superiori e quindi forniscono i modelli sistemici del funzionamento cerebrale), guidato dal francese Stanislas Dahaene, ha abbandonato il progetto sbattendo la porta. La ragione: il direttore dell'HBP, lo svizzero Henry Markram, non ha autorizzato l'uso dei fondi per svolgere nuove ricerche sulle funzioni cognitive superiori. Nella sua idea (e, a dire il vero, anche nella formulazione del progetto presentata per ricevere il finanziamento), il denaro dell'Unione Europea dovrebbe servire a sviluppare nuove tecnologie informatiche in grado di consentire l'elaborazione matematica di quanto già si sa sul funzionamento del cervello, dal livello molecolare fino a quello cognitivo. Niente soldi per le neuroscienze classiche o per nuovi esperimenti, quindi, ma solo soldi per sviluppare computer, database e tecnologie in grado di portare alla simulazione completa di un cervello umano, come promesso con grande sicurezza da Markram stesso. Tra i firmatari della lettera di protesta vi sono quindi, in prima linea, gli scienziati che provengono dalle neuroscienze cognitive, ma non solo: anche molti \"computazionali\", come si chiamano quelli che lavorano sui modelli matematici, hanno aderito. Perché dietro la protesta si celano due diverse motivazioni, ognuna delle quali ha qualche ragione di fondatezza. La prima è puramente scientifica. Secondo alcuni esperti di neuroscienze computazionali, tra i quali l'italiano Alessandro Treves della SISSA di Trieste, il progetto è prematuro: non abbiamo ancora le conoscenze necessarie a portarlo a termine con successo e, soprattutto, è l'assunto teorico di base a essere fallace. \"Se non capisco l'originale, cioè il modello biologico che voglio riprodurre nella simulazione, non c'è ragione per cui io debba capire a fondo la copia\" spiega Treves. A questa critica risponde Idan Segev, numero uno del settore computazionale dell'HBP, secondo il quale riprodurre in forma di equazione matematica un processo biologico, così come è necessario fare perché i dati possano essere elaborati da un computer, è la forma più pura di conoscenza di un fenomeno, anche se si tratta per forza di cose di una semplificazione del processo. In sostanza, esiste una divergenza di fondo sulle potenzialità delle neuroscienze computazionali che però da sola non giustifica tanto astio e clamore: basti pensare che per via della lettera aperta, la comunità neuroscientifica mondiale si è spaccata in due e sono rimaste vittime, sotto le macerie, amicizie pluridecennali e collaborazioni scientifiche di portata storica. Ci sono neuroscienziati che erano amici per la pelle e che ora non si parlano più, e ferite tanto profonde che ci si chiede come potranno essere risanate. Il secondo argomento, più sostanziale e forse anche più interessante per i lettori, riguarda invece il modello di investimento in ricerca proposto dall'HBP e in generale dal bando che ha portato al suo finanziamento. La Comunità Europea, quando ha lanciato la gara per i cosiddetti flagship projects - vinti dall'HBP e da Graphene, un progetto per lo sviluppo di nuovi materiali che potrebbero cambiare il nostro modo di vivere -ha deciso di puntare sulla cosiddetta Big Science: grandi consorzi con obiettivi ambiziosi e molto visionari che impongono a un determinato settore di ricerca una visione unitaria, un unico modo di arrivare alla meta. E infatti i firmatari della protesta sono tassativi: questo modo di vedere la scienza non piace, perché impone una sorta di pensiero unico in un mondo dove la competizione, la creatività e la molteplicità dei modelli interpretativi ha finora funzionato alla grande. I partecipanti all'HBP, però, non demordono e insistono nel dire che continuare a produrre una gran mole di informazioni in formati e con metodologie diverse non consentirà mai di arrivare alla comprensione ultima dei fenomeni studiati perché, di fatto, impedisce di confrontare dati provenienti da laboratori diversi. La piattaforma ICT, che è il cuore dell'HBP, dovrebbe avere esattamente questo ruolo: consentire la \"sommatoria\" di tutte le ricerche disponibili e quindi, attraverso l'elaborazione di una mole immane di dati, la risoluzione di un gran numero di dubbi lasciati aperti dal modello di ricerca classico.  Chi protesta ha uno scopo ben preciso: bloccare l'erogazione della seconda e più consistente tranche di finanziamento e ridimensionare le aspettative dell'HBP; in alternativa, i firmatari chiedono un controllo serrato sulla produttività del progetto e sulla distribuzione dei fondi. Chiedono anche che le neuroscienze cognitive riprendano il loro ruolo guida nella ricerca sul cervello, ruolo soppiantato da \"computazionalisti\". In questo modo sperano di ottenere anche qualche finanziamento in più, dal momento che con l'apertura dell'HBP l'Europa ha tirato i remi in barca, per quel che riguarda la ricerca sul cervello, nella convinzione di aver già dato abbastanza. \"Non erano questi i patti, però\" spiega ancora Segev. \"I soldi dell'HBP vengono da un bando per lo sviluppo di nuove tecnologie e non da un bando per la ricerca biologica: e infatti quel che gli oppositori fanno finta di non capire è che il cuore del progetto è lo sviluppo della piattaforma ICT, non le neuroscienze in senso stretto e che quindi i soldi risparmiati da un eventuale blocco dell'HBP non beneficieranno certo le neuroscienze, ma semmai altri progetti nel campo delle tecnologie\". I funzionari della UE, però, non fanno tutti questi distinguo e tagliano i fondi a disposizione di tutti quelli che, nella ricerca sul cervello, per una ragione o per l'altra non hanno voluto partecipare al progettone. Per chi, come me, è molto curioso di politica della ricerca (anche perché tutto ciò si fa con i soldi dei contribuenti europei, cioè di ciascuno di noi), questa vicenda segna uno spartiacque. È la prima volta che una comunità scientifica appare così divisa dai tempi della messa a punto della bomba atomica (non a caso considerato il primo esempio di Big Science), solo che allora ci si scannava per ragioni etiche, oggi in parte per presupposti scientifici discordanti e molto per una visione non conciliabile di come deve essere finanziata la scienza. Nel frattempo il direttivo dell'HBP ha emesso un suo docuemnto di risposta: ben quattro pagine in cui cerca di controbattere punto per punto alle accuse dei \"congiurati\". L'affermazione più significativa, a mio avviso, è la seguente: \"La lettera aperta esprime la preoccupazione che gli obiettivi dell'HBP siano così irrealistici da danneggiare le neuroscienze, e afferma che non abbiamo conoscenze sufficienti per raccogliere una tale sfida. Condividiamo questa incertezza. Nonostante ciò affermiamo che nessuno sa davvero quanti dati neuroscientifici sono oggi disponibili perché non sono mai stati organizzati e soprattutto perché nessuno sa quanti dati sono necessari per avviare una tale sfida. Ricostruire e simulare il cervello umano è una visione, un obiettivo; i benefici verranno dalla tecnologia necessaria per arrivare a ciò. Questa tecnologia, sviluppata dall'HBP, beneficierà tutte le neuroscienze così come gli ambiti correlati. Molti altri ambiti scientifici hanno dimostrato che la simulazione può essere uno strumento che crea nuove conoscenze, non solo una conferma di risultati già esistenti\". Insomma, quelli dell'HBP sarebbero i visionari pronti a sfidare i limiti (quindi coloro che rischiano molto per una grande posta in gioco), gli altri quelli che preferiscono non lasciare la strada vecchia per la nuova. C'è n'è abbastanza perché valga la pena di stare a guardare come andrà a finire."},
{"title": "Un Nobel al GPS interiore", "text": "Ci sono pazienti che, quando li incontri, rimangono impressi per sempre. A me è capitato quando ero ancora una ragazzetta imberbe che frequentava il reparto di neurologia del Policlinico di Milano: era un architetto poco più che quarantenne che, in seguito a un ictus estremamente limitato, aveva perso la facoltà di orientarsi nello spazio. Persino il breve tragitto dal suo letto al bagno comune del reparto era un'avventura. Per scoprire qualcosa di più sulla sua malattia cominciai a interessarmi dei meccanismi di orientamento e poi di rappresentazione interna dello spazio e da lì è nata la mia passione per la neuropsicologia. Potete quindi immaginare la soddisfazione di vedere che l'accademia dei Nobel ha premiato quest'anno i tre scopritori del sistema di localizzazione interna del cervello, coloro che hanno identificato con precisione le strutture cerebrali che ci consentono di sapere dove siamo e di trovare la strada tra un punto e l'altro dello spazio, così come fa il navigatore GPS della nostra auto.  Il premio è diviso a metà tra l'inglese John O'Keefe, dello University College, e i suoi allievi, i coniugi norvegesi May-Britt (nella foto) ed Edvard Moser, dell'Università di Trondheim. Il primo ha scoperto, nel 1971, alcune cellule nell'ippocampo (una delle strutture essenziali per la memoria) che si attivavano quando i ratti sui quali conduceva gli esperimenti si trovavano in un punto preciso della stanza. O' Keefe concluse che le cellule dell'ippocampo conservano una sorta di mappa dello spazio che abbiamo già esplorato in precedenza, e ci consentono così di muoverci con rapidità e sicurezza. Non si tratta di una mera registrazione degli stimoli visivi ma di una vera e propria ricostruzione interiore dei luoghi che ci sono già noti. Nel 2005 i coniugi Moser hanno scoperto un altro elemento essenziale del sistema di posizionamento del cervello a livello della corteccia entorinale, che confina con l'ippocampo. In realtà il loro scopo era di studiare le connessioni dell'ippocampo ma, mentre analizzavano l'attività elettrica dei neuroni in ratti che si muovevano liberamente in una stanza, si accorsero che alcune delle cellule \"scaricavano\" quando gli animali attraversavano determinati punti su una sorta di griglia esagonale. In pratica ogni neurone si attivava quando l'animale si trovava in uno qualsiasi dei punti che costituivano una sorta di schema immaginario, consentendo quindi al ratto di \"navigare\" lo spazio e di comprendere le relazioni tra i diversi punti, esattamente come il GPS dell'auto utilizza almeno tre coordinate satellitari per localizzarci con precisione sulla mappa. In anni successivi gli studi di imaging hanno dimostrato che le scoperte fatte da O' Keefe e dai Moser a livello cellulare nel ratto sono valide anche per l'uomo. D'altronde, se non avessimo il senso della posizione in cui ci troviamo e la capacità di muoverci nell'ambiente, la nostra stessa esistenza sarebbe in difficoltà, come lo era quella del mio paziente e come lo è quella degli innumerevoli malati di Alzheimer che, proprio per via dell'atrofia dell'ippocampo che costituisce una delle caratteristiche di questa forma di degenerazione del cervello, manifestano tra i primi sintomi proprio il disorientamento spaziale. Questo premio è però importante anche per altre ragioni: innanzitutto con la loro scoperta i Moser hanno anche fornito uno dei primi esempi di substrato biologico di un meccanismo computazionale del cervello umano. Le cellule a griglia della corteccia entorinale che costituiscono il nostro GPS interiore elaborano le informazioni spaziali in modo peculiare. Altri possibili modelli computazionali avrebbero potuto dare lo stesso risultato, ma se l'evoluzione ha scelto proprio questo è perché è più efficiente degli altri, e quindi può essere utilizzato per costruire computer che ragionino nello stesso modo. Non solo: dando una base fisiologica ad un processo cognitivo, i Moser stanno lavorando al livello più profondo della cognizione, quello subcellulare. Proprio su questo piano si stanno orientando, negli ultimi anni, tutti i grandi gruppi di ricerca che cercano di elaborare modelli computazionali dei processi cognitivi. Ed è probabilmente anche a questo che hanno pensato i membri dell'accademia dei Nobel quando hanno deciso di dare loro un premio così prestigioso per una scoperta che ha solo 9 anni di vita. Quest'anno il Nobel per la medicina e la fisiologia farà felici anche gli umanisti, dato che il tema dell'orientamento e della memoria spaziale era presente in molti pensatori del passato, non ultimo Kant, che considerava la nostra capacità di ricordare strade e percorsi come la dimostrazione del fatto che vi erano abilità innate nel nostro cervello. E il pattern esagonale individuato dai Moser è presente fin dall'antichità in molti esperimenti matematici e nei primi modelli logici. Infine mi pare giusto anche mettere in rilievo alcuni elementi della biografia dei premiati: i Moser sono una coppia nella vita e nella scienza da oltre 30 anni. Hanno pubblicato insieme, contribuito in modo assolutamente paritario alla scoperta che li ha portati al più prestigioso premio scientifico. Sono un bell'esempio per chi oggi cerca con fatica di coniugare la famiglia e la ricerca, tra pregiudizi di genere e norme penalizzanti."},
{"title": "Più ci provo meno mi riesce", "text": "Quando ho letto su Nature, qualche giorno fa, la notizia che un tentativo di riprodurre un centinaio di famosi esperimenti di psicologia era sfociato in risultati incongruenti in 61 casi su 100, non ho potuto fare a meno di pensare al liceo, quando mi barcamenavo con le equazioni che davano un risultato diverso ogni volta che le rifacevo. \"Più ci provo meno mi riesce\", la mia esclamazione tipica a quel tempo, potrebbe essere il motto del Reproducibility Project, un interessante progetto di collaborazione tra laboratori di tutto il mondo per verificare se le scoperte date per assodate sulla base di esperimenti che hanno fatto la storia della moderna psicologia sono davvero tali. La replicabilità è uno dei cardini del metodo scientifico: se si spreca tanto tempo e tanta carta per descrivere nei dettagli un esperimento, invece di limitarsi ad annunciare al mondo intero i risultati salienti in dieci righe, è perché chiunque dovrebbe essere in grado, come con una ricetta di cucina, di riprodurlo ottenendo un risultato analogo. Quando ciò sistematicamente non accade, le ragioni possono essere solo tre: la procedura dell’esperimento (il cosiddetto protocollo) è stato descritto male o parzialmente; i dati raccolti nell’esperimento originale sono stati falsificati (cioè, in pratica, il ricercatore ha mentito); la statistica con la quale sono stati analizzati i dati originali è sbagliata. Quest’ultima evenienza dovrebbe essere scongiurata dalla cosiddetta revisione tra pari o peer review, il processo di controllo che tutti gli articoli scientifici subiscono prima della pubblicazione sulle riviste serie, ma anche i revisori sono uomini e gli errori possono sfuggire. In particolare, esistono studi basati essenzialmente sull’analisi dei dati, come quelli di psicologia sociale, per i quali verificare la correttezza della statistica significa quasi ripetere l’esperimento stesso, con un impegno e un dispendio di tempo ed energie che la media dei revisori di certo non ha. Quello che il Reproducibility Project ci dice, però, è che esistono diverse gradazioni di mancata riproducibilità: andando a interrogare i ricercatori, infatti, i coordinatori hanno scoperto che in 24 casi su 61 il risultato non era esattamente uguale a quello dell'originale ma ci si avvicinava, o almeno andava nella stessa direzione concettuale, come dimostra lo schema che potete vedere qui sotto.  L’intero progetto è stato supportato da una piattaforma digitale chiamata Open Science Framework finanziata dalla Arnold Foundation, una delle poche fondazioni che prevede esplicitamente dei fondi di ricerca per riprodurre esperimenti già condotti e, soprattutto, interessata a favorire la scambio e la condivisione dei dati in psicologia. La scelta di una piattaforma open, cioè priva di diritti e accessibile a chiunque, è nella natura del progetto, che vuole incentivare le buone pratiche scientifiche, tra le quali anche la trasparenza sui metodi utilizzati e soprattutto sulle elaborazioni statistiche dei dati. Secondo i finanziatori, questo è l'unico antidoto agli scandali per frode o falsificazione che aumentano di giorno in giorno, anche a causa della pressione a produrre a cui sono sottoposti gli scienziati. Il Replication Project è uno dei loro primi successi: i risultati possono essere commentati sulla piattaforma stessa (anche se, come racconta Nature, al momento sono parzialmente oscurati perché c'è un articolo scientifico in attesa di accettazione sulla rivista Science che impone l'embargo) ed è ciò che faranno alcuni dei gruppi che hanno condotto l’esperimento originale e che non si sono ritrovati nei risultati ottenuti con la sua riproduzione. La psicologia è uno degli ambiti in cui è più frequente la critica metodologica. Molte delle teorie classiche sono vecchie di decenni, oppure frutto di studi fatti su campioni relativamente piccoli di soggetti. Inoltre, specie per quel che riguarda la psicologia sociale, cioè il comportamento di un soggetto in relazione agli altri o all’ambiente circostante, sono frutto di una generalizzazione non sempre dimostrata sperimentalmente. Un certo comportamento potrebbe non essere universale, ma culturalmente determinato e noi non abbiamo modo di dimostrarlo se non replicando l’esperimento iniziale in un contesto diverso da quello originale. Non si tratta di fisime da puristi della scienza: questi esperimenti sono serviti a modellare, tra le altre cose, anche le norme di convivenza civile, i regolamenti economici oppure i metodi educativi. Non a caso era stato un premio Nobel per l’economia, lo psicologo israeliano Daniel Kahneman, a invitare i suoi colleghi, nel 2012, con una mail finita anche sulle pagine dei giornali, a replicare i suoi esperimenti per dar loro maggiore consistenza (e i suoi esperiemnti ne sono usciti benissimo). Tra gli esperimenti non replicati ve ne sono due che si basano tutti sullo stesso presupposto teorico: l’esistenza dell’effetto di priming sociale (il priming è in psicologia un fenomeno percettivo che orienta il ragionamento in una determinata direzione).  Secondo Travis Carter, psicologo sociale del Colby College di Waterville, nel Maine, se si mostra a un individuo una fotografia di un paesaggio nel quale è inserita una bandiera americana e poi lo si sottopone a un questionario socio-politico, esprimerà opinioni più conservatrici rispetto a chi è esposto alla stessa fotografia senza la bandiera. Nel corso del Replication Project questo effetto non compare, così come non sono stati confermati gli studi di Eugene Caruso, dell’Università di Chicago, secondo il quale esporre gli individui al denaro favorisce una maggiore accondiscendenza nei confronti del sistema sociale attuale. Tutto ciò non significa che il priming sociale non esiste, ma che il fenomeno è meno costante di quanto ipotizzato da chi lo ha scoperto e probabilmente influenzato da molte variabili difficilmente riproducibili. Per concludere, non è un caso che sia stato proprio il mondo della psicologia a iniziare un lavoro consistente sul tema della riproducibilità: da sempre la psicologia è considerata una scienza \"poco scientifica\". Se si tiene conto degli esperimenti con risultati \"quasi uguali\", si può dire che invece questa disciplina presenta tassi di fallacia analoghi a quelli di altre scienze intrensicamente \"inesatte\" come la medicina (secondo un lavoro ormai storico pubblicato su PLoS Medicine nel 2005 da John Ioannidis, professore di medicina preventiva presso la Stanford University School of Medicine, la medicina si fonderebbe su un buon 30 per cento di esperimenti sbagliati perché statisticamente scorretti, in gran parte per errori metodologici ma anche per vere e proprie frodi). Infine mi rassicura molto quel 39 per cento di esperimenti perfettamente riproducibili, sebbene la ripetizione sia stata condotta in Paesi diversi da quello dell'originale e talvolta anche con modifiche dei test per renderli più adatti al mutato contesto culturale o sociale. Significa che la psicologia è pur sempre una scienza, che il comportamento umano può essere standardizzabile (a grandi linee) e misurabile e che gli uomini, a qualsiasi latitudine, si somigliano in alcuni meccanismi di base del comportamento individuale e sociale. Una parte di questo testo è tratta da un mio articolo sulla prima fase del Replication Project uscito su Mente e Cervello nel mese di marzo 2014"},
{"title": "Te lo devo, Dr. Sacks", "text": "Oliver Sacks era un estraneo per me. O, meglio, ci siamo incontrati, una volta, a Milano per un'intervista qualche anno fa. Avrei voluto dirgli molte delle cose che oggi scrivo, ma non ebbi il coraggio di uscire dai binari di ciò che è professionalmente consentito, e me ne dispiaccio. Perché adesso che se n'è andato, per le conseguenze di un melanoma che lo ha colpito una decina di anni fa, è come se avessi perso una persona cara.  A 17 anni avevo già in mente di fare il medico, ma leggendo l'Uomo che scambiò sua moglie per un cappello scoprii la neuropsicologia e decisi che quello era ciò che volevo fare nella vita: studiare il cervello umano, i processi cognitivi, il modo con cui interagiamo con l'ambiente, pensiamo, proviamo emozioni. È grazie a Sacks che ho scopero Lurjia, il grande neurologo russo di cui si sentiva in un certo senso discepolo pur non avendo mai studiato direttamente con lui. E ancora grazie al dottor Sacks bussai un giorno all porta della Clinica neurologica del Policlinico di Milano e iniziai un internato in neuropsicologia. Non posso dimenticare il giorno in cui vidi il mio primo paziente con neglect: avevo, vivide nella mente, le pagine in cui Sacks descrive la sua paziente che mangia mezzo piatto, si trucca mezza faccia, infila un braccio solo nella giacca e riesce a \"vedere\" solo una metà del mondo, pur non avendo alcun disturbo della vista. Non avevo mai pensato che i pazienti \"veri\" potessero essere così: pensavo che, come ogni scrittore, avesse dato un'anima a un banale disturbo neurologico esagerando un pochino. Invece il mio paziente era proprio come la donna del libro: altrettanto affascinante, intrigante e al contempo disperante ed emozionante perché la sua vita era rovinata ma io non potevo fare nulla per lui. Dopo questo primo caso ho visto tantissimi pazienti con neglect ma grazie a Sacks non ci ho mai fatto l'abitudine e ancora oggi ogni volta è una sorpresa, e ogni volta mi stupisco dell'incredibile strumento che è il nostro cervello, nelle sue espressioni più felici e anche nella malattia. Molti esperti hanno accusato Sacks di aver descritto come eccezionali casi che in realtà sono abbastanza comuni in una clinica neurologica: è vero, ma sono convinta che non lo abbia fatto per conquistarsi la gloria ma proprio perché per lui, come per me, erano sempre eccezionali, uno diverso dall'altro. Nessun essere umano che possa sopravvivere avendo perso metà del mondo può essere definito meno che eccezionale, e questo è il suo maggior insegnamento. Il dottor Sacks mi ha commosso con Risvegli, fatto scoprire il mondo dei sordi con Vedere voci; mi ha intrigato con la descrizione di tutte le possibili forme di aura in Emicrania; quando ho cominciato a interessarmi di autismo e sindrome di Asperger, ha pubblicato Un antropologo su Marte; prima che lo sviluppo della neurochirurgia e degli stimolatori cerebrali riportasse in auge la sindrome di Tourette, ha raccontato il dilemma che i tourettici vivono nel dover scegliere tra essere sommersi dai tic e dalle stereotipie e la perdita della creatività indotta dai farmaci; quando la relazione tra schema corporeo e coscienza era ancora un argomento di frontiera della ricerca neuroscientifica, ha scritto Su una gamba sola, ascoltando il proprio corpo e gli effetti che un intervento ortopedico aveva avuto sulla sua capacità di utilizzare l'arto ferito; ma, soprattutto, ha rafforzato la mia convinzione (scientificamente tutta da dimostrare!) che l'uomo sia la specie più musicale che esiste in Musicofilia. Come ogni musicofilo che si rispetti, adorava Bach e la sua perfezione formale che nulla toglie all'emozione, e anche questo me lo ha fatto sentire vicino. In un tempo in cui la medicina narrativa sembrava morta, Sacks ha riportato in primo piano l'uomo che c'è prima della malattia e malgrado la malattia; in un periodo in cui la riabilitazione neurologica lavorava sulla riparazione del deficit, lui raccontava quanto è importante lavorare sulle abilità residue invece di incistarsi ad aggiustare ciò che non è aggiustabile, una strategia che la neuroriabilitazione ha fatto propria. In anni in cui la neuroetica e i dibattiti sulla coscienza non erano ancora di moda, ha messo in luce, nei suoi racconti, i dilemmi etici che solo le malattie che colpiscono il cervello sono in grado di sollevare. Tra le righe dei suoi casi affiora sempre la stessa domanda: cos'è che fa di noi esserei umani delle creature così uniche, così intriganti e che cosa accade quando perdiamo pezzi di questa nostra identità. Non era un genio, non era nemmeno un grande scienziato ma era certamente un grande narratore e un grande essere umano e grazie a ciò ha intuito ciò che altri hanno studiato con più sistematicità e lo spiegato in modo tale da renderlo comprensibile a tutti. In un commovente articolo pubblicato due settimane fa sul New York Times, Sacks ha confrontato la sua dipartita con il riposo del sabato ebraico, la cessazione obbligata da ogni attività che aveva sperimentato durante l'infanzia vissuta in una famiglia di ebrei ortodossi nella Londra prima della Seconda Guerra mondiale: \"E ora, debole, col fiato corto e i muscoli, una volta sodi, disfatti dal cancro, mi accorgo che i miei pensieri - non su faccende soprannaturali o spirituali ma su cosa si intende per aver vissuto una  vita bella e utile  - creano una gran senso di pace dentro di me.  Scopro che i miei pensieri vanno allo Shabbat, il giorno del riposo, il  settimo   giorno della settimana, e forse il settimo giorno della nostra vita,  quando possiamo sentire di aver concluso il nostro lavoro, e di potere, in  buona coscienza, riposare\". Buon viaggio, e grazie."},
{"title": "Gay, adozioni e scienziati col comunicato facile", "text": " Apro la mail e trasecolo: alle 16 di oggi Giuseppe Di Mauro, presidente della Società  italiana di pediatria preventiva e sociale invia a noi giornalisti un comunicato stampa nel quale esprime “seria preoccupazione per la rapidità e la leggerezza con la quale, a livello mediatico, si stanno diffondendo informazioni superficiali e spesso, fuorvianti, sulle “Adozioni gay”, argomento molto delicato che andrebbe valutato con maggiore rigore scientifico, soprattutto per le ripercussioni che comporta sulla crescita e lo sviluppo del bambino”.    Dopo tale esordio, il dottor Di Mauro ci avverte di aver mandato una “denuncia all’Autorità per le Garanzie nelle Comunicazioni (Agcom) nei confronti di una trasmissione di Canale 5, per tutelare il pluralismo e le libertà fondamentali nel settore dell'editoria e dei mezzi di comunicazione”. Cosa avrà mai fatto Canale 5? Ha affermato che non ci sono studi scientifici che dimostrino che le adozioni gay sono un male per i bambini. “Il dibattito, - sostiene Di Mauro, - è molto complesso e scientificamente ancora aperto e quindi, sarebbe auspicabile, da parte dei mezzi d’informazione, una maggiore cautela e più consapevolezza dei messaggi che vengono trasmessi ai tanti telespettatori. Di studi a riguardo ne esistono tanti, ma la loro qualità è spesso scarsa, soprattutto riguardo al metodo di campionamento: uno studio scientificamente valido deve essere condotto su un campione casuale e su un numero significativo di soggetti. Invece, la maggior parte delle ricerche su questo argomento sono state realizzate su campioni non casuali e di piccole dimensioni e quindi non rappresentativi.” Di Mauro fa poi riferimento a una revisione di 59 studi di piccole dimensioni condotta nel 2004 dall'American Psychological Association (APA) che sosteneva la sostanziale equivalenza tra genitori eterosessuali e genitori gay, affermando che “questo studio è stato successivamente screditato da una buona parte della comunità scientifica e dall’Ex Presidente della stessa Società Scientifica”. Di Mauro, però, non ci dice chi sono i componenti di questa “società scientifica\" e sorvola sul fatto che l'APA ha continuato a monitorare la situazione dal 2004 a oggi, ribadendo più volte i risultati raggiunti con la prima analisi (l'ultima volta l'11 giugno scorso, come potete vedere dal comunicato che trovate sul sito dell'APA stessa). È bene far notare che gli studi riguardanti gli esiti di adozioni da parte di coppie gay sono basati su campioni piccoli per ovvie ragioni: anche laddove le coppie omosessuali hanno il diritto di accedere all'adozione, i casi in cui poi questa arriva effettivamente a conclusione sono pochi. Alla fine del 2010 ne è stato pubblicato un altro, a firma di Farr, Forsell e Patterson su Applied Developmental Science basato su un campione di 109 famiglie, metà delle quali con genitori omosessuali, dal quale si evince di nuovo che non ci sono grandi differenze. Ciò che determina eventuali problemi psicologici o di sviluppo nella progenie sono fattori come l'instabilità della coppia e lo stress ambientale, che non dipendono dalle scelte sessuali dei genitori bensì dal contesto in cui si trovano a vivere. In sostanza questo studio non fa altro che dimostrare cose già note, ovvero che essere figli adottivi è di per sé un \"rischio”, esattamente come è “a rischio” l'abitudine di bere troppo caffè o di mangiare troppi grassi: sono tutte situazioni di maggiore fragilià sulle quali si può però agire con adeguate misure preventive (per esempio con un'accurata scelta delle persone adatte all'adozione e con un sostegno sociale alla stessa). Vi sono, invece, molti studi sulla genitorialità omosessuale in senso lato (figli nati all'interno di coppie omosessuali oppure figli di prime relazioni eterosessuali che si trovano a vivere con un genitore che ha un nuovo compagno o compagna dello stesso sesso). Non sto a sintetizzarli perché potete trovarli in questo bell'articolo di Chiara Lalli (che sul tema ha scritto anche un libro intitolato \"Buoni genitor\" e pubblicato da Il Saggiatore), dal quale si evince, peraltro, che il dottor Di Mauro non è nuovo a queste prese di posizione. Il comunicato termina però con una notizia bomba: esiste uno studio, il più ampio mai condotto, “l’unico studio che ha attualmente una riconosciuta validità”, e lo ha pubblicato nel 2012 un sociologo dell’Università del Texas, Mark Regnerus. Il campione è enorme (oltre 12.000 soggetti) e la metodologia più attendibile perché “fa parlare direttamente i figli” (invece di valutarne lo stato di salute e sviluppo attraverso strumenti testistici validati come hanno fatto gli altri). Secondo questo studio “il 12% (dei figli di coppie omosessuali) pensa al suicidio (contro il 5% dei figli di coppie etero), sono più propensi al tradimento (40% contro il 13%), sono più spesso disoccupati (28% contro l’8%), ricorrono più facilmente alla psicoterapia (19% contro l’8%), sono più spesso seguiti dall’assistenza sociale rispetto ai coetanei cresciuti da coppie etero­sessuali sposate. Nel 40% dei casi hanno contratto una patologia trasmissibile sessualmente (contro l’8%), sono genericamente meno sani, più poveri, più inclini al fumo e alla criminalità” (i grassetti sono dell'autore del comunicato). Conclude Di Mauro:     “I bambini hanno una grande capacità di adattamento e quindi possono certamente crescere con genitori dello stesso sesso, tuttavia, sulla base della letteratura scientifica disponibile, i bambini sembrano più adatti ad avere una vita adulta con successo quando trascorrono la loro intera infanzia con i loro padri e madri biologici sposati e specialmente quando l’unione dei genitori rimane stabile a lungo”. Tralasciamo la prosa per cui vanno bene solo i genitori biologici sposati (ma se questi sono una coppia disfunzionale? Se si drogano? Se non possono mantenermi? Se (sia mai!) divorziano?) e andiamo a vedere nel dettaglio lo studio texano. Poiché anche noi non vogliamo essere obnubilati dai pregiudizi, facciamo finta di non sapere che Regnerus è un conservatore integralista, vicino ai Tea Party, che sostiene da anni che gli unici rapporti adatti a crescere dei figli sono quelli tra due eterosessuali sposati (in questo concorda pienamente con Di Mauro). Diciamo però che intervistare figli di coppie gay di età variabile tra i 18 e i 39 anni significa valutare situazioni molto diverse tra loro, perché 30 anni fa la vita per i gay era piuttosto difficile anche negli Stati Uniti e la riprovazione generale certamente più forte di oggi. È normale quindi che i figli di quella generazione di omosessuali abbiano sofferto più di quelli di questa generazione (e di ciò è convinto anche Regnerus, che non nega affatto l'importanza del contesto in cui si cresce, indipendentemente dalle scelte sessuali dei genitori). Ci sono però altri punti molto controversi nella ricerca. Per esempio sono classificati come “figli di coppie gay” tutti coloro che rispondono sì a questa domanda: “Uno dei vostri genitori è mai stato coinvolto in una relazione romantica con una persona dello stesso sesso?”. Il campione quindi non è composto da figli di coppie gay stabili e conviventi, come devono essere eventuali genitori che aspirino all'adozione, ma da chiunque sappia (o pensi) che uno dei propri genitori (magari regolarmente sposato con persona di sesso diverso) abbia avuto una love story omosessuale. Quanti, secondo voi, hanno risposto sì alla domanda di cui sopra sul campione di svariate migliaia di soggetti? Per l'esattezza 253, di cui 176 dichiarano che la madre ha avuto una relazione omosex. Solo il 42 per cento del campione è vissuto con un padre gay e il suo compagno per almeno quattro mesi e solo il 2 per cento (cioè circa 2 o 3 persone su tutto il campione!) ha vissuto con una coppia gay per almeno tre anni. Riportiamo ora le percentuali di cui sopra riguardanti i suicidi e la disoccupazione ai valori assoluti del campione e ragioniamo su quanto attendibile possa essere uno studio che confronta da un lato (se va bene) poche decine di persone selezionate con criteri contestabili e dall'altro migliaia di persone “normali” che fungono da gruppo di controllo. Come hanno detto diversi esperti, se questo studio dimostra qualcosa, questo qualcosa non è l'effetto negativo di avere dei genitori gay, ma quello di avere dei genitori poco presenti o in difficoltà nell'accettare se stessi. Potreste però dirmi: questa ricerca è stata pubblicata su una rivista peer reviewed, ovvero Social Science Research dell'editore Elsevier. Come è possibile? Qui la faccenda si fa tanto complessa che ci vorrebbe un altro post: vi basti sapere che Regnerus è stato finanziato, per questo studio, da una fondazione conservatrice presieduta da Brad Wilcox, sociologo dell'Università della Virginia, che dirige il National Marriage Project, un grande studio sullo stato di salute dei matrimoni americani. Brad Wilcox siede anche nel comitato editoriale della rivista che ha pubblicato lo studio e, secondo alcune fonti riportate dal New Yorker (ma non del tutto confermate), avrebbe anche partecipato all'analisi dei dati, il che lo porrebbe in chiara situazione di conflitto di interessi. L'Università di Regnerus (che riceve donazioni consistenti dai maggiori esponenti conservatori del Texas, piuttosto arrabbiati per via della politica liberale nei confronti dei gay adottata dall'amministrazione Obama) ha aperto un'indagine interna dopo le prime reazioni indignate sollevate dalla pubblicazione dello studio, prontamente archiviata perché \"non sono state trovate nelle mail del professor Regnerus prove del coinvolgimento del professor Wilcox nello studio\".  Cerchiamo ora di trarre alcune conclusioni da questa storia: "},
{"title": "Ma tu, quanto sai rischiare?", "text": "Da quando questo pomeriggio è uscita la sentenza di condanna per i membri della Commissione Grandi Rischi che ha rassicurato la popolazione aquilana il giorno prima del terremoto che ha fattto 309 morti, tra colleghi non si parla d'altro. Il sentimento prevalente è lo scoramento: come è possibile che si condannino degli esperti per non aver previsto qualosa che non è prevedibile? E in effetti mi pare che dietro questa sentenza si possa ravvisare una cattiva comprensione di ciò che la scienza può dire e quindi, di conseguenza, di quale può essere il ruolo di una commissione che deve valutare cosa fare in situazione di incertezza, anche se per dirlo dovrei leggere la motivazione che verrà depositata entro i prossimi tre mesi.  Ci sono però altre questioni più sottili che a mio avviso vale la pena discutere. Prima però è giusto che chiarisca il mio pensiero: trovo la sentenza aberrante per la sua durezza ma soprattutto per non aver fatto i dovuti distinguo tra membri politici e membri scientifici della commissione. Chi pronuncia certe parole davanti a una telecamera ha una responsabilità diversa da chi ha dato la propria opinione di tecnico e poi è tornato a casa propria. Le responsabilità sono soprattutto nell'uso politico e strumentale delle informazioni scientifiche al fine di tenere a bada la legittima preoccupazione di una popolazione. Ci sono però responsabilità anche dalla parte degli scienziati che, invece di accettare supinamente le eccessive semplificazioni dei politici, avrebbero dovuto opporsi e smentire, se non altro per dimostrare che le cariche consultive nelle commissioni tecnico-scientifiche vengono prese sul serio e non sono solo una scocciatura imposta da un'organizzazione borbonica dello Stato. Questa storia è un caso emblematico di che cosa accade in un Paese che non sa fare i conti con l'incertezza, con la comunicazione del rischio e con qualche concetto molto elementare di calcolo delle probabilità. Sono sicura che la frase eccessivamente rassicurante che ha contribuito ad accrescere il numero delle vittime del terremoto sia stata influenzata dalle sparate di Giampaolo Giuliani, il tecnico che affermava di poterlo predire in base a teorie già ampiamente smentite da diverse sperimentazioni effettuate in tutto il mondo. Davanti ai ciarlatani che vendono certezze, la scienza a volte fa l'errore di venderne a sua volta, perché l'inevitabile incertezza non sembra essere un contraltare sufficiente. Siamo un Paese che rifiuta di affrontare i rischi, sia sul piano personale sia su quello collettivo: ci rifiutiamo di accettare il fatto che a volte non esistono risposte e che le decisioni devono essere prese a livello individuale, valutando in modo assolutamente soggettivo il peso dei diversi fattori in gioco. Come qualcuno ha detto nei vari articoli che ho letto in queste ore, anche un'evacuazione di massa può fare vittime, come ben sanno in California, dove alcuni allarmi rivelatisi poi ingiustificati hanno provocato un aumento della mortalità per incidenti stradali e allontanamento dagli ospedali. Quindi, paradossalemnte, se la Commissione avesse detto che il rischio c'era, oggi potrebbe trovarsi sul banco dei condannati per procurato allarme. Un collega di grande esperienza ha scritto, su Facebook, che disquisire di come si comunica l'incertezza è una questione per accademici che non hanno mai fatto lo sporco mestiere del giornalista e ha ricordato le sale stampa piene di cronisti privi di qualsiasi formazione scientifica che di fronte a qualsiasi risposta men che sicura si precipitano fuori per dettare un bel titolone allarmistico a quattro colonne. È vero, chi lo nega: ma possiamo pensare che argomenti da cui dipendono le scelte di vita delle persone siano gestiti con in testa un modello di comunicazione di questo genere? O vogliamo approfittare di questa assurda sentenza per convincere chi di dovere che è giunta l'ora di investire nella formazione di chi prende certe decisioni, di chi passa le notizie al pubblico e anche, perché no, del pubblico stesso? Un po' meno Giacobbo e un po' più di serietà nel trattare certi argomenti, per esempio, non guasterebbe. Possiamo noi giornalisti specializzati cominciare a ribellarci al diktat delle certezze, quella sorta di tabù redazionale per cui quando si scrive di argomenti incerti (dall'utilità degli screening oncologici ai rischio di nubifragi fino al riscaldamento globale) non si può lasciare il lettore nel dubbio ma si deve dare a tutti costi la risposta giusta? Non so quante volte ho cercato di spiegare (con scarso successo) a un caporedattore o a un direttore che il nostro compito è fornire tutti gli elementi di una questione perché ciascuno poi prenda da sé le proprie decisioni: qualcosa di non molto diverso da quanto avrebbe dovuto fare quella notte la commissione chiamata a spiegare che cosa stava succedendo in Abruzzo. Purtroppo, per molti dei problemi che affrontiamo sui giornali non esiste una risposta giusta: non ce l'abbiamo noi e non ce l'hanno nemmeno gli esperti. E questa è l'unica cosa sensata che si può dire a chi ci ascolta. Ma siamo disposti, tutti noi, a prenderci la responsabilità individuale di scegliere in un contesto di incertezza? Temo che dovremo per forza imparare, perché dopo questa condanna, gli scienziati si chiuderanno in un timoroso silenzio e la platea sarà occupata dal cronista che grida \"attentato\", come il comico di Zelig, accompagnato dal Giuliani o dal Di Bella di turno. Perché di dispensatori di certezze è pieno il mondo."},
{"title": "Elogio di una donna imperfetta", "text": "Non amo i coccodrilli, né le agiografie post mortem. Eppure sento di dover scrivere qualche riga in memoria di Rita Levi-Montalcini, se non altro perché, avendo conosciuto da vicino una parte della sua famiglia torinese, ho avuto la fortuna di poter ridere alle sue spalle. Sì, proprio così: mi sono permessa, da ragazzina, di ridacchiare di quella che era già diventata una star della scienza dopo aver ricevuto il premio Nobel per la scoperta dell'NGF.  Ho infatti frequentato la casa di alcune sue cugine che, con l'understatement tipico di un certo ebraismo piemontese, godevano sottilmente nel raccontare, alla giovane appassionata di scienza che ero allora, tutti i pettegolezzi e piccole meschinerie di cui la grande Rita si sarebbe macchiata nei suoi anni acerbi, spinta, dicevano loro, da un'ambizione smisurata, che le permise di superare il doppio handicap di essere donna e appartenente a una minoranza religiosa (seppure solo nominalmente, poiché si è sempre fieramente dichiarata laica e atea) contro la quale l'Italia aveva promulgato le leggi razziali. Quando, molti anni più tardi, mi capitò di intervistarla (l'ultima volta per l'inaugurazione dell'EBRI, lo European Brain Research Institute che doveva essere il luogo d'eccellenza della ricerca neuroscientifica in Italia) non osai dirle che conoscevo di lei un lato familiare e forse meno brillante di quello che mostrava all'esterno, ma che me la rendeva tanto più simpatica e umana. Non osai anche perché, malgrado la sua squisita gentilezza e buona educazione, era una donna che intimidiva, come terribilmente intimidente era quell'\"Elogio dell'imperfezione\" che scrisse per raccontare quanto perfetta fosse stata la sua vita e la sua carriera scientifica. È un libro che ho molto amato (ero al secondo anno di medicina quando lo lessi) perché, tra le righe, diceva che per arrivare ad essere come lei bisogna essere capaci di sminuirsi in apparenza per esaltare al meglio le proprie doti e i propri traguardi. Il vero moto di ammirazione, però, me lo strappò nel 2006, quando si presentò in Senato, pur non stando bene e alla veneranda età di 97 anni, per votare la fiducia al governo Prodi, dando un senso, ai miei occhi, all'istituzione dei senatori a vita. Alle donne di scienza la Montalcini ha fatto un altro regalo, tutt'altro che scontato: ha detto che è lecito essere geniali e vanesie allo stesso tempo, lei che non si faceva fotografare se non con i capelli candidi e perfettamente a posto, il vestito con la piega giusta, quei colletti così anacronistici e i gioielli che amava molto. Molto prima di qualsiasi maldestro spot della Comunità Europea per convincere le donne che si può fare lo scienziato con il tacco 12, lei vestiva solo Capucci: se questa non è classe..."},
{"title": "Il post numero 100 e il cervello del futuro", "text": "Da molti mesi non riesco a trovare un argomento che mi faccia dire: vale la pena di parlarne sul blog. Contagiata dalla generale involuzione del nostro Paese, e dall'emergere di un atteggiamento decisamente ostile alla scienza, come dimostrato dai recenti fatti di cronaca, mi pareva che qualsiasi scoperta mi passasse tra le mani fosse, in fondo, di scarso interesse per i lettori, in tutt'altre (e più importanti) faccende affaccendati. Si dà anche il caso che, se il contatore di Wordpress non mente, questo sia il post numero 100 da quando il blog è nato. Sono passati un bel po' di anni dal mio primo e maldestro scritto, durante i quali sono cresciuta insieme ai lettori, scoprendo persone appassionate di tutto quanto ruota intorno alla ricerca sul cervello persino più di me, il che non cessa di stupirmi. Nella spasmodica attesa di qualcosa che mi ispirasse tanto da produrre il post perfetto e degno della cifra tonda, il silenzio ha preso il sopravvento.  Nelle ultime settimane, però, sono successe alcune cose notevoli nel mondo delle neuroscienze che, insieme, delineano il futuro di questo settore e, a mio parere, anche il modo con cui il genere umano guarderà a se stesso, attraverso ciò che si andrà scoprendo. Innanzitutto c'è stata la vittoria, niente affatto scontata, dello Human Brain Project nel mega bando europeo per un finanziamento di oltre 1 miliardo e 300 milioni di euro a favore della ricerca scientifca del futuro. In questo video di presentazione potete ascoltare dalla voce dei protagonisti la portata della scommessa: ricostruire un cervello umano in un supercomputer, nel quale verranno inserite tutte le informazioni in nostro possesso, dal livello molecolare (neurotrasmettitori e proteine), a quello cellulare (connessioni sinaptiche) fino a quello funzionale, con i collegamenti tra le diverse aree deputate a svolgere le funzioni cognitive superiori. Come ben sanno fisici e chimici, le simulazioni consentono di far emergere gli elementi sconosciuti di un processo, qualsiasi sia la sua natura. I ricercatori del Brain Mind Institute dell'Ecole polythechnique de Lausanne (EPFL), in Svizzera, guidati da Henry Markram, saranno a capo di un progetto che coinvolge decine di laboratori in tutto il mondo, compresa l'Italia. Sono stata all'EPFL qualche settimana dopo l'annuncio della vittoria e vi si respira un'aria di entusiasmo ed eccitazione che non dipende esclusivamente dall'enormità della cifra stanziata quanto dalla sensazione di essere nel posto dove si costruirà un modello nuovo di scienza: una sensazione che avevo provato solo con l'annuncio della scoperta del bosone di Higgs, quando, tra amici e colleghi, ci si consolava dell'incapacità degli \"altri\" di comprendere il perché della nostra commozione e delle ore passate a guardare in streaming quanto accadeva nella sala conferenze del CERN. Nei prossimi anni si raccoglieranno i risultati di questo progetto: saranno sicuramente tantissimi e anche se non emergerà, da quell'ammasso di reti e cavi, un perfetto cervello artificiale o una coscienza sintetica, ci saranno computer concepiti con tecnologie innovative, in grado di reggere l'immane mole di dati, e anche molte nuove scoperte a livello biomolecolare che forse ci aiuteranno a curare le malattie neurodegenerative. Dall'altro lato dell'Oceano, invece, come avrete certamente letto su tutti i giornali, il presidente Obama ha deciso di stanziare 300 milioni di dollari in 10 anni per un progetto analogo e, in un certo senso, complementare: la Brain Initiative. Se lo Human Brain Project vuole conoscere il cervello molecola per molecola, la Brain Initiative si concentra sul suo funzionamento e sul cosiddetto connettoma, ovvero l'insieme di connessioni tra neuroni che li unisce tra loro e permette la comparsa delle funzioni cognitive superiori e della coscienza. Anche in questo progetto ci saranno supercomputer e simulazioni di singoli neuroni, ricostruzioni in 3D del percorso di un pensiero e molto altro. Infine proprio qualche giorno fa, l'American Psychological Association ha rilasciato la nuova edizione del DSM, la quinta. Si tratta del manuale che raggruppa, come in una sorta di inventario, tutte le malattie psichiatriche che affliggono l'uomo (o, almeno, l'uomo occidentale), organizzato sulla base della manifestazioni sintomatiche. Come per tutte le edizioni precedenti, anche questo DSM-V ha suscitato un vespaio di polemiche per via della relativa arbitrarietà della scelta delle patologie che entrano e di quelle che escono, anche se questa volta è stato fatto lo sforzo di aprire, seppure parzialmente, la discussione alla società civile, attraverso un sito web che è stato accessibile per oltre tre anni e grazie al quale è stato possibile interagire con i gruppi di lavoro incaricati di buttar giù la bozza del nuovo manuale. La vera notizia, però, non è questa, ma il fatto che i National Institutes of Health (NIH) statunitensi hanno dichiarato, qualche giorno prima della pubblicazione del DSM-V, che cercheranno di incentivare un nuovo modo di fare ricerca e di classificare le malattie mentali basato non più sull'aggregazione dei sintomi ma sulle caratteristiche biologiche e cognitive dei disturbi. In sostanza, dicono gli NIH, è ora di abbandonare la vecchia concezione di psichiatria basata sulla valutazione soggettiva e fortemente influenzata da fattori culturali, per passare a una nuova psichiatria che si avvicina sempre più alle neuroscienze e che cerca nelle molecole, nei marcatori, nei collegamenti difettosi a livello corticale l'origine dei  disturbi. Verranno così privilegiate le ricerche che supereranno gli schemi del DSM per andare a cercare la comune origine biochimica o genetica di malattie apparentemente lontane tra loro ma magari accomunate dalla stessa manifestazione esteriore (per esempio la depressione o le allucinazioni) che devono per forza, secondo questo modello, essere legate a una comune matrice biochimica. Anche se non mancano le obiezioni, scientificamente ineccepibili e più che sensate, a progetti faraonici come lo Human Brain Project e la Brain Initiative (a partire dal fatto che ognuno di loro genererà 300 exabyte di dati l'anno, ovvero un trilione di byte, all'interno dei quali bisognerà individuare le informazioni interessanti così come si cerca il classico ago nel pagliaio) -  e anche se l'annuncio degli NIH ha messo in allarme coloro che considerano l'ambiente in cui si vive un elemento importante nella genesi di una malattia mentale tanto quanto la biochimica - è ovvio anche per i più strenui detrattori che la ricerca in neuroscienze è alle soglie di una rivoluzione epocale, nel corso della quale le barriere che ancora esistono tra biochimica, processi cognitivi, malattie e intelligenza artificiale andranno a sfumare e forse, un giorno, a scomparire. Per quel che mi riguarda, nell'appiattimento generale che ci circonda, e malgrado i dubbi scientifici che condivido pienamente, l'idea che ci sia ancora qualcuno che non rinuncia a pensare in grande, che ancora crede in folli progetti e nella possibilità di trovare un modello unificatore per spiegare come funzioniamo, mi pare davvero consolante e degno del post numero 100. PS: un ringraziamento ai ragazzi e ai docenti del liceo Enriques di Ostia che, in un pomeriggio di due settimane fa passato a chiacchierare di cervello, coscienza, libero arbitrio, ricerca, fantascienza, cyborg e tante altre cose, mi hanno fatto tornare la voglia di raccontare."},
{"title": "Un sabato pomeriggio alternativo", "text": "Come molti (molte), quando non lavoro, il sabato pomeriggio vado in giro a fare shopping. La cosa mi rilassa e mi permette di dare un'occchiata a come cambia il centro di Milano (questo è un segnale indiretto di quanto poco spesso mi capiti di avere un sabato pomeriggio libero). Domani, invece, dalle 15,30 alle 18,30, in centro a Milano, e per l'esattezza a Palazzo Reale, ci sarò per moderare un incontro promosso da giovani studenti e giovanissimi ricercatori per una Italia unita per la corretta informazione scientifica. Sì, lo so, il nome è un tantino altisonante e pure un po' vetero (prima di me lo ha detto molto bene Silvia Bencivelli, che sarà della partita a Roma), ma mi pareva giusto esserci (non solo perché hanno scelto un cervello come logo   ) e cerco di spiegare perché.  La cosa ha a che fare col mio mestiere, sempre più bistrattato e sempre meno considerato, ma nondimeno, a mio avviso, sempre più necessario. Siamo un Paese dove la scienza è vista o come una sorta di totem onnipotente e inattaccabile, uno strumento salvifico che va accettato acriticamente (e abbiamo i nostri bravi Scienziatoni, come amo chiamarli, che sono i vescovi della nuova religione delle sorti magnifiche e progressive) oppure come il coacervo di tutti i mali, una gang di spocchiosi al soldo di multinazionali che affamano contadini indiani, torturano povere bestie e negano terapie efficaci ai bambini malati. Come spesso ci accade, ci manca l'equilibrio: quello che nasce dalla conoscenza delle cose, dalla consapevolezza dei limiti del metodo scientifico ma anche delle garanzie che offre; dal ruolo che la conoscenza e le scoperte giocano nello sviluppo di una civiltà, purché, e sottolineo questo purché, esista una società e una classe di informatori (giornalisti, divulgatori) che con la scienza dialoghi senza timore di farle le pulci quando serve, ma con in mente il valore intrinseco del metodo scientifico. Quanto di tutto riuscirà a emergere nelle conferenze che domani sono state organizzate un po' in tutta Italia (e di cui parla anche Le Scienze, che è media partner dell'iniziativa), non è dato sapere, ma è un primo passo, anche perché l'idea l'hanno avuta dei giovani, anzi dei giovanissimi, un po' ingenui e anche loro talvolta inbevuti di certezze come si è quando si è giovani e si ama ciò che si fa. Come ha scritto anche Beatrice Mautino, che guida l'incontro torinese, collaborare con iniziative \"nate dal basso\" non è sempre semplice e non lo è stato neanche per me. L'associazione di giornalisti e divulgatori scientifici a cui apparteniamo tutte e tre (Science Writers in Italy) ha patrocinato il tutto non senza discussioni tra i suoi membri su quale dovesse essere il nostro ruolo come giornalisti. Il timore, come sempre, è che il pubblico ci veda come semplici e acritici megafoni degli scienziati, una versione umana di Google translate nella versione gergo tecnico/parla come mangi. No, non siamo questo: specie quando si parla di OGM, sperimentazione animale o ricerca sulle staminali, come faremo a domani, il nostro ruolo è quello di partire dai fatti (e quelli sono dati dalle sperimentazioni condotte a rigore di scienza) inserendoli però nel contesto sociale in cui ci troviamo, altrimenti facciamo della scienza una disciplina asettica e avulsa dalla vita reale, quale certamente non è né deve essere. A Milano, sono previste tre ore di incontro ma gli esperti convocati (Sergio Della Sala, Direttore dell'unità di Human Cognitive Neuroscience dell'Università di Edimburgo e presidente del CICAP; Piero Morandini, Docente in Biotecnologie Ambientali e in Biotecnologie Industriali Vegetali e Ambientali all'Università Statale di Milan; Roberto Giovannoni, Docente in Patologia Generale nel Corso di Laura in Medicina e Chirurgia dell'Università di Milano-Bicocca) occuperanno solo una parte del tempo disponibile. Il microfono sarà a disposizione di chi vuole veramente capire come funziona la ricerca e anche esporre dubbi metodologici o etici, che sono tutti legittimi e meritevoli di risposte, purché nel contesto di un dialogo sereno e costruttivo, come purtroppo raramente accade quando si discute pubblicamente di queste tematiche (e infatti l'idea che sia stato necessario approntare un servizio d'ordine a tutela del sereno svolgimento di un incontro dove si parla di scienza non smette di angosciarmi). Quanto a me, volevo anche raccontare, domani e qui sul blog, quel che la psicologia può dirci per spiegarci le origini e la pervasività del pensiero antiscientifico. Lo farò a voce a Palazzo Reale se ce ne sarà il tempo e l'occasione e sarà l'argomento di uno dei prossimi post. L'incontro di Milano si tiene dalle 15,30 alle 18,30 presso la Sala Conferenze di Palazzo Reale, piazza Duomo 12 ed è promosso dai Gruppi Consiliari Milano Civica x Pisapia Sindaco, Sinistra Ecologia e  Libertà, Radicale - Federalista Europeo e Partito Democratico in  collaborazione con l'associazione Pro-Test Italia e con il Movimento  Milano Civica."},
{"title": "Perché non riesci a crederci? È scienza!", "text": "Quanto sta accadendo in Italia nei confronti della scienza, dall'incredibile autorizzazione alla sperimentazione del metodo Stamina alla votazione sulla normativa anti OGM (per un elenco esaustivo potete leggere il post del direttore), mi ha fatto tornare in mente un curioso esperimento condotto nel 1954 dallo psicologo Leon Festinger. Festinger si era unito ai Seekers, una setta di Chicago, creata da una transfuga dei Dianetics che affermava di comunicare con gli alieni. La leader del gruppo, Dorothy Martin, aveva profetizzato la fine del mondo per la notte del 21 dicembre di quell'anno. Solo i seguaci della setta sarebbero stati salvati e trasportati su astronavi aliene verso un mondo migliore. I Seekers ci credevano fermamente, tanto che molti vendettero le loro proprietà e si licenziarono, aspettando tutti insieme il trasporto stellare che, ovviamente, non avvenne. Festinger era lì con loro, con l'intento di comprendere che cosa accade quando si dimostra a un individuo, con prove concrete, che ciò in cui crede è falso. La reazione fu inaspettata: dopo qualche ora di smarrimento, arrivò qualcuno con un messaggio dagli alieni. Il mondo era stato salvato grazie alla forza spirituale del gruppo. Da quel momento in poi, racconta Festinger in When Profecy Fails (un superclassico della psicologia sociale), i Seekers, che inizialmente non facevano proselitismo, si lanciarono in una frenetica attività di reclutamento di nuovi adepti, ancora più rafforzati nel credo malgrado la prova della sua fallacia fosse davanti ai loro occhi. La loro storia è stata raccontata con molti particolari anche in Unscientific America (uscito nel 2009) scritto da due giornalisti americani,  Chris Mooney e Sheril Kirshenbaum. Dopo Festinger, molti studi di psicologia e neuroscienze hanno dimostrato che i nostri convincimenti preesistenti sono molto più tenaci di qualsiasi dimostrazione scientifica o persino di qualsiasi fatto che avviene davanti ai nostri occhi e ciò sia quando cerchiamo prove a sostegno sia quando cerchiamo smentite. È il cosiddetto “ragionamento motivato” che spiega perché le opinioni comuni riguardo alle questioni scientifiche sono così estremizzate. Il libro di Mooney è pieno di resoconti di esperimenti interessanti che dimostrano, anche alla luce delle neuroscienze, che siamo tutt'altro che esseri razionali. Giudichiamo principalmente con le emozioni e sulla base di pregiudizi: questo perché avere già una “giudizio preconfezionato” sulle cose è un vantaggio evolutivo (almeno secondo alcuni scienziati come Arthur Lupia, che studia le opinioni in politica , secondo il quale applichiamo la strategia del fuggi-o-combatti non solo davanti ai pericoli reali ma anche nei confronti di dati e opinioni, quindi abbiamo bisogno di capire in fretta che cosa pensare di un determinato argomento). È interessante notare che anche una eccessiva fiducia nei risultati scientifici, e una difficoltà a riconoscere la componente emotiva nei giudizi altrui (e quindi a farsene carico, come individui o come società) rientra perfettamente nel novero dei “pregiudizi”. Anch'io possiedo magliette per nerd comprate su siti per scientisti   ma cerco di mitigare la mia indignazione e di chiedermi perché la persona che mi sta di fronte pensa in un certo modo. Attribuire tutto all'ignoranza spesso è fuorviante, perché strettamente parlando non si tratta di individui che ignorano le prove che porto a sostegno del mio modo di vedere: semplicemente, non ci credono.  Le persone, anche quando usano argomenti apparentemente razionali, cambiano raramente idea. In un famoso esperimento del 1979, condotto da Charles G. Lord e collaboratori della Standford University, sono stati portati due finti studi sugli effetti deterrenti della pena  di morte, assolutamente identici dal punto di vista metodologico, a due gruppi, uno pro e uno contro la pena capitale. Ciascun gruppo ha criticato aspramente la metodologia dello studio che arrivava a conclusioni opposte ai propri convincimenti. Poiché nessuna ricerca è perfetta, e c'è sempre qualcosa da criticare, quando cerchiamo prove scientifiche a sostegno o contro un certo argomento dobbiamo stare attenti al bias di selezione psicologica, ovviamente a parità di qualità generale della fonte. Mooney cita anche i lavori di Dan Kahan, uno psicologo forense che si occupa soprattutto di ricerche nell'ambito dei pregiudizi verso le prove scientifiche in tribunale. C'è però un aspetto del suo lavoro che trovo interessante, ed è quello che riguarda i gruppi con una forte convinzione “morale” alla base dei propri pregiudizi. Per morale non si intende solo la religione, ma anche, per esempio, convinzioni con una forte componente etica come quelle ambientaliste o animaliste. In questi gruppi il pregiudizio influenza molto ciò che i membri ritengono essere scientifico, le sperimentazioni che considerano legittime e persino le caratteristiche che deve avere un esperto per essere ritenuto affidabile. È quindi inutile cercare di convincere un attivista antiOGM dell'irrazionalità della sua opposizione alla sperimentazione se alla base del suo pensiero c'è un universo cognitivo (e valoriale) totalmente differente da quello di chi si affida alle prove scientifiche. Continuare a mostrare studi a sostegno dell'innocuità degli OGM non farà che confermare i suoi pregiudizi, derubricare quegli esperti al rango di cialtroni (o venduti alle multinazionali) ed elevare i Seralini di turno a mente pensante di riferimento. Negli Stati Uniti gli studi sui pregiudizi morali sono stati condotti soprattutto in ambito politico e hanno portato a distinguere due “gruppi psicologici”: i più inamovibili sono spesso Repubblicani (quindi di destra) e i più aperti e capaci di accettare anche gli errori e i mutamenti della scienza sono i Liberali (più di sinistra). A mio avviso, questa distinzione in Italia non è corretta e vi sono ampie sacche di simpatizzanti della sinistra che, come si dice in questo post, hanno una visione “morale” di questioni relative alla scienza e quindi sono cognitivamente refrattari alle dimostrazioni fattuali. Questo forse spiega anche perché il nostro Parlamento legifera con la pancia invece che con la testa. Da un punto di vista strettamente psicologico, la difficoltà ad accettare prove scientifiche che vanno contro le proprie opinioni è molto comprensibile: in fondo si tratta di sovrastrutture mentali, ma pur sempre costruite sui “fatti”, includendo in questa definizione anche tutto ciò che un individuo ha letto o osservato nel corso della sua vita. Il clima culturale all'interno del quale si cresce influenza quindi la struttura cognitiva e le opinioni successive con una forza di gran lunga superiore alla cruda evidenza dei numeri. Come se ne esce? A dire il  vero non lo so, come non lo sanno gli esperti che nel corso degli anni hanno cercato di mettere in piedi programmi di alfabetizzazione scientifica della popolazione. Quel che è certo, rileggendo il libro di Mooney in cerca di ispirazione, è che l'approccio top-down, che prevede di “educare” l'interlocutore mediante un lungo elenco di dimostrazioni scientifiche a sostegno di una determinata opinione non serve a nulla. Anzi, come ho detto, è controproducente. Non si combatte Stamina con la dimostrazione della scarsa scientificità dei suoi protocolli. O meglio: la si combatte così sul piano scientifico e normativo, ma non su quello sociale. Servirebbe forse altro: servirebbe la presa in carico della componente emozionale dei movimenti di opinione. Servirebbe scendere dal piedistallo per dire: ok, ho capito il tuo universo cognitivo, ora cerca tu di comprendere il mio. Più facile a dirsi che a farsi. Tutti noi che scriviamo e facciamo scienza sappiamo che esiste una gerarchia tra fatti e opinioni e che non possiamo mettere tutto sullo stesso piano. E non dobbiamo farlo. Ma dobbiamo imparare a riconoscere e a fare i conti con le emozioni, senza derubricarle a robaccia per ignoranti, senza denigrare l'interlocutore (una volta appurata la sua buona fede), come purtroppo ho visto fare da alcuni scienziati nella vicenda Stamina, in particolare nei confronti dei familiari dei bambini malati. Continuare a sventolare l'ultimo trial davanti agli occhi di chi non solo non vuole ma nemmeno può comprenderlo, non serve a granché."},
{"title": "Estrapolatori di dati significativi", "text": "Stavo guardando ieri questo studio uscito molti mesi fa su Neuroimage che identifica diversi substrati neurali coinvolti in compiti matematici. Il primo, che comprende le aree frontali e parietali, sembra essere importante nei compiti di identificazione (per esempio di un certo problema matematico nascosto dietro una sequenza apparentemente casuale di numeri); il secondo, che coinvolge il network striato-talamico è invece fondamentale per estrapolare un nuovo dato significativo da una gran mole di dati forniti. Gli autori, nel commento, ipotizzano l'esistenza di due forme di \"intelligenza\": la prima che ci serve per approfondire e la seconda che invece è particolarmente sviluppata in chi coglie legami e connessioni tra elementi apparentemente lontani tra loro. Per via del lavoro che faccio, negli anni ho sviluppato soprattutto la seconda abilità: durante una giornata vengo in contatto con una quantità impressionante di dati e informazioni, e ho sviluppato una particolare abilità nell'archiviare in qualche angolo della memoria tutto ciò che potrebbe tornarmi utile. Quello che noto, però, è che con il passare degli anni, e soprattutto con la mancanza di tempo, sto perdendo la capacità di approfondire, il piacere di dedicarmi a un unico concetto fino a comprenderlo a fondo, invece di tentare di mettere in connessione quante più informazioni possibili per far emergere qualcosa di nuovo e originale. I due modi di guardarsi intorno e analizzare ciò che ci circonda non sono gerarchicamente ordinati, ambedue sono necessari per far sì che vi sia un progresso (a livello individuale, certo, ma anche a livello collettivo). La sensazione che ho, però, è che il modo che abbiamo di studiare e di lavorare consenta (e richieda) sempre più la presenza di bravi estrapolatori di dati significativi (un po' sul modello dei computer) e sempre meno di intelligenze dedite all'approfondimento verticale delle questioni che ci troviamo ad affrontare.  Personalmente supplisco a questa che sento come una carenza ritagliandomi (quando posso) piccoli spazi di pensiero verticale. Ho bisogno di antidoti alle mie giornate dedicate a sorvolare i prodotti dell'intelligenza umana senza mai potermi fermare e sceglierne uno. È ovviamente il lato negativo della grande fortuna di avere accesso a una quantità un tempo impensabile di informazioni. Il problema, però, è che dovremmo chiederci in che modo tutto ciò sta plasmando le nostre intelligenze e come possiamo fare per conservare il bello del nuovo senza perdere il bello del passato. Ognuno trova la propria soluzione. Io lo faccio con lo studio di testi, o con la musica. Mi piace poter passare di tanto in tanto una serata a leggere anche solo una frase, analizzarne la struttura, la scelta delle parole, la loro posizione reciproca; e poi passare al significato, magari discutendone con altri. Mi sento bene dopo che per due ore ho studiato le stesse 10 battute di una partitura, che però, ogni volta che le ripeti, non sono mai davvero le stesse. Trovare la soluzione a un problema per intuizione è certamente esaltante, è qualcosa che dà una scarica di adrenalina, ma è anche la capacità che perdiamo per prima invecchiando, come noto spesso con i pazienti che mostrano i primi segni di un decadimento cognitivo. C'è un momento in cui la nostra abilità nell'estrapolare dati significativi dal rumore di fondo viene meno, ma rimane intatta più a lungo quella forma più \"lenta\" di intelligenza, quella che approfondisce le questioni e si dà il tempo di comprendere."},
{"title": "Guerre di religione", "text": "Finalmente dopo qualche giorno piuttosto convulso sono riuscita a leggere con calma il numero monografico di Science appena uscito e dedicato alla guerra vista con gli occhi della scienza (un riassunto generale lo trovate in questo articolo di Gianbruno Guerrerio su questo stesso sito). Un compito non è facile perché, come spiega l'editoriale che accompagna la pubblicazione, la guerra è fondamentalmente irrazionale e la scienza cerca di ricondurre i fenomeni più complessi a un disegno logico. L'articolo di Scott Atran e Jeremy Ginges dedicato al ruolo della religione nei conflitti merita un po' più di attenzione, se non altro perché associare il fanatismo alle guerre non mi pare una novita che meriti di finire su una rivista tanto prestigiosa. La revisione, però, (perché di questo si tratta) offre alcuni spunti di riflessione nuovi e propone un nuovo filone di studi. \"La religione, nel promuovere bizzarre credenze e costosi rituali, aumenta la fiducia intragruppo ma può anche aumentare la sfiducia e i conflitti intergruppi\" scrivono nell'abtsract i due autori. \"Divinità moralizzatrici sono emerse negli ultimi millenni, promuovendo la cooperazione su larga scala e le conquiste sociopolitiche anche senza guerra. Che siano al servizio della cooperazione o del conflitto, i valori sacri, come la devozione a Dio o a una causa collettiva, determinano l'identità di gruppo e operano come imperativi morali che ispirano azioni non razionali indipendenti dagli obiettivi auspicabili. In situazioni di conflitto, le pereferenze sociopolitiche che in altri momenti sono parte della sfera del mondano possono diventare valori sacri. I valori sacri sono alla base dei conflitti insanabili che sfidano la negoziazione di tipo commerciale, ma possono anche riservare sorprendenti opportunità di risoluzione\". In sostanza, le religioni sono la benzina dei conflitti, e fin qui niente di nuovo, dal momento che viviamo in un'epoca che ha purtroppo una certa dimestichezza con le guerre di religione. Bisogna però intendersi bene su che cosa si considera \"religione\", spiegano Atran e Ginges, che considerano tali anche fenomeni di \"sacralizzazione\" di valori mondani, come avviene in alcune ideologie che creano, intorno ai propri elementi costitutivi, veri e propri rituali. La stessa nozione di Patria, ovviamente, può essere sacralizzata fino a diventare una \"religione\". Solo una minoranza dei conflitti è riconducibile a una matrice religiosa in senso stretto, ricordano gli esperti. Nella maggior parte dei casi la \"colpa\" è proprio dovuta alla sacralizzazione e ritualizzazione di valori mondani, come è accaduto, per esempio, nella Germania nazista. L'articolo si apre al dibattito sulla natura delle religioni che è oggi, di fatto, polarizzato: da un lato ci sono i sostenitori dell'origine puramente biologica dei fenomeni religiosi (la ricerca del trascendente e la creazione di rituali è determinata dai nostri geni che a loro volta plasmano il fuzionamento del nostro cervello); dall'altro ci sono i \"culturalisti\", per i quali la religione è un sottoprodotto di moduli mentali nati per altri ragioni  e selezionati culturalmente. Per i primi esisterebbe un vero e proprio \"gene di Dio\" che ci porta a credere nell'esistenza di esseri sovrannaturali, per i secondi la religione sarebbe un accidente secondario (e potenzialmente pericoloso) di facoltà cognitive che hanno un'altra funzione. Per esempio saremmo portati a credere nel sovrannaturale per via di un  meccanismo di difesa che ci fa riconoscere in qualsiasi forma vagamente umana o animale un essere dotato di volontà e movimento: sempre meglio spaventarsi per un ramo secco che farsi mordere da un serpente nascosto nell'erba. L'attribuzione a un essere sovrannaturale di pensieri e azioni che sono invece prettamente naturali sarebbe un sottoprodotto cognitivo di questo necessario meccanismo di sopravvivenza. Ugualmente i rituali sarebbero solo sottoprodotti culturali di gesti ripetitivi necessari a cementare la vita sociale. Man mano che i gruppi sociali si ampliano e si trovano ad accogliere individui di famiglie od origini non omogenee, la religione, con il suo corteo di divinità che sorvegliano e puniscono, assolverebbe al ruolo di collante morale. A sostegno di questa ipotesi ci sarebbero, secondo gli autori, gli studi archeologici che dimostrano una coevoluzione tra religione, rituali e società complesse mentre alcuni studi antropologici ci rivelano l'esistenza di piccoli gruppi umani isolati che non hanno sviluppato alcuna nozione di trascendente. In quest'ottica evolutiva, le società multiculturali e aperte a cui oggi siamo abituati sono minacciate con sempre maggiore forza da fondamentalismi che hanno come scopo il ritorno alle origini e il rafforzamento dei legami intragruppo a scapito di quelli intergruppi. Possiamo quindi assolvere le religioni \"classiche\" dalla loro pessima fama di benzina per i conflitti? Non proprio, perché altri studi citati su Science affermano che un forte substrato religioso è un fattore di rischio per l'adesione acritica a una chiamata alle armi. Attenzione, non sempre ne è la causa, ma può diventare quell'elemento che consente all'individuo di superera le barriere dell'autoconservazione e del rispetto per la vita umana in nome di un valore che, agli occhi di altri, appare assolutamente mediocre o perlomeno negoziabile. I conflitti che hanno sacralizzato valori mondani (gli autori citano esplicitamente il conflitto israelo-palestinese che, dando alla terra un valore sacro, rende di fatto non negoziabile l'unico elemento concreto che potrebbe invece portare a una soluzione pacifica) sono i più difficili da risolvere, i più lunghi, i più sanguinosi.  \"La religione cerca di rispondere a problemi di cooperazione in grandi gruppi sociali attraverso agenti sovrannaturali che puniscono chi non coopera; utilizza rituali per forgiare stretti legami di gruppo; si occupa della sacralizzazione di questioni mondane minacciate da dispute tra gruppi\" affermano Atran e Ginges. \"Future ricerche dovranno studiare l'interazione tra questi tre meccanismi. Mentre non vi è un legame necessario tra credenze religiose, valori sacri e guerra, nel corso di conflitti tra gruppi i protagonisti possono trasformare interessi materiali in valori sacri, in seguito consolidati sotto forma di credenze religiose. I valori sacri non sono esclusivi delle religioni: elementi mondani possono essere sacralizzati attraverso rituali che li collegano a valori sacri non religiosi, come la nazione. Nuovi studi potrebbero cercare di capire in che modo le credenze religiose rinforzano i valori sacri, conferendo loro un'origine e un interesse divino. Ciò implica che quando un valore sacro acquisisce un'associazione sovrannaturale, diventa concettualmente ipermeabile alla sfida della ragione, riducendo le possibilità di dissocizione da parte di singoli individui; inoltre cattura l'attenzione, diventa memorabile e ciò lo rende culturalmente contagioso\". Coloro che vivono e fanno propri valori religiosi dovrebbero quindi essere molto consapevoli degli aspetti perniciosi della sacralizzazione, così come una persona sana deve essere consapevole dei propri fattori di rischio nei confronti dello sviluppo di future malattie, per mettere in atto stili di vita sani e adeguati. Fin qui Science, che invoca un nuovo paradigma di studio dei conflitti che vada a guardare, nella mente delle persone, i meccanismi biologici che fanno emergere,  consolidare e diffondersi la sacralizzazione dei valori. Non solo: sarebbe necessario lavorare in modo interdisciplinare per combinare i risultati di studio cognitivo-comportamentali con i dati antropologici ed etnografici e, infine, con le nozioni della psicologia sociale e della psicobiologia per arrivare a un modello unitario della conflittualità sociale che dia una risposta alla domanda che fa da titolo a questo numero speciale della rivista: Perché ci combattiamo? Pur partendo da tutt'altre premesse concettuali e metodologiche, questo articolo giunge a conclusioni sorprendentemente simili a quelle dello psicoanalista junghiano Luigi Zoja nel suo recente saggio \"Paranoia- La follia che fa storia\". In apparenza la tesi di Zoja è opposta a quella dei cognitivisti e vede l'origine dei conflitti in una sorta di lucida follia individuale che trascina con se le folle in un delirio di persecuzione di un gruppo nei confronti del resto del mondo. Eppure mi pare che si stia parlando di un fenomeno analogo: l'assurgere a valore sacro, totalizzante, di un elemento che ha consistenza solo per un certo individuo, un certo gruppo limitato ma che, come un'epidemia, si propaga e diventa resistente al potere bonificatore della ragione. PS: appena ho inziato a scrivere mi è venuta in mente questa bellissima canzone di George Brassens (Mourir pour des idées). Esiste anche la versione in italiano tradotta da De André ma, per una volta, non la trovo tanto efficace quando l'originale. Atran, S., &amp; Ginges, J. (2012). Religious and Sacred Imperatives in Human Conflict Science, 336 (6083), 855-857 DOI: 10.1126/science.1216902"},
{"title": "L'evoluzione della morale", "text": "Qui a Erice, in Sicilia, dove sono per seguire un interessante workshop sull'evoluzione della morale, si incontrano in sala personaggi alquanti curiosi. Ci sono gli etologi, come il ben noto Frans de Waal, che cercano di capire quanto le regole morali siano sviluppate negli animali per arrivare a comprendere se è possibile dare un fondamento biologico alle regole della convivenza civile e se si può tracciare un percorso evolutivo che dagli animali arrivi fino all'uomo.  Ci sono poi i filosofi, come come Philip Kitcher che puntano invece a costruire una nuova filosofia morale basata sulla natura invece che sulla trascendenza. Senza arrivare a posizioni estreme come Dawkins, Kitcher si chiede che cosa può dare un fondamento alle leggi morali nel momento in cui non le attribuiamo più a un essere sovrannaturale. Non che la morale non esista, è un dato di fatto, ma che cosa giustifica, dal punto di vista filosofico, la sua esistenza se non esiste un Supremo Legislatore? La risposta la fornisce in parte un'altra filosofa arrivata fin qui dagli Stati Uniti, Patricia Churchland, che risponde semplicemente: la nostra mente. Le leggi morali sono il frutto dell'harware cerebrale, del modo in cui interpretiamo la realtà e viviamo l'interazione sociale. Sono determinate da neuroni e neurotrasmettitori e questa \"naturalezza\" costituisce il fondamento su cui poggiano. Qui potete vedere una sua presentazione di qualche mese fa che ricalca più o meno quella che ha fatto ieri qui in Sicilia. Quindi per studiare l'evoluzione della morale, cioè analizzarla come semplice prodotto cognitivo, dobbiamo smontarla nei suoi elementi costitutivi, partendo da quelle pulsioni, quei sentimenti, che ne costituiscono i mattoni: altruismo, socialità, tendenza a cooperare, religiosità, capacità di dare un valore alle intenzioni... Proprio sul tema delle intenzioni lavora una giovane e brillante ricercatrice dell'MIT Moral Lab, Liane Young. Se siete curiosi di scoprire come si fa studiare l'intenzionalità di un'azione riprovevole, o come lavora il nostro cervello quando deve attribuire agli altri un'intenzione, potete guardare questo episodio di una trasmissione in cui la Young spiega il suo lavoro. Pensate che la maggior parte dei conflitti tra persone è dovuto a una errata attribuzione di intenzioni alla controparte, e questo fenomeno, secondo la psicologa dello sviluppo Melanie Killen è ancora più pronunciata nei bambini. Lo studio della morale infantile è quello che più mi intriga: è incredibile scoprire che già a tre anni i piccoli umani distinguono (e pongono su piani gerarchicamente diversi)  una norma morale come \"non fare del male agli altri\" e una norma sociale come \"non parlare a scuola senza alzare la mano\". Nel primo caso danno un giudizio molto chiaro: è un'azione riprovevole, non importa se non ti scoprono e non importa nemmeno se l'altra persona non si è fatta male davvero. Anzi, non va bene anche se non esiste una regola esplicita che vieti di fare del male agli altri. Viceversa le norme sociali possono essere violate, l'importante è che nessuno copra e punisca la violazione. È tardi e non riesco ad approfondire le mille questioni suscitate dagli interventi, ma volevo condividere con voi anche alcuni filmati curiosi che sono stati mostrati tra ieri e oggi. Poi ne parliamo insieme nei commenti. La cooperazione tra elefanti per il raggiungimento di uno scopo Frans de Waal dimostra che le scimmie comprendono (e rifiutano) una retribuzione iniqua"},
{"title": "Breivik, Sofri e la follia", "text": "E così alla fine Anders Breivik - l'autore della strage di Utoya, in Norvegia, che ha assassinato a sangue freddo oltre 70 ragazzi in nome della superiorità della razza bianca e di altre follie nazistoidi - è stato condannato alla pena detentiva massima concessa dalla legislazione norvegese: 21 anni. Avrebbe rischiato molto di più se lo avessero dichiarato incapace di intendere e di volere e pericoloso per la società: in quel caso sarebbe finito in un ospedale psichiatrico, probabilmente a vita. Non a caso i suoi avvocati hanno subito annunciato che non si appelleranno alla sentenza: lo avrebbero fatto se fosse stato dichiarato \"matto\", perché colui che ha ucciso per provare la propria supremazia sugli altri non avrebbe potuto tollerare un tale oltraggio.In questo editoriale, uscito sabato su La Repubblica, Adriano Sofri vede in questa decisione una presa di coscienza della società norvegese, come se i giudici avessero voluto dire: il male è dentro di noi, è parte della normalità e non è frutto della follia.  Sarà: io leggendolo sono rimasta alquanto perplessa, non tanto perché non condivida dal punto di vista emotivo quanto sostiene, ma perché leggo, nel suo modo di trattare l'argomento dei crimini e della follia, un errore metodologico importante, che ha ripercussioni anche sul come vorremmo che i giudici pronunciassero le sentenze.Quel che la legge chiede al perito neurologo o psichiatra non è se l'imputato è \"pazzo\" nel senso comune del termine: chiede invece se è capace di comprendere la gravità di ciò che ha fatto e non solo a posteriori ma anche nel momento dell'azione. L'imputato, mentre uccideva, si rendeva conto delle conseguenze che le sue azioni potevano avere per le vittime e per se stesso? Breivik e altri assassini del suo genere sanno benissimo inquadrare le proprie azioni dal punto di vista dei valori morali comuni e persino della legge. Sono in grado di frenare i propri impulsi finché non vi sono le condizioni più adatte alla realizzazione dei loro piani. Sanno che commettendo quei crimini rischiano di andare in prigione, o di morire, e accettano queste conseguenze in  nome di un valore per loro più grande. Sono in sostanza capaci di intendere e di volere, ma non significa che siano necessariamente sani di mente.Non ho letto le perizie effettuate su Breivik ma dai racconti riportati sui giornali sembra chiaramente un caso di personalità antisociale: è sempre vissuto in isolamento, coltivando un personale delirio di grandezza e superiorità. Il suo comportamento, durante e dopo la strage, dimostra che non è capace di empatia e, di conseguenza, di pietà. Il suo eloquio durante il processo, i sorrisi, l'abbigliamento sempre impeccabile, rivelano la sua volontà (e in parte anche capacità) di sedurre l'interlocutore. Ha bisogno di essere curato? Certamente sì, anche se i margini di recupero delle psicopatie gravi sembrano (le statistiche in verità sono poche e lacunose) assai scarsi. In questi casi, anche se è istintivo cercare risposte filosfiche ai dilemmi filosofici, dalla natura del male a quella dell'uomo, bisognerebbe fermarsi un attimo e pensare che la legge non deve rispondere alle grandi domande, ma deve attenersi ai dati oggettivi, tecnici.Non c'è alcun dubbio che Breivik sia un individuo pericoloso, e di questo sono consapevoli anche i giudici che, pur ritenendolo responsabile delle sue azioni, hanno utilizzato un escamotage giuridico, quello della detenzione \"preventiva\", per consentire in futuro ad altri giudici di prorogare la pena e aggirare così il civilissimo limite di 21 anni che la Norvegia ha scelto per punire i propri criminali.Forse nessun legislatore, lassù al Nord, si immaginava di trovarsi un giorno davanti a un personaggio di questo genere. O forse chi ha scritto le leggi pensava, sbagliando, che per trucidare a sangue freddo decine di adolescenti piangenti bisogna per forza essere matti. PS: io, con la pancia, avrei preferito vederlo rinchiuso a vita in un ospedale psichiatrico. Ma è un giudizio emotivo e non razionale. Non sarebbe stato giusto nei confronti di chi è davvero in preda a impulsi non governabili quando commette reati che in fondo non vorrebbe commettere. E non sarebbe stato giusto nei confronti di Breivik, che sconterà i suoi crimini con una pena che i suoi concittadini ritengono appropriata all'efferatezza con cui li ha perpetrati."},
{"title": "Il caso di Como e le neuroscienze in tribunale", "text": "Il caso di S.A., una donna di 28 anni che ha ucciso la sorella e tentato di uccidere la madre e il padre, è finito in questi giorni sui giornali perché il giudice di Como che ha emesso la sentenza ha concesso all'imputata la parziale infermità mentale sulla base di un'accurata e precisa perizia che comprende anche alcune tecniche neuroscientifiche di cui molto si è parlato negli ultimi anni.  Le tecniche usate dai periti e la motivazione della sentenza hanno suscitato dibattito anche all'estero (vedi Nature e New Scientist, tra gli altri, che però si soffermano solo su un aspetto della vicenda, quello dell'uso del neuroimaging, che è tutto sommato marginale). La storia è complessa, così come la perizia che solleva diverse questioni importanti dal punto di vista scientifico, etico e sociale: ora provo a raccontarvi i punti più interessanti, dato che ho in mano tutta la documentazione del processo (perizia, sentenza e trascrizione del dibattimento in aula) e che trovo questo caso affascinante e inquietante allo stesso tempo. La storia: S.A. è arrestata nel 2009 in flagranza di reato mentre tenta di dar fuoco alla madre. Le forze dell'ordine irrompono in casa e riescono a salvare la malcapitata signora perché S.A. è sottoposta a intercettazioni ambientali. C'è infatti il forte sospetto che sia stata lei a far sparire, e poi a uccidere, la sorella, di cui non si hanno notizie da alcuni mesi. Il padre di S.A. possiede una piccola impresa che va piuttosto male: a metterla in cattive acque è stata l'imputata stessa, che ha sottratto somme ingenti ai conti societari all'insaputa della famiglia. Con i soldi rimasti, il padre ha deciso di comprare alla figlia maggiore, sorella di S.A., una casa. S.A. non tollera questa idea e cerca di impedire a tutti i costi che ciò avvenga: riesce a farsi affidare il ruolo di intermediaria tra il venditore e i genitori e per qualche tempo adduce diverse scuse per evitare la transazione. Inventa anche un fantomatico avvocato Frigerio che la assiste nella compravendita, producendo finte mail, documenti firmati eccetera. Dopo qualche tempo, però, l'inganno non regge più: la sorella, allertata dal venditore che non vuole più aspettare, scopre i magheggi di S.A. che quella sera stessa la invita a uscire, la stordisce con barbiturici e la porta in pronto soccorso (salvo poi convincerla a non restare). Infine la sequestra e poco dopo la uccide, bruciandone il cadavere nel giardino dietro casa, dove viene sepolto. S.A. è un'abile manipolatrice: produce finte lettere in cui la sorella si autoaccusa del fallimento dell'impresa familiare e convince i genitori a non sporgere denuncia di scomparsa per ben due mesi. Nel frattempo, però, fa qualche errore che ci svela molte cose sul suo stato mentale: usa il bancomat della sorella per ricaricare il proprio telefonino, ritira dei soldi in banca spacciandosi per lei e arriva persino ad avvertire la vicina di non chiamare i pompieri quando brucia il corpo, affermando di dover eliminare sterpaglie e rifiuti. Infine, dopo due mesi, si reca lei stessa in questura per denunciare la scomparsa. La polizia non ci mette molto a sospettare che qualcosa non quadri: perché una famiglia non denuncia una scomparsa per ben due mesi? Come mai i bancomat sono in possesso di S.A.? Basta interrogare i vicini per scoprire la faccenda del rogo e fare un po' di ricerche per rinvenire il cadavere. È allora che la polizia installa le microspie che permettono di scoprire che S.A. sta tentando di uccidere anche la madre e di avvelenare il padre con psicofarmaci. La donna viene sottoposta a una prima perizia psichiatrica che, sulla base di un paio di colloqui piuttosto frettolosi, propende per la parziale infermità mentale, senza fornire però una diagnosi specifica. Il tribunale chiede quindi una ulteriore perizia. E un secondo psichiatra, sulla base di un'altrettanto frettolosa valutazione, fornisce un parere opposto: S.A. è sana di mente, è isterica e ha una personalità istrionica, ovvero finge. Finge in particolare di non ricordare la notte del crimine. È a questo punto che la difesa chiede una terza perizia e questa volta la affida a due professionisti ben noti, che già hanno svolto questo tipo di compito in precedenza in un paio di casi che hanno fatto scuola. Si tratta di Giuseppe Sartori, psicologo dell'Università di Padova, e Pietro Pietrini, psichiatra, docente di biochimica a Pisa ed esperto di genetica comportamentale. La perizia di Sartori e Pietrini è veramente ben redatta e ben scritta, e anche il giudice, nella sentenza finale, non può non farlo notare: è una delle ragioni per cui, malgrado ciò non sia la prassi, fa propria la tesi della difesa in contrasto con le conclusioni del perito dell'accusa. Lo staff di Sartori e Pietrini sottopone S.A. a nove colloqui ed effettua un preciso lavoro di riscontro tra ciò che l'imputata racconta e ciò che è noto dall'indagine per verificare quanto la donna è attendibile (cosa che gli altri periti non hanno fatto, bevendosi tutte le bugie come vere). La valutazione psichiatrica non si limita al colloquio, ma utilizza diverse scale validate (compresa quella per la sociopatia di cui abbiamo già parlato su questo blog) oltre a test proiettivi come il Rorschach. A tutto ciò i periti affiancano una valutazione neuropsicologica completa che dimostra come S.A. abbia difficoltà di memoria e soprattutto di planning e di valutazione del rischio. Sottoposta per esempio all'Iowa Gambling Test (un test in cui il soggetto viene invitato a partecipare a un gioco dal quale si traggono vantaggi sul lungo termine se si accetta di perdere nell'immediato), l'imputata dimostra di non avere nessuna capacità di rimandare nel tempo la ricompensa: pensa solo al beneficio immediato, il che è assolutamente congruente con comportamenti apparentemente assurdi che hanno contribuito a incastrarla, come l'uso delle carte di credito della sorella. Al test di Hayling,  che misura l'impulsività di un soggetto, S.A. appare incapace di controllarsi. A prove di teoria della mente (test che richiedono di interpretare le emozioni e i pensieri altrui), la donna ottiene risultati al di sotto della norma, perché manca di empatia. Non è in grado di classificare sulla base della gravità alcune violazioni di comuni norme morali. I due periti concludono quindi che è affetta da psicosi dissociativa e che è legittimo riconoscerle un parziale vizio di mente. Per corroborare la propria diagnosi aggiungono però altri tre esami. Dato che S.A. afferma di non ricordare nulla del crimine, la sottopongono allo IAT (Implicit association test): si tratta di una tecnica che, misurando i tempi di reazione di un soggetto di fronte a un'affermazione, è in grado di discriminare con una buona precisione (intorno al 92 per cento, affermano i periti stessi) se esistono, nei confronti di quella stessa affermazione, dei meccanismi di difesa. Lo IAT è nato per studiare le reazioni di piacere e disgusto, oppure i pregiudizi impliciti, quelli che non ammettiamo neanche a noi stessi. In questo caso è stato utilizzato per uno scopo ancora sperimentale: verificare se una certa informazione è codificata nel cervello di AS come traccia mnesica oppure no. In pratica i periti \"leggono\" nella mente dell'imputata e sono in grado, a loro avviso, di confermare che quando afferma di non ricordare i particolari del crimine, essa dice la verità, perché non c'è alcun rallentamento nei suoi tempi di reazione (un rallentamento che sarebbe lecito aspettarsi se mentisse scientemente). Questo test, afferma la stessa sentenza, è fondamentale per convincere il giudice del fatto che S.A. ha davvero una amnesia dissociativa e non è una simulatrice. L'imputata viene anche sottoposta a una risonanza magnetica per verificare la morfologia del suo cervello: risulta avere una riduzione del volume del cingolo anteriore, un'area importante per il controllo degli impulsi. Infine viene sottoposta anche a un'analisi di genetica e risulta portatrice di una variante di tre geni - quello per la serotonina, quello delle monoaminossidasi e quello per il metabolismo delle catecolamine - noti in letteratura perché associati a un aumento del rischio di comprotamenti violenti. Questa è la storia. Vediamo ora quali sono, a mio avviso, i punti critici, premettendo però che, per quel che si comprende dalle carte, non c'è alcun possibile dubbio sul fatto che S.A. sia davvero psicotica e quindi, immagino, che sia giusta la decisione del giudice di riurre la pena da 30 a 20 anni per vizio di mente (di cui almeno 3 da scontare presso un istituto psichiatrico carcerario). Non ci sono errori nella sostanza, ma forse ci sono alcuni punti sui quali è bene che si rifletta, a livello scientifico, e altri su cui è importante che si discuta a livello sociale, per le ricadute che possono avere su noi tutti e sul modo con cui interpretiamo il comportamento umano. Innanzitutto questa è una perizia ridondante: bastava una buona valutazione psichiatrica e una buona valutazione neuropsicologica per giungere a conclusioni altrettanto attendibili. Perché i periti sono andati avanti sottoponendo l'imputata a tutte queste nuove tecniche? C'è probabilmente una ragione pratica (rafforzare la tesi della difesa contro quella dell'accusa) ma anche, conoscendo i due periti, che ho intervistato più volte in precedenza, per accreditare e validare questi nuovi strumenti che sono oggetto di un acceso dibattito, specie negli Stati Uniti. Cominciamo con lo IAT: in questo caso si può dire che il test non è nato, né è stato validato, per l'uso che se ne fa in questo caso. In particolare non ci sono studi attendibili sull'uso dello IAT negli psicotici, che hanno notoriamente una maggiore facilità a mentire. Ci sono alcuni studi su popolazioni con scarsa empatia, ma sono stati considerati metodologicamente deboli. In che modo questo può influenzare i risultati del test? E anche quando il test è attendibile, lo è al al 92 per cento: una percentuale elevata, certo, ma non sufficiente per ammettere altri test (come la famosa macchina della verità che si vede nei film) come prova nei tribunali. Ancora più delicata è la questione della morfologia cerebrale rilevata con tecniche di imaging. Al momento non conosciamo le \"misure normali\" delle diverse parti del cervello, né sappiamo con assoluta certezza se a un minore volume corrisponde sempre una funzionalità alterata. Deduciamo che un paziente ha una diminuzione di volume in una certa area su base puramente statistica. In questo caso i periti hanno confrontato le misure di S.A. con quelle di un gruppo di 10 donne di analoga età e caratteristiche. Il dato è significativo, ma finché non si avranno indicazioni precise sulle misure normali in un ampio campione di popolazione (con le relative deviazioni standard), l'uso di queste tecniche in tribunale è assai controverso (e in effetti negli Stati Uniti, dove sono state usate per la prima volta, spesso vengono rifiutate dai giudici). Infine c'è l'aspetto genetico: i tre geni alterati sono correlati a comportamenti violenti quando il soggetto viene sottoposto, in età infantile, a maltrattamenti. Potete avere informazioni più dettagliate sulla genetica comportamentale ascoltando questa intervista che ho fatto un annetto fa a Nita Farahany, una giurista americana che si occupa proprio dell'argomento. Nel caso di S.A. si possono identificare delle disfunzionalità nella famiglia, ma non c'è traccia di abuso e maltrattamenti in senso stretto, almeno non nelle carte del processo. Ed è bene ricordare che l'interazione tra genetica e ambiente è tutt'altro che lineare: ci sono soggetti portatori di forme mutate dei geni e maltrattati nell'infanzia che sono bravissime persone, perché le esperienze di vita sono molteplici e possono modulare l'espressione dei geni più di quanto si creda. In conclusione, ho scritto questo lungo post perché il caso è esemplare per molte ragioni: perché la perizia è stata condotta con tutti i metodi disponibili, anche quelli che, secondo me, forse sarebbe utile tenere, al momento, ancora nell'ambito degli studi sperimentali; perché il giudice dice chiaramente, nella sentenza, che la psichiatria \"classica\" non è più in grado di dare risposte soddisfacenti alle esigenze della legge, fondamentalmente perché basa le sue conclusioni su impressioni soggettive invece che su prove scientifiche (ma questo, a mio avviso, non è vero: basta che la perizia sia fatta bene, il che non è avvenuto con i primi due periti interpellati); perché il giudice ha sentenziato la pericolosità sociale  di S.A., il che probabilmente è vero, ma ci si può chiedere quanto è stato influenzato in questo dalle prove biologiche (e quindi quanto, anche a livello implicito, questi strumenti contribuiscano a creare una visione deterministica del comportamento criminale). Credo molto nelle potenzialità delle tecniche neuroscientifiche. Possono dirci molte cose sul comportamento (non necessariamente su quello umano ma, per esempio, su quello animale) e sono utilissime in ricerca: in questo caso specifico convergono tutte su una conclusione che non appare contestabile. Ma cosa accadrà se, grazie al successo ottenuto in questo e in altri casi analoghi, qualcuno deciderà di utilizzare lo IAT, l'imaging o la genetica su soggetti che non hanno patologie psichiatriche dimostrabili in altro modo, oppure per determinare la responsabilità di un individuo in assenza di altre prove convergenti? Come possiamo evitare che nei tribunali si affermi una visione riduzionistica del comportamento? E, infine, come si può tradurre in modo comprensibile per i giudici i termini del dibattito scientifico affinché utilizzino questi strumenti con tutta la consapevolezza possibile? PS: per comprendere meglio la discussione in ambito legale vi segnalo anche questo post di Barbara Bottalico, una dottoranda di ricerca in neuroscienze e legge. Permettetemi anche di ringraziare gli altri membri del gruppo italiano della European Association for Neuroscience and Law presieduta dal giudice Amedeo Santosuosso per l'utile scambio di opinioni."},
{"title": "Lifting cerebrale", "text": "La scuola delle mie figlie comprende tutti gli ordini scolastici, dall'asilo fino al liceo: così la mattina, al bar lì davanti, mi capita di chiacchierare con genitori di ragazzi grandi. In questo periodo il tema ricorrente è quello della stanchezza: i ragazzi sono stanchi, la scuola è impegnativa, i corsi doposcuola ancor di più e così bastano due mesi a pieno ritmo per far emergenre difficoltà di concentrazione, sonnolenza e, dicono i genitori, difficoltà di memoria e apprendimento. In genere ascolto perplessa, memore della mia esperienza americana, questa estate. Uno degli argomenti del mio corso è stato proprio il brain enhancement, il potenziamento mentale. Quel che da noi è solo un accenno, una preoccupazione dei genitori a cui si pone rimedio con un buon sonno e la classica bistecca (o la massimo con qualche beverone alla caffeina e taurina), lì è diventato un fenomeno sociale inquietante e pervasivo. Il nostro corso poteva contare sulla presenza di una diciassettenne: d'estate, i liceali più ambiziosi fanno i volontari dando una mano nell'organizzazione dell'università in cui aspirano entrare e guadagnano così crediti e corsie preferenziali. Ho notato che Maryann, la ragazza in questione, era davvero colpita dalle lezioni in cui parlavano dei farmaci per il potenziamento delle prestazioni mentali, e li conosceva tutti. Secondo alcune indagini, i ragazzi (ma non solo) usano soprattutto il famoso Ritalin, o metilfenidato, uno psicostimolante prescritto per l'ADHD, il disturbo da iperattività e deficit attentivo; qualcuno però usa anche amfetamine o persino i farmaci per l'Alzheimer, che agiscono sul sistema colinergico. Parlavamo spesso anche dell'abuso che se ne fa, anche se i nostri docenti lamentavano l'assenza di dati attendibili sulla loro diffusione. Non è tanto preoccupante il fatto che si usino sostanze per essere più concentrati o studiare meglio, sostenevano, ma lo sono i danni che queste possono provocare a lungo andare. Abbiamo chiesto a Maryann, e la risposta è stata disarmante: \"Il mio è un liceo molto competitivo, so che circa il 40 per cento degli studenti ne ha fatto uso, ma credo che in realtà siano molti di più. Il problema è che senza è molto difficile riuscire a preparare un esame in 48 ore\". La scuola statunitense, per come me l'hanno descritta, è una sorta di implacabile tritacarne dal quale bisogna per forza uscire vincitori, perché ne va del proprio futuro. Le modalità di misurazione delle prestazioni sono molto contestate, un po' come da noi sono contestati i test di ammissione all'università: non sono costruiti per selezionare i più curiosi o i più intelligenti, ma solo quelli con le migliori capacità di memorizzazione. Ora scopro che la Comunità Europea, nell'ambito del 7 Programma quadro, ha lanciato un bando che ha come tema proprio il brain enhancement. Non mi pare che sia ancora un problema diffuso o sentito in Europa, ma evidentemente vogliono portarsi avanti informando il pubblico prima che sia troppo tardi. La sensazione che ho avuto è che dopo l'ubriacatura per la prestanza fisica, con l'esplosione della medicina estetica, delle palestre e delle diete preventive, oggi si viva nel costante timore di non essere più in grado di mantenere sempre al massimo la performance mentale.Se gli Stati Uniti sono il luogo in cui si manifestano precocemente i fenomeni sociali che poi sbarcano da noi, siamo alle soglie di una vera invasione.Ogni supermercato, ogni drugstore americano ha il suo \"neurocorner\", un reparto di prodotti che promettono meraviglie per il cervello.C'è anche una marca di acque minerali dalle proprietà \"neuromodulatrici\": le vedete qui sotto, e le vendono anche alla mensa del campus. Ne ho assaggiate un paio, e non ne ho tratto grandi benefici, credo.  Ci sono poi anche prodotti venduti on line che promettono meraviglie per il cervello (qui sotto trovate una pubblicità, tra le molte che mi sono arrivate nella casella di posta, che vanta la \"scientificità\" e l'efficacia di un preparato rispetto ai suoi concorrenti). Uno dei più venduti è questo:  Si tratta di un integratore alimentare per la concentrazione. Niente di molto diverso dal vecchio \"fosforo\" che andava di moda quando ero piccola, ma impacchettato in modo nuovo, con un preciso riferimento al potenziamento cerebrale. Capsule simili si trovano persino dalle estetiste e nelle SPA più eleganti su Park Avenue a New York, e ogni prodotto è \"garantito\" dal volto e dal sorriso plastificato di qualche eminente neurologo. Prodotti probabilmente innocui, ma che danno l'idea di un clima culturale che rende estremamente facile (ed eticamente accettabile) ricorrere ai farmaci veri, quelli che citavo prima, per riuscire a reggere la pressione e restare tra i vincenti. Con la differenza che quelli, invece, hanno fior di effetti collaterali, specie se a prenderli sono adolescenti che non conoscono nemmeno i dosaggi corretti. Raccontavo tutto questo agli altri genitori al bar, e intanto pensavo che più che mettere in guardia le persone sui rischi che corrono a pretendere dal proprio cervello prestazioni per le quali non è costruito, è sul concetto di performance mentale che dovremmo riflettere. Ci sono tante cose che possiamo fare per mantenere sano il nostro organo più importante, cose molto semplici, che riguardano gli stili di vita, l'alimentazione e l'attività fisica: eppure l'idea di una pillola magica che si faccia diventare degli intelligentoni ci pare una scorciatoia accettabile, quando non necessaria."},
{"title": "Di crisi, parole e auguri", "text": "Ho appena buttato via un post sulla psicologia della crisi, dopo aver passato giorni a riguardare tutto quanto eminenti psicologi e sociologi hanno pubblicato su come noi umani reagiamo alle avversità. Dopo averlo scritto mi sono detta che non è sano chiedersi sempre “e ora come faremo?”. In qualche modo faremo, credo, perché così han fatto generazioni e generazioni di esseri umani prima di noi. Quindi non avrete un post su come affrontare le ansie da spread ballerino. Ne avrete invece uno natalizio sul potere delle parole e delle passioni.  Sarà che l’altro giorno ho incontrato la signora Matilde. È ospite di una residenza per anziani, ha 81 anni e l’Alzheimer. E scrive.Me l’ha detto la figlia, che ha voluto farmi controllare gli esami: con l’occhio tecnico dovrei dire che la situazione è piuttosto deteriorata. La cosa sorprendente, però, è che Matilde, appassionata di letteratura e, pare, un tempo abile scrittrice di racconti famigliari, si mette tutte le mattine al tavolo e scrive: lunghi racconti dadaisti, di cui non si comprende la trama ma dai quali emana comunque un certo fascino. La scrittura è la sua terapia, il suo mondo e, credo, anche quel che resta della sua personalità precedente la malattia. Sono due giorni che non riesco a pensare ad altro che a lei: perché per me la scrittura, che un tempo è stata una passione, un piacere, è diventata uno strumento di lavoro. Un lavoro sempre più convulso (e sempre menovalutato), che mi obbliga a buttar giù migliaia di battute quotidiane.Io che amo le parole, che provo piacere nel scegliere un termine piuttosto che un altro, mi ritrovo a dover cambiare stile come si cambia d'abito, per adattarlo al committente. E se questo esercizio può essere talvolta stimolante e interessante, spesso è solo frustrante e noioso. Raccontare è per me un istinto naturale: l’ho sempre fatto e forse questo spiega anche come mai sono finita a fare il mestiere che faccio, pur volendone fare un altro. E amo molto ascoltare i racconti degli altri, il modo peculiare che ognuno ha di mettere insieme le parole. O di non metterle, il che suscita in me allarme: spesso chi è avaro di parole e anche un po’ avaro di sé, perché gli esseri umani sono fatti per comunicare. Come diceva Moretti, e come mi ha ricordato Matilde, le parole sono importanti. In questo periodo sto traducendo un libro: è un altro esercizio interessante di mimesi linguistica, che mi obbliga a entrare nella testa e nello stile dell’autore. È anche un esercizio di umiltà perché sono tante le volte in cui mi dico che quella cosa, io, la scriverei diversamente, in modo più sintetico, più efficace. Ma quelle parole non mi appartengono e non posso modificarle, anzi. Devo trovare nella mia lingua una esatta corrispondenza con quelle che l’autore ha scelto. C’è della creatività anche in questo, qualcosa di simile al lavoro del musicista che interpreta note scritte da altri. E per tornare al tema che doveva essere quello di questo post, ovvero i modi con cui affrontiamo l'ansia della crisi, Matilde mi ha anche regalato una ricetta per questi tempi incerti, pieni di fatica e timori. Nella lista dei buoni propositi per l’anno nuovo (che la maggior parte di noi esprime, e anche su questo ci sono un tanti studi che stasera non citerò perché questo è un post di digressione e all’evidence based ci pensiamo domani) ho inserito anche “scrivere per passione e non solo per dovere”. Mi porto avanti e comincio con questo post decisamente fuori stile . Un anno in cui scoprire nuove passioni, e coltivarle: è l'augurio che faccio a tutti voi che mi leggete e spesso mi scrivete (anche in privato) raccontando pezzi di vita, scoperte e curiosità. Quanto a me, forse nel 2012 scriverò finalmente il libro che ho nel cassetto da anni. E magari imparerò a suonare il violoncello: chissà…"},
{"title": "Psicologicamente agli antipodi", "text": "Uomini e donne hanno tratti di personalità totalmente diversi e solo nel 10 per cento dei casi c'è  sovrapposizione tra i sessi. Così afferma la ricerca pubblicata in questi giorni su PlosOne da Marco Del Giudice, ricercatore del Laboratorio di biologia del comportamento sociale dell'Università di Torino. Potete leggere una sintesi sommaria del lavoro in questo articolo uscito oggi su Repubblica, anche se la parte a mio avviso più interessante è quella metodologica.  Qualche premessa: la faccenda della differenza di comportamento tra i sessi è spinosa, come potete ben immaginare, perché considerata potenzialmente pericolosa per i diritti che le donne hanno conquistato nell'ultimo secolo. Inoltre non mancano studi che dimostrano quanto la pressione sociale nei confronti delle donne e degli uomini è in grado di determinarne le prestazioni (per esempio accrescendo la fiducia nelle proprie capacità relazionali nel caso delle donne e in quelle logico-razionali nel caso degli uomini). Tanto che, come spiega anche Del Giudice nel suo lavoro, la questione è stata considerata più o meno archiviata nel 2005 con la pubblicazione su American Psychologist di una metanalisi di gran parte degli studi esistenti sulle differenze tra i sessi a opera di Janet Shibley Hyde e intitolata \"The gender similarities hypothesis\". Secondo la Hyde, una volta messi insieme tutti i dati disponibili, le differenze sono poco importanti e, come afferma l'autrice nella premessa allo studio, non vanno esaltate in quanto ciò provoca effetti negativi sia nelle relazioni personali sia nell'ambiente di lavoro. La metanalisi di Hyde comprendeva però sia studi prettamente personologici (cioè che consideravano tra le variabili i tratti di personalità - estroversione, gradevolezza, apertura al nuovo e molti altri - così come codificati da diverse teorie psicologiche come quella sui Big five, i cinque tratti fondamentali) sia studi cognitivi, che valutavano la differenza nelle perstazioni dei due sessi in determinati compiti (linguistici, matematici eccetera). Lo studio di Del Giudice, invece, si concentra sulle variabili personologiche e non entra nel merito delle singole funzioni cognitive. Questo è uno dei suoi pregi, perché si tratta proprio di due questioni a mio avviso molto diverse, sebbene si influenzino l'un l'altra: la personalità può essere plasmata in modo importante dal contesto sociale - e dal diverso ruolo che biologia ed evoluzione hanno delinetao per i due sessi - mentre a mio avviso le funzioni cognitive sono meno \"malleabili\" e più legate, per semplificare un po' brutalmente, a un hardware comune, che è il nostro cervello. Certo, rimane sempre aperta la questione di quanto gli ormoni che circolano in proporzioni diverse nelle nostre vene ed arterie influenzino a loro volta le strutture cerebrali, ma non vi sono studi conclusivi in merito. Su questo punto, però, mi piacerebbe che Marco Del Giudice, che legge spesso questo blog e talvolta commenta, venisse a dire la sua, perché può anche darsi che questa mia impressione dipenda da un pregiudizio legato al tipo di formazione che ho ricevuto. Peraltro ne abbiamo già parlato insieme in occasione di un articolo uscito un annetto fa su Mente e Cervello e che magari ripubblico qui come post, tra qualche giorno. Come mai, però, nessuno studio precedente aveva mai riscontrato una differenza così marcata tra uomini e donne? Secondo gli autori, dipende ancora una volta dalla metodologia utilizzata. Innanziatutto hanno somministrato un lungo questionario, il 16PF, che comprende 16 tratti personologici principali invece dei Big five usati più comunemente. Hanno un campione di oltre 10.000 soggetti e hanno utilizzato metodi statistici raffinati per non perdere nell'analisi le differenze più sottili, che sono ovviamente molto importanti in un costrutto multidimensionale qual'è la personalità individuale. Se ci si limita a fare una media brutale dei risultati che si ottengono ai test, si aumenta la percentuale di sovrapposizione tra i due sessi. Nel caso dello studio in questione, anche eliminando il tratto di personalità che più sembra pesare nel determinare la differenza tra i sessi (qual è? la sensibilità, ovviamente  ) si ottiene solo un 24 per cento di sovrapposizione di personalità tra uomini e donne, il che in psicologia costituisce comunque una differenza enorme. Ora Del Giudice e collaboratori propongono di applicare le loro linee guida metodologiche anche ad altri studi sulla differenza di genere in psicologia, come per l'appunto quelli sulle funzioni cognitive. Se si dovesse dimostrare una differenza altrettanto marcata in abilità primarie come il calcolo, per esempio, sarebbe un bel guaio   (ricordate il putiferio scatenato dall'incauto rettore di Harvard che dichiarò le donne meno portate degli uomini per la matematica?). PS: vi state chiedendo in che cosa siamo così diversi? vabbé, ve lo dico: noi fanciulle siamo più calorose, più attente agli altri, gli uomini sono più aggressivi; le donne sono più sensibili, gli uomini più disinibiti; le donne sono più vivaci e allegre, gli uomini più sospettosi... Ok, ho fatto una selezione un tantino tendenziosa dei risultati  . Del Giudice, M., Booth, T., &amp; Irwing, P. (2012). The Distance Between Mars and Venus: Measuring Global Sex Differences in Personality PLoS ONE, 7 (1) DOI: 10.1371/journal.pone.0029265 "},
{"title": "Ma quanto mi ami?", "text": "Lo so, non ha nessun valore scientifico: è un gioco un po' furbo e un po' burlone per far parlare del Centro di risonanza magnetica funzionale dell'Università di Standford. Mettere in una risonanza magnetica sette persone a casaccio - giovani, vecchi, donne e uomini - e chiedere loro genericamente di pensare all'amore (ciascuno il proprio, con le caratteristiche scelte individualmente) non è propriamente un esperimento controllato. E tra l'altro San Valentino è appena passato e mai mi è venuto in mnte di celebrarlo. Però questo filmino sulla prima competizione d'amore certificata dall'imaging cerebrale mi è capitato sott'occhio solo stasera perché un amico l'ha postato su Facebook. Nella gara, vinceva chi riusciva ad attivare più aree cerebrali al solo pensiero dell'essere amato (ma c'è anche una ragazza che sperava di arrivare prima meditando sull'amore in generale, povera illusa). Se ci penso, un po' mi fa arrabbiare, perché è un ottimo esempio di quella spettacolarizzazione delle neuroscienze sulla quale sempre si discute. E chissà quanta gente, domani, sarà convinta di poter misurare la passione e i sentimenti del compagno mandandolo a farsi un giro in neuroradiologia. Il mio inguaribile lato romantico, però, non può fare a meno di trovare bellissimo il cervello del vincitore, con tutti quei voxel attivati al solo pensiero della donna conosciuta quarant'anni prima. Cosa non darei, per un cervello così   The Love Competition from Brent Hoff on Vimeo."},
{"title": "L'autismo e la sanità secondo Binetti e colleghi", "text": "La vicenda della conferenza stampa che si è tenuta ieri alla Camera dei deputati ha del paradossale. Parlamentari di tutti gli schieramenti, spalleggiati da associazioni di familiari di pazienti e da alcuni operatori hanno protestato contro le nuove linee guida per il trattamento dell'autismo emesse a fine 2011 dall'Istituto Superiore di Sanità, affermando che peccano di parzialità. In che senso? Lo dice la stessa Paola Binetti, deputata Udc, membro della commissione Affari Sociali della Camera e neuropsichiatra infantile: \"Basandosi, per la realizzazione della Linea guida, solo sulla revisione  della letteratura scientifica e quindi su criteri di medicina basata  sull’evidenza, l’Istituto Superiore di Sanità non ha tenuto conto di molte altre iniziative che,  anche se non validate a livello scientifico, hanno dato interessanti  risultati. Se da una parte c'è  l'esistente, codificato nelle Linee guida, dall'altra parte c'è la  sperimentazione, che ha altrettanta importanza. Non chiediamo al  Ministero di sostituire questo modello con un'altra formula, chiediamo  solo di mantenere la ricerca scientifica molto aperta e di valorizzare  le esperienze che hanno dato risultati clinici anche se non hanno ancora  risultati di letteratura scientifica”.  In sostanza, dicono i nostri deputati, un istituto pubblico il cui ruolo è di orientare gli interventi sanitari sulla base di ciò che si è scientificamente dimostrato efficace ha sbagliato perché si è attenuto alle sole prove scientifiche. È un paradosso solo per chi non conosce la travagliata storia della diagnosi e della cura dell'autismo, solo recentemente riconosciuto per quello che è: una malattia con basi biologiche, a eziologia multifattoriale ma con una forte componente genetica, il cui deficit di fondo è una incapacità di comunicare (non solo con gli altri esseri umani, ma in generale con l'ambiente e persino con se stessi). Prima di questa presa d'atto, l'autismo è stato attribuito alle cause più fantasiose, dalle famose \"madri frigorifero\" così ben descritte da Bruno Bettelheim nel suo libro \"La fortezza vuota\" (prima che lo stesso Bettelheim fosse accusato di maltrattamenti e manipolazioni dei suoi pazienti) a tossicità da metalli pesanti oppure da vaccini (lo studio che denunciò la relazione tra autismo e vaccino antimorbilloso è una delle maggiori frodi della medicina ed è stata definitivamente smentita solo nel 2011 grazie al lavoro del giornalista scientifico Brian Deer). Inoltre l'innegabile peso psicologico e pratico della presa in carico di un bambino autistico ha fatto fiorire ogni genere di terapia di supporto, molte delle quali mostrano certamente una qualche efficacia, ma mai sottoposte a uno studio di confronto che dica se è davvero l'intervento in sé a essere utile o più in generale la presa in carico. Come ha scritto un amico sulla mia pagina Facebook commentando questa vicenda, se chiediamo a un genitore come sta suo figlio, e questo ci risponde \"meglio, siamo andati a passeggiare nel parco\", non possiamo ritenere che le passeggiate nel parco siano una terapia, anche se sono indubbiamente utili. Gli unici interventi che sono stati studiati alla luce della scienza e che hanno mostrato una qualche efficacia sono quelli di tipo cognitivo-comportamentale (oltre a qualche intervento farmacologico su soggetti selezionati). A questo si è attenuto l'Istituto Superiore di Sanità nel riassumere ciò che davvero serve. Un atteggiamento che i deputati hanno definito addirittura \"quasi discriminatorio\". Potrà non piacere, ma alla luce di ciò che è stato prodotto nella letteratura medica, molti interventi -  quelli dietetici (basati sulla presunzione che vi sia un legame tra intolleranze alimentari e autismo), l'uso di integratori, le sedute di ossigeno iperbarico, le terapie basate su stimoli acustici con frequenze simili alla voce umana, la musicoterapia e altro ancora, tra cui probabilmente tutti gli interventi psicodinamici che le linee guida nemmeno prendono in considerazione - non servono oppure, prima di essere offerti a tutti i pazienti, andrebbero validati con appositi studi. È vero, come dice l'onorevole Donato Mosella (Api) che a volte le prove non ci sono perché non ci sono i fondi per fare ricerca, ma non è accettabile che i deputati contestino la legittimità di una linea guida che sulle prove scientifiche deve necessariamente basarsi. È un precedente pericoloso perché confonde i piani: gli altri interventi sono probabilmente da annoverare tra quelli \"di conforto\". Non che siano inutili, anzi: fornire assistenza, anche psicologica, insegnare a questi bambini a esprimersi anche attraverso strumenti artistici o altre cose simili è assolutamente necessario. Ma non è una cura in senso stretto: fa parte di quel supporto sociale che un welfare in asfissia sta eliminando pian piano, nell'autismo come in altre patologie con un grave peso sul caregiver come l'Alzheimer. Per non parlare delle scuole, dove i bambini autistici che avrebbero bisogno di un sostegno full time vengono affidati per poche ore a settimana a insegnanti che non sempre conoscono davvero il metodo di riabilitazione cognitiva scelto dalla famiglia, il che è invece un presupposto indispensabile alla sua efficacia. Per comprendere questa protesta bisogna leggere tutta la cronaca della conferenza stampa, fino al punto in cui si dice che ora le Linee guida andranno alla Conferenza Stato-Regioni. Serviranno, in sostanza, a determinare che cosa il Sistema sanitario nazionale pagherà per aiutare i bambini autistici e le loro famiglie e che cosa invece non verrà pagato. È giusto che sia così se si parla di salute. È meno giusto se parliamo di welfare, quindi di una presa in carico sociale della globalità del problema. Non è però delegittimando gli strumenti della scienza che riusciremo ad aumentare i fondi che lo Stato destina alle categorie più sfortunate. In compenso rischiamo di far passare l'idea che qualsiasi cura debba per forza gravare sulla collettività, anche quando non c'è una dimostrazione della sua efficacia. Una prassi che non aumenterà certo i fondi per l'assistenza, semmai il contrario."},
{"title": "Doha/WCSJ 2 - Da Tahrir a taamir", "text": "Ci sono momenti, anche in congressi di aggiornamento professionale come quello che si tiene qui a Doha, in Qatar, in questi giorni, in cui senti l'emozione che ti sale alla gola. Come oggi, durante la conferenza della pausa pranzo in cui ha parlato Adel El Zaim, esperto di nuove tecnologie ma soprattutto del loro impatto sociale e del ruolo che giocano nel rendere il mondo piu' democratico. El Zaid e' di origine libanese e vive tra il Canada e l'Egitto. Ha partecipato attivamente alle rivolte di piazza Tahrir ed e' venuto qui a raccontarci come sono nate, come si sono diffuse e che cosa sta cambiando nel mondo arabo in questi giorni. Lo fa con le parole e lo sguardo di chi e' fiero di cio' che ha compiuto ma sa, al contempo, che siamo solo all'inizio. E poi, a mitigare i suoi entusiasmi, c'e' il suo ruolo di ricercatore, la consapevolezza che anche le rivoluzioni vanno studiate attraverso la lente della sociologia, per comprenderne i contorni, gli obiettivi ma anche per orientarne l'evoluzione verso qualcosa di costruttivo. Il titolo di questo post e' il titolo della sua presentazione: Tahrir, il nome della piazza dove si e' svolta la protesta egiziana, significa in arabo \"liberta'\", taamir significa costruire. E tra i due termini ne ha aggiunto un terzo, taghyeer, che significa \"cambiare\". Ci ha spiegato che fare una rivoluzione non significa solo mandare a casa un dittatore ma significa cambiare dentro. Ci ha raccontato quale shock sia stato, per la societa' egiziana, il movimento di piazza. E ha anche detto che non ha alcuna importanza se e' stato eterodiretto, almeno agli inizi, perche' non e' la prima volta che qualcuno tenta di mobilitare gli abitanti dei paesi arabi. Perche' stavolta ha funzionato? Perche' sono cambiati i cittadini e, secondo lui, i nuovi mezzi di comunicazione hanno avuto un ruolo preminente in questo cambiamento. \"La gente era ormai convinta di non avere piu' voce\" ha detto. \"E poi ecco che viene fuori uno strumento che da' visibilita' alla voce di chiunque\". Ci ha raccontato anche della sua vicina di casa, una donna molto ricca che non aveva mai camminato per le strade della propria citta' perche' all'uscita l'aspettava sempre l'autista. E poi del suo stupore nell'incontrarla in piazza Tahrir, durante le rivolte, desiderosa di riallacciare i contatti con i propri concittadini. Ci ha parlato dei tentativi di democrazia partecipativa che nascono in questi giorni. Il sindaco di Porto Said, per esempio, ha aperto un sito web per parlare con i propri cittadini. Ha indetto due ore al giorno di riunione aperta a tutti, ogni mattina dalle 9 alle 11: basta andare in comune e sedersi nella sala. E quando qualcuno gli ha proposto di filmare queste riunioni e metterle su Youtube, lui ha detto di si'. L'Egitto oggi sta provando a rendere trasparenti tutti gli atti e i documenti dell'amministrazione pubblica, perche' questo e' l'unico modo, per un politico, di sfuggire a possibili accuse di corruzione. El Zaim ha bussato per anni alle porte dei ministeri invitando a una maggiore trasparenza, ora sono loro, i ministri, che lo chiamano per chiedergli di mettere on line tutto il loro lavoro, compreso il budget dei loro ministeri e gli indici di efficienza. Nessuno vuol piu' passare per corrotto, nessuno vuol dare l'impressione di avere dei segreti da nascondere. E per facilitare la discussione pubblica sulla nuova costituzione che gli egiziani sono stati chiamati a votare qualche settimana fa, sono stati messi in piedi ben quattro diversi siti interattivi. Come ogni rivoluzione, anche questa sta passando per la fase dell'entusiasmo e della confusione. Per questo, dice El Zaim, bisogna guardare all'Egitto (ma anche alla Tunisia, alla Libia e alla Siria, quando finalmente taceranno le armi di chi tenta di conservare un potere che non puo' piu' reggere di fronte ai cambiamenti epocali dei popoli che i ditattori pretendono di dominare) con l'occhio della scienza e della sociologia. Perche' le nuove tecnologie aiutano, ma certamente non bastano. La sfida e' troppo grande: in gioco c'e' l'equilibrio geopolitico di questa delicata regione del mondo ma anche lo sviluppo economico e scientifico del Nord Africa e di tutto il Medio Oriente. E, non ultimo, il mantenimento di questo nuovo senso di identita' nazionale, di fierezza e di consapevolezza dei propri diritti che forse e' il vero dono che la tecnologia ha fatto agli egiziani."},
{"title": "Mostri scientifici e mostri etici", "text": "Della legge sul biotestamento votata in questi giorni alla Camera hanno parlato tutti i giornali e le ragioni per cui la trovo assolutamente inaccettabile sono riassunte molto bene in questo post di Chiara Lalli. Due parole però le vorrei spendere sui riferimenti scientifici della legge, perché mi paiono esemplificativi di come nel nostro paese si utilizzi la scienza a capocchia, piegandola alle interpretazioni utili a sostenere le proprie tesi.  Innanzitutto la questione di quando e su quali pazienti sarebbero applicabili queste direttive anticipate, o DAT. Il testo afferma che le DAT assumono \"rilievo nel momento in cui il soggetto si trovi nell'incapacità permanente di comprendere le informazioni circa il trattamento sanitario e le sue conseguenze per accertata assenza di attività cerebrale integrativa cortico-sottocorticale e, pertanto, non può assumere decisioni che lo riguardano\".La mia prima reazione leggendo questa frase è stata: ma come la dimostro, la disconnessione cortico-sottocorticale? E poi, perché mai una persona che non è più in grado di comprendere la natura delle terapie a cui deve essere sottoposta non dovrebbe poter contare sulle proprie direttive anticipate anche se non ha una disconnessione cortico-sottocorticale? Che questa espressione in questo specifico contesto legislativo suoni come una legge finanziaria che ci obbligasse a pagare le tasse in dollari, l'hanno purtroppo confermato anche persone più competenti di me, per esempio l'Associazione degli anestesisti-rianimatori ospedalieri italiani (Aaroi), che poi sono quelli che si troveranno a doverla utilizzare per giustificare il proprio operato. Con questo termine si intende comunemente una condizione molto particolare (e  difficilmente dimostrabile con le valutazioni neurologiche di routine), tipica di alcuni stati di coma,  come quelli che seguono un trauma cranico. La botta ricevuta, il brusco  movimento del cervello all'interno della scatola cranica possono infatti  danneggiare gli assoni, i prolungamenti dei neuroni che connettono una  cellula nervosa all'altra. Quando questo avviene in modo massiccio e  pressoché uniforme in tutto il cervello, si parla di danno assonale  diffuso, e quindi di interruzione (che può essere momentanea o definitiva, totale o settoriale) della comunicazione tra la corteccia del cervello e le parti profonde -  sottocorticali, appunto - che sono necessarie per compiti cognitivi  complessi come la memoria, l'attenzione e la consapevolezza di sé e dell'ambiente. Anche una degenerazione della corteccia, come si incontra nelle fasi terminali delle demenze, può provocare la disconnessione, ma è davvero raro che ciò accada in modo uniforme in tutto il cervello e che quindi si interrompano tutte le vie di comunicazione. Al momento, dimostrare che nel cervello di un paziente non avvengono più scambi di informazioni tra i diversi livelli è un compito davvero arduo. Solo nella morte cerebrale (cioè in quella condizione che per la legge italiana è già morte accertata e che consente l'espianto degli organi) si attua la vera disconnessione cortico-sottocorticale, ma mi pare evidente che il legislatore non intende varare una norma che consenta ai medici di smettere di curare chi è già morto, come ha detto non senza ironia il senatore Ignazio Marino. Disconnessioni parziali sono presenti in molte malattie, mentre persone totalmente incoscienti e in stato vegetativo da molti anni possono mostrare, se esaminate con tecniche sosfisticate, segni di comunicazione conservata tra le diverse parti del cervello. E allora? Vogliamo dire che i consulenti tecnici di chi ha scritto questa legge sono degli ignoranti? Che non sanno che la questione è dibattuta a livello mondiale e che tale definizione è senza senso? Perché non trovare una formula meno criptica, meno foriera di interpretazioni e dubbi, se proprio si vuole far passare il principio che le direttive anticipate valgono solo in casi estremi? Una risposta me la sono data. Questa legge avrebbe potuto essere riassunta in un unico articolo che recitasse più o meno così: \"non sarà consentito a nessuno di disporre a proprio piacimento del proprio corpo e della propria vita come è accaduto nel caso di Eluana Englaro o, peggio, di Piergiorgio Welby\". Una legge ad personam, in fondo, esattamente come molte di quelle che siamo abituati a vedere. Una legge che fa fare alla bioetica italiana un balzo indietro di decenni (e sì che non eravamo certo tra le avanguardie), perché sancisce il principio del tabù, dell'argomento che non entra nemmeno in discussione, il che è l'antitesi della bioetica. Purtroppo nel nostro Paese non si è ancora capito che lo scopo della bioetica è di permettere a una società di giungere, su temi complessi, a un compromesso. Le posizioni dell'uno o dell'altro devono essere posizioni di partenza e le leggi in materia etica in genere si situano in un punto intermedio , con l'intento di placare le tensioni sociali sui grandi temi della vita e della morte ma allo stesso tempo di mantenere, ogni volta che è possibile, il principio di autodeterminazione degli individui. Quella appena approvata alla Camera è, tra l'altro, una legge che contiene altri mostri scientifici, come l'esclusione dell'idratazione e della nutrizione dal novero delle terapie mediche A livello internazionale se ne discute, ovviamente, ma il parere più diffuso e autorevole, come potete leggere in questo articolo e in molti altri simili come questo - rivolto ai familiari - è a favore del riconoscimento di alcune tecniche di nutrizione tra le forme di terapia medica e quindi, in quanto tali, rifiutabili dal paziente. E contiene anche mostri legali, come l'introduzione del concetto di \"orientamento non vincolante\": insomma, ci è consentito dire la nostra, ma poi nessuno è tenuto a rispettare la nostra opinione. Avevo annunciato un post sulla sessione bioetica che ho organizzato al congresso mondiale dei giornalisti scientifici a Doha. Mi è passata la voglia, sinceramente. Perché è tale l'abisso tra il livello della discussione (civile e appassionata) tra persone di varia umanità e religione a cui ho assistito e l'impianto teorico e tecnico di questa norma che mi pare proprio di vivere su un altro pianeta."},
{"title": "Vittime imbelli", "text": "Ogni volta che in questi giorni tiro fuori uno studio scientifico da commentare qui sul blog, l'attualità mi obbliga a intervenire e a mettere da parte le curiosità. Perché prevale la consapevolezza che davanti a certe cose non si può stare zitti. E di cose per le quali varrebbe la pena di parlare ne ho ascoltate molte in questi giorni, dopo la strage dei giovani di Utoya. Ma una delle più crudeli è questo editoriale di Vittorio Feltri, in cui quello che dovrebbe essere un mio collega (peraltro reduce dalla sospensione che gli ha comminato l'Ordine dei Giornalisti in un procedimento disciplinare legato a passate violazioni del codice deontologico) afferma in tutta tranquillità che se ci sono state tante vittime è perché i giovani di oggi sono egoisti, hanno pensato a se stessi e non hanno saputo fare squadra per fermare il fanatico che sparava loro addosso. Non solo: Feltri si cimenta in un paragone naturalistico, affermando che qualsiasi animale sacrifica se stesso per la salvezza del branco, dimenticando che gli animali si comportano così davanti ai loro predatori naturali, non certo ai loro simili.  Ora, questa storia delle vittime imbelli è piuttosto vecchia. L'hanno tirata fuori, tra gli altri, i nazisti al processo di Norimberga, per dimostrare che in fondo loro non potevano essere così cattivi visto che milioni di ebrei, zingari, omosessuali, testimoni di Geova e membri di minoranze varie avrebbero potuto con facilità avere la meglio su qualche migliaio di SS. L'hanno tirata fuori al tribunale internazionale dell'Aia i responsabili della strage di Srebrenica, perché 8.000 bosniaci musulmani avrebbero ben potuto fermare le truppe di Ratko Mladic. O no?... Forse Vittorio Feltri, prima di scrivere quel che ha scritto, avrebbe dovuto leggere un po' di lavori di vittimologia, termine che indica lo studio degli effetti della violenza sulla psiche delle persone. Avrebbe scoperto che chi è oggetto di una violenza, tanto più se inspiegabile e sproporzionata, viene privato della propria natura di essere umano, viene percepito (e percepisce se stesso) come oggetto. È un processo chiamato reificazione della vittima (da res, cosa) che avviene sia nella mente del carnefice, consentendogli di compiere azioni efferate, sia, di riflesso, in quello della vittima. È un processo paralizzante, per combattere il quale è necessario tempo: quel tempo che i ragazzi di Utoya non hanno avuto. La sensazione di non valere niente, di essere nulla agli occhi del proprio carnefice, che in quel momento rappresenta la vita o la morte, è raccontata da molte persone che hanno subito violenze inaudite e improvvise (come in Norvegia), umilianti (come nel caso degli stupri e delle torture), prolungate nel tempo (come nel caso dei campi di concentramento). Per ribellarsi serve il tempo dell'elaborazione, serve a volte una personalità ancora integra, scampata per miracolo all'annientamento, come è avvenuto nel ben noto episodio della ribellione del SonderKommando di Auschwitz, per restare a uno dei casi più studiati. A volte serve un'epifania individuale, come quella che racconta Primo Levi in Se questo è un uomo, quando in un raro momento di tregua, traduce in francese il canto di Ulisse dalla Divina Commedia di Dante: fatti non foste a viver come bruti... Ma proprio Levi, con le parole asciutte di cui era capace, ci racconta ne \"I sommersi e i salvati\" che dal senso di colpa per essere sopravvissuti non si esce facilmente. E purtroppo è quello che confermano gli studi sulla sindrome post-traumatica da stress che colpisce chi si salva da eventi come quello di Utoya. Perché a credere che si sarebbe potuto fare qualcosa per fermare la violenza non ci sono solo le persone come Feltri, ma in primo luogo i salvati. Immaginate uno qualsiasi di questi adolescenti, nascosto dietro un cespuglio mentre un uomo in uniforme della polizia fredda un loro amico che supplica pietà. E chiedetevi quale può essere l'effetto di una scena come questa sul loro futuro: perché me la sono cavata? Sono stato un codardo? Ho fatto morire qualcun altro al mio posto? Cosa ho fatto DI MALE per essermi salvato? Gli psicologi che lavoreranno con queste e con altre vittime hanno davanti a sé un impegno lungo e faticoso, che parte dalla ricostruzione della consapevolezza del proprio valore di essere umano, negato da una violenza senza ragione apparente - del valore della propria vita - per arrivare alla fine all'elaborazione del senso di colpa per il fatto di essersi salvati. E ogni stupido e crudele articolo come quello pubblicato su Il Giornale contribuisce a far credere a tutti noi che ci vuol poco a essere eroi e che la norma è reagire. Non è così: la norma, per le vittime, è purtroppo subire, e bisognerebbe aiutarle a capire che non c'è proprio nulla di male in questo."},
{"title": "UPenn1 / Benvenuti nel Nuovo mondo", "text": "Niente vacanze quest'anno, ma non posso lamentarmi. Sono negli Stati Uniti, all'Università della Pennsylvania, a Philadelphia, per seguire un corso unico al mondo, almeno per come è strutturato e per i partecipanti che raggruppa. Si tratta di un \"boot camp\", un corso concentrato (e piuttosto faticoso, devo dire), organizzato dal Center for Neuroscience and Society, un centro di ricerca diretto da una neuroscienziata, Martha Farah, che da alcuni anni è molto attiva nell'ambito della neuroetica e dell'analisi delle ricadute sociali delle neuroscienze.  I miei compagni di studio sono quanto di più vario si possa immaginare: psicologi, neuroscienziati ma anche moltissimi giudici, avvocati, docenti universitari di legge, marketing, filosofia, storia, teologia... Ci sono un paio di giornalisti scientifici, un filmaker, due autori di libri divulgativi di successo sulle neuroscienze. Da una tale varietà di persone ci si potrebbe aspettare un livello di preparazione medio piuttosto basso sull'argomento del corso, ma è esattamente l'opposto. Sono tutte persone che hanno a che fare con le neuroscienze per lavoro o che le seguono da molti anni per interesse personale . Il livello medio di preparazione è molto alto e quel che più colpisce chi come me viene dall'Europa, dove le competenze professionali sono a \"compartimenti stagni\", è l'interdisciplinarietà e la facilità con cui queste persone sono capaci di cambiare lavoro, adattare le proprie conoscenze, sommare corsi e master per acquisire nuove competenze che vengono poi riconosciute dal mercato del lavoro. Abbiamo appena passato tre giorni molto intensi a discutere insieme a Geoffrey Aguirre di neuroimaging, dei suoi limiti tecnici e statistici, di come la scelta del modello sperimentale influenzi i risultati. Abbiamo parlato di \"neuroriduzionismo\", ovvero della tendenza a ricondurre comportamenti umani complessi all'attivazione di questa o quella area del cervello, anche quando questa operazione è un non senso in termini scientifici. Abbiamo analizzato alcune pubblicazioni scientifiche molto prestigiose per identificarne i punti controversi e sviluppare una sana capacità critica anche verso i migliori. Potrei raccontarvi quante cose sto imparando, ma più che le informazioni tecniche (che ci sono, dato che i nostri docenti sono tra i migliori sulla piazza in ogni determinato argomento) è l'approccio che mi colpisce.Non posso non pensare a come è diverso questo modo di insegnare da quello a cui siamo abituati nelle università italiane. Sono colpita dal fatto che ricercatori con un impact factor stratosferico ci tengano a presentarsi innanzitutto con l'elenco dei riconoscimenti che hanno ottenuto come docenti. E bravi lo sono davvero: ti coinvolgono nella discussione, ti obbligano a metterti in gioco. Non esistono domande stupide, ma le risposte non sono scontate: devi collaborare alla ricerca della risposta. È un modo di insegnare che non permette distrazioni e che coinvolge anche quando ci si addentra nei meandri di qualche oscuro fenomeno elettrico a livello di singoli neuroni: uno di quegli argomenti di neurofisiologia che all'università mi facevano impazzire e che ho odiato con tutta me stessa! Non riesco a capire da cosa dipenda questa differenza così notevole: non è solo una questione di soldi (questo corso è piuttosto caro ed è ovvio che i partecipanti si aspettano un ritorno adeguato all'investimento). Credo che vi sia, tra i miei docenti, un reale interesse per l'oggetto dei loro studi, la curiosità di trasmetterne i segreti a un gruppo di studenti decisamente atipico e anche la consapevolezza che la loro carriera nell'università (e la fama di cui godono) dipende dal modo con cui insegnano. Niente vacanze quest'anno ma non sono affatto pentita: credo che tornerò a casa con molte idee nuove da sviluppare. PS: il signore nella foto è Benjamin Franklin. La UPenn è la più vecchia università degli Stati Uniti ed è stato proprio lui a fondarla."},
{"title": "La rinuncia ai sogni", "text": "Stamattina sono un po' di corsa, ma ci tenevo a condividere alcune riflessioni suscitate da  questo articolo uscito su La Repubblica. I dati sono impressionanti: 47 per cento dei laureati, in Italia, fa un lavoro sottoqualificato. E così c'è chi, pur di avere il posto fisso, rinuncia a fare ciò per cui ha studiato e accetta, per esempio, di fare il netturbino, o il custode in un museo. Le domande che mi pongo, e che giro a voi perché non ho una vera risposta, sono le seguenti:1. È possibile che la paura del precariato porti alla rinuncia totale alle proprie ambizioni personali? E quanto sta accadendo in Italia nel mondo del lavoro sta creando una generazione di rinunciatari?2. Il precariato esiste anche in altri paesi, come gli Stati Uniti, dove il fenomeno della sottoqualificazione è meno diffuso. Perché? È una questione di mentalità? O è una questione di valore degli studi (le lauree americane valgono di più delle nostre dal punto di vista formativo?)? O, ancora, il precariato in altri Paesi è meno \"precario\" che da noi e quindi pesa meno psicologicamente sulle spalle dei giovani?3. È possibile essere felici e soddisfatti solo perché si porta a casa uno stipendio fisso? La stabilità del guadagno consente forse ad alcune persone di essere sufficientemente serene da sviluppare solo nel tempo libero gli interessi personali e culturali che non possono portare avanti sul lavoro? Oppure con la rinuncia alle proprie ambizioni si verifica anche una sorta di involuzione della persona?  Ho un gran rispetto per qualsiasi tipo di lavoro e ho fatto di tutto, dalla cameriera alla tata, dalle pulizie in casa d'altri ai lavori agricoli. Ma era una fase della mia vita in cui il solo fatto di lavorare era un valore, per me: intanto, però, studiavo per un futuro diverso, per fare ciò che davvero mi interessava. Oggi, se non trovassi altro e dovessi mantenere la famiglia, credo che farei la netturbina senza nessuna remora, ma certamente preferirei un lavoro più faticoso e precario ma più vicino alle mie aspirazioni intellettuali. Inoltre leggo sempre questi articoli con un pensiero alle mie figlie e si rafforza in me l'idea che l'obiettivo della loro educazione è di farle sentire cittadine del mondo, capaci di star bene dovunque si offra loro un contesto adatto alla loro realizzazione personale. Forse questi sono i pensieri di una persona che considera la soddisfazione sul lavoro una parte sostanziale della propria felicità. Per alcuni di voi potrebbe non essere così. Permettetemi un'ultima considerazione, che esula dalle questioni sociologiche e psicologiche. Ho provato un moto di tristezza leggendo la storia dei custodi con la laurea in archeologia che non possono sfruttare le loro conoscenze facendo da guida al pubblico perché i sindacati considerano ciò un privilegio inaccettabile. Mi pare impossibile, ma sto cercando conferma della notizia. E mi pare che questa sia semplicemente l'altra faccia di un sistema economico e imprenditoriale che chiede ai propri giovani di usare il cervello il meno possibile."},
{"title": "Stress and the city", "text": "È davvero raro che uno studio di psicologia sociale, sebbene supportato da una parte di imaging cerebrale, assurga agli onori di una copertina su Nature. Eppure è quanto è successo oggi con il numero che vedete qui a fianco (qui trovate il podcast con la notizia e l'intervista ai ricercatori). Un gruppo di canadesi del Douglas Institute for Mental Health, affiliata alla McGill University, ha infatti dimostrato che vivere in città è un rischio per la salute mentale e che nel cervello dei \"cittadini\" alcune strutture vengono modificate e quindi diventano eccessivamente sensibili agli stimoli stressogeni.Non che sia una novità: studi precedenti hanno già detto, per esempio, che i cittadini soffrono di ansia e disturbi dell'umore più dei campagnoli e che anche l'incidenza di schizofrenia aumenta tra chi è nato e cresciuto in un ambiente urbano invece che in un ambiente bucolico. La differenza non è piccola, poiché è dell'ordine del 20 per cento per l'ansia e del 40 circa per i disturbi dell'umore. Giacché oltre metà della popolazione mondiale vive in ambiente urbano, i ricercatori che hanno condotto questo studio (e anche noi   ) pensano che creare un ambiente cittadino più vivibile è una assoluta priorità (o dovrebbe esserlo). Vivere in città ha certamente anche dei benefici dal punto di vista della salute, per esempio per la maggiore accessibilità delle strutture sanitarie, ma rende inequivocabilmente più instabili dal punto di vista psichico, o almeno così diceva finora l'epidemiologia.Ora i canadesi ci dicono cosa ha di diverso un cervello \"cittadino dalla nascita\" da quello di chi vive in campagna e da quello che in città ci sta vivendo, ma magari non da tutta la vita. Un gruppo di volontari tedeschi è stato infatti sottoposto a un test, il Montreal Imaging Stress Task (MIST) che si può eseguire anche nella risonanza magnetica funzionale. Si può quindi vedere che cosa succede nel cervello quando deve far fronte a eventi stressanti e se ci sono differenze tra persone cresciute in ambienti diversi. Il risultato dell'imaging dice che i cittadini hanno un modo diverso di processare e valutare gli stress, in particolare quando questi ultimi sono legati a qualche forma di interazione sociale. Chi vive in città attiva in modo maggiore l'amigdala, struttura profonda ben nota perché coinvolta in meccanismi come la paura, mentre chi ci è nato ha un'attivazione particolare della corteccia cingolata anteriore, a sua volta importante perché controlla l'amigdala e regola le emozioni negative. Non ci sono altre differenze tra i due gruppi e anche questo è importante, perché le strutture coinvolte dimostrano, in soldoni, che vivere in città ti fa stare sempre sul chi vive, facilita l'ansia e ti dota di una reattività eccessiva agli stimoli esterni negativi, necessaria peraltro a sopravvivere in un ambiente caotico e rumoroso. Ci sono molti elementi interessanti in questo studio: innanzitutto l'idea di applicare l'analisi della struttura cerebrale a un fenomeno complesso come il nascere e il vivere in un ambiente urbano nella speranza di identificare un quadro di attivazione peculiare, come è stato. L'altro elemento caratterizzante è la scoperta che nascere e crescere in città o vivere in città nel momento del test sono condizioni che danno luogo a risultati diversi e questo potrebbe avere un significato importante in termini di prevenzione. In particolare è utile ricordare che l'area cerebrale che mostra differenze nei nativi urbani, cioè la corteccia del cingolo, è stata chiamata in causa anche nella schizofrenia, e molti studi effettuati su adolescenti agli esordi della malattia mostrano una sorta di \"disconnessione\" tra quest'area e l'amigdala, come a dire che il legame tra la malattia e il fattore di rischio (nel caso specfico la città) è solido. Ma c'è di più: i ricercatori sanno che le cause di queste differenze cerebrali potrebbero essere molteplici (inquinamento, sostanze tossiche, rumore, sovraffollamento, fattori demografici o altro) ma affermano con sicurezza che la causa sono gli stress sociali, cioè proprio il tipo e le modalità di interazione tra esserei umani tipica della grande città. Si tratta, secondo loro, dell'ipotesi più parsimoniosa dal momento che studi su animali hanno già dimostrato che la corteccia anteriore del cingolo e l'amigdala subiscono un rimaneggiamento a livello sinaptico e neuronale quando l'animale è sottoposto a stress sociali. In un contesto umano, questo significa che la città è un ambiente che potenzia gli effetti negativi di ogni tipo di stress, compreso, per esempio, quello legato alle disparità socioeconomiche o alla difficoltà di creare un solido network di relazioni. Insomma, questo primo tentativo di interdisciplinarietà tra neuroscienze e sociologia ci dà un supporto scientifico in più (semmai fosse stato necessario) per fare delle nostre città luoghi più vivibili anche dal punto di vista delle relazioni e non solo da quello ambientale. Lederbogen, F., Kirsch, P., Haddad, L., Streit, F., Tost, H., Schuch, P., Wüst, S., Pruessner, J., Rietschel, M., Deuschle, M., &amp; Meyer-Lindenberg, A. (2011). City living and urban upbringing affect neural social stress processing in humans Nature, 474 (7352), 498-501 DOI: 10.1038/nature10190 "},
{"title": "Doha/WCSJ 1 - I segreti delle star del science blogging", "text": "Eccoli qui, tutti riuniti allo stesso tavolo per la sezione dedicata ai blogger di scienza: sono le star del science blogging, arrivate qui a Doha, in Qatar, per il congresso mondiale dei giornalisti scientifici organizzato dalla World Federation of Science Journalists. Già, perché non c’è solo il mio collega Giovanni Spataro qui nella Penisola Arabica: ci sono anch’io, dato che ho organizzato la sessione dedicata alla neuroetica, ma di questo vi parlerò in un altro post. Sono andata invece a sentire la sessione dedicata al science blogging con un filo di apprensione: questi sono dei giganti, con migliaia e migliaia di visitatori al giorno. Ovvio, scrivono in inglese e hanno una platea globale, ma anche la verve dei migliori e quel tanto di anarchia che rende il loro modo di raccontare la scienza intrigante e affascinate. Ve li elenco: il primo a sinistra è Moheb Costandi, autore di Neurophilosophy, uno dei miei blog di riferimento. È un ricercatore ma ormai, come racconta, dedica più della metà del suo tempo al blog ed è stato recentemente “acquistato” da The Guardian, quotidiano britannico che intende potenziare la propria squadra sul web e quindi lascera' a breve ScienceBlog. Sì, perché questi sono corteggiatissimi, e passano da un editore all’altro, pur avendo tutti cominciato come amatori - anche quando, come nel caso di Maryn Mc Kenna , la seconda da sinistra, autrice di Superbug, acquistata da Wired - hanno un passato (e un presente) di reporter classico. Nello specifico, Maryn segue da anni il lavoro dei Centers for Disease Control di Atlanta, il colosso mondiale dell’epidemiologia ma anche quelli che vanno alla ricerca delle origini di tutte le epidemie più strane (e per questo sono diventati protagonisti di telefilm di successo). Jennifer Ouelette, invece, è la brillante e simpatica autrice di Cocktail Party Phisics, il blog di “fisica for dummies” che, aperto in un momento di stanca professionale, ora sta per entrare alla far parte della prestigiosa :-)  famiglia dei blog di Scientific American, dopo essere passata attraverso ScienceBlog e Discovery, editore per il quale tiene anche un blog monotematico (Discovery News Space). Chi bazzica la blogosfera scientifica non può non conoscere Ed Yong, autore di Not Exactly Rocket Science: anche lui a breve passerà sotto l’ala protrettrice di un grande editore. Infine, sulla destra, è seduto Mohamed Yahia, blogger egiziano e giornalista scientifico di Nature Middle East, che racconta la scienza come è vista (e come può essere capita) dai paesi in via di sviluppo, specie da quelli islamici. Il suo blog, House of wisdom, ha partecipato attivamente al movimento rivoluzionario che ha portato alla caduta di Mubarak perché, come ha detto in conferenza, c’è un momento in cui anche la scienza deve lasciare il passo ad argomenti più importanti. Qual è il segreto per diventare un blogger di successo? A dire il vero, i loro consigli non sono del tutto applicabili in Italia dove, che io sappia, nessun blogger arriva a guadagnare metà del proprio stipendio grazie ad esso, né possiamo contare su un pubblico ampio come quello di scrive in inglese. Però qualcosa di importante l’hanno detto, per noi che stiamo da questa parte della tastiera e che talvolta siamo un po' in crisi e non sappiamo bene se dobbiamo essere piu' noi stessi o piu' giornalisti: bisogna scrivere solo di ciò che ci appassiona e non dimenticare, come purtroppo tendiamo a fare soprattutto noi professionisti della comunicazione, che il blog è uno spazio libero; che non è una rubrica del giornale ma uno spazio nostro, dal quale deve trasparaire chi siamo e perché abbiamo scelto di parlare proprio di quella cosa con i nostri lettori. Poi un blog di successo deve essere parte di una comunità, deve linkare ed essere linkato anche da altri perché, come ha detto Jennifer, “everyting in web is generative”, ogni cosa postata sulla rete genera altre cose. E infatti l’unico consiglio tecnico che hanno dato è quello di lavorare su più piattaforme: se qualcuno di voi vuole aprire un blog, apra in contemporanea una pagina Facebook e un account di Twitter e colleghi i tre mezzi con feed reciproci e automatici, in grado di pubblicare un commento uscito su Facebook anche sul blog e viceversa (sempre che la piattaforma che utilizzate ve lo consenta, il che non è purtroppo il nostro caso qui a Le Scienze). Questo mi pare un buon suggerimento: anche io mi accorgo che molti dei miei post suscitano una discussione parallela qui sul blog e sulla mia pagina di FB, dove peraltro al momento sono presenti solo amici, dato che è la mia personale. Quanto alla tecnica di scrittura e al contenuto, il segreto del loro successo è uno solo, come vi dicevo: sii te stesso, ma controlla con estrema attenzione la fondatezza di ciò che dici, perché il blog non perdona. Se fai un errore, qualcuno prima o poi te lo farà notare. Una bella palestra anche per noi giornalisti italiani che non abbiamo, come invece hanno i colleghi americani e inglesi, un ufficio preposto al fact checking, cioè alla verifica della fondatezza di quanto raccontiamo al nostro pubblico. PS: questa qui sotto e' Doha. Ci vuole qualche giorno per capire dove sta il suo fascino, ma credo di averlo finalmente colto e ve ne parlero' quanto prima. E' un incredibile mix di oriente, islam, deserto e modernita', in uno scenario che ricorda Blade Runner. Alzi gli occhi e ti aspetti di vedere, tra un grattacielo e l-altro, spuntare un'astronave. Ah, scusate la mancanza di alcuni accenti: sto scrivendo da un computer con la tastiera araba e faccio una certa fatica! "},
{"title": "Dentro lo scafandro", "text": "Sabato scorso sono stata a Bra per un interessante convegno dedicato al coma e alla locked-in-syndrome (LIS), una malattia che è facile confondere con il coma perché l'individuo colpito è totalmente paralizzato e incapace di comunicare con l'esterno, per cui viene spesso scambiato per incosciente. All'incontro partecipavano diversi esperti italiani come Piergiorgio Strata e Giovanni Berlucchi, che hanno parlato di cos'è la coscienza e di quanto è complesso determinarne i confini e persino l'esistenza. Altri, come Gabriella Bottini ed Eraldo Paulesu hanno invece parlato di tecniche diagnostiche e della necessità di disporre quanto prima di protocolli certi per discriminare, nelle rianimazioni, i pazienti che sono davvero in coma da quelli che sono chiusi all'interno del loro corpo, incapaci di dire che sono invece coscienti e consapevoli. Infine Steven Laureys, uno dei neurologi dell'Università di Lovanio che insieme ad altri medici come Adrian Owen ha messo a punto un test di immaginazione mentale alla risonanza magnetica che potrebbe aiutare a identificare i pazienti rinchiusi nel proprio corpo, ha mostrato alcuni dati interessanti di risonanze effettuate in pazienti con diversi gradi di coma. L'obiettivo di questi studi è quello di cercare indici prognostici, cioè qualcosa che consenta ai curanti e ai familiari di un paziente in coma di capire se è lecito attendersi una qualche forma di risveglio o se questa evenienza è improbabile, per non dire impossibile. I risultati ottenuti con i primi tentativi sono importanti e sollevano un milione di questione etiche e metodologiche, ma per una volta mi pare più giusto lasciare la parola ai pazienti e non agli scienziati. È soprattutto l'aspetto umano di questa malattia che mi ha colpito, a partire dalla storia di chi ha organizzato l'incontro: l'Associazione Amici di Daniela, fondata nel 2007 dai coniugi Ferraro. Daniela Ferraro era presente in sala, sulla sua sedia a rotelle, completamente paralizzata se non per qualche movimento degli occhi e delle palpebre, eppure presente. Accanto a lei la sua bimba di cinque o sei anni, intenta a \"parlarle\" attraverso un complesso codice di battiti di palpebre e di lettere pronunciate con l'aiuto di una facilitatrice linguistica. Da quando, nel 2005, Daniela Ferraro si è trovata congelata nel proprio corpo a causa di un'emorragia cerebrale non ha mai smesso di lottare, insieme al marito, per poter nuovamente comunicare con gli altri e per poter vivere normalmente, per quanto rimane di normale in una situazione tanto estrema.  I coniugi Ferraro sono stati aiutati, nella loro opera, da ALIS, l'associazione francese per la locked-in-syndrome, fondata da Jean Dominique Bauby, il giornalista autore del libro \"Lo scafandro e la farfalla\" dal quale è stato tratto l'omonimo film. Bauby è morto pochi anni dopo l'incidente che lo ha paralizzato, ma l'associazione che ha fondato è diventata un importante interlocutore a livello europeo per tutto quanto riguarda il coma e gli stati di postcoma. Ora sta nascendo una federazione europea per dare a questi pazienti più voce e più forza. La ragione per cui scrivo questo post è molto semplice: da quando è nata LISA-Locked-in-syndrome association, la versione italiana di ALIS promossa sempre dal signor Ferraro, sono stati trovati solo 30 casi di LIS in Italia, quando le statistiche dicono che dovrebbero essere circa 600. Dove sono gli altri? Può darsi che non siano stati trovati malgrado la ricerca sistematica, ma il pensiero più tragico è che molti di loro siano ricoverati da qualche parte, o assistiti in casa, nella convinzione che non siano coscienti: l'errore diagnostico è sempre possibile in questi casi e, purtroppo, anche le tecniche di neuroimaging non sono facilmente applicabili né sempre dirimenti. Nel corso del convegno ci è stato chiesto di far girare la voce, affinché i malati di LIS sappiano che c'è qualcuno a cui possono rivolgersi e perché i familiari che hanno qualche dubbio sulla diagnosi di un loro caro possano chiedere all'associazione maggiori informazioni, e io raccolgo l'invito."},
{"title": "Hunziker, lo stalker e le parole", "text": "Una piccola e rapida  riflessione innescata da un titolo di cronaca letto scorrendo Repubblica.it: \"Assolto molestatore della Hunziker. Era incapace di intendere e di volere\". Lo sguardo si ferma per via dell'implicito ossimoro: in che senso, assolto? Era incapace di intendere e volere solo al momento del fatto? Oppure non è stato nemmeno processato? Quindi leggo tutto l'articolo (che trovo, con titolo analogo e contenuto simile, su moltissimi siti), anche se delle vicende della Hunziker, sinceramente, non mi importa nulla. Sapevo che era stata perseguitata da un fan troppo invadente e che c'era un processo in corso. Ora scopro che il soggetto in questione ha una chiara diagnosi psichiatrica e che il giudice ha ritenuto che non sia in grado di sostenere un giudizio. Non è mica stato assolto: non è stato nemmeno processato perché il nostro ordinamento prevede che il processo si possa svolgere (e abbia una ragione d'essere) se la persona imputata è in grado di capire di cosa la si accusa. In poche parole, di difendersi. Altrimenti è un malato, che non è imputabile. Assolto (seppure con formule diverse) è colui che il processo l'ha subito ed è stato giudicato non colpevole.  Ma perché mi infervoro tanto per un titolo, vi chiederete. Perché in quell'\"assolto\" c'è tutto un retroterra di luoghi comuni che attribuiscono alle perizie psichiatriche o neurologiche un valore di escamotage, di trucco per evitare al colpevole il carcere e la giusta punizione.Invece no, permettere ai malati mentali di essere curati invece che puniti è un segno di civiltà. E non è così in tutti i paesi e in tutti gli ordinamenti. Tanto per dirne una, negli Stati Uniti i malati di mente possono essere condannati a morte, talvolta persino quando compiono il fatto da minorenni.E quindi, caro collega che hai fatto il titolo, in questo caso \"le parole sono importanti\"."},
{"title": "Musica per la tristezza", "text": "Esiste la musica della tristezza? E che musica ascoltate, voi, quando siete tristi? Per molto tempo gli psicologi della percezione si sono concentrati soprattutto sugli aspetti uditivi della musica, poi hanno tentato di mettere in relazione emozioni e strutture musicali, arrivando alla ben nota conclusione per cui il modo minore (e in particolare l'intervallo di terza minore) sarebbe quello della tristezza mentre il modo maggiore trasmetterebbe gioia: una certezza in parte smentita con esperimenti effettuati su persone esposte a culture musicali diverse da quella Occidentale, ma che alle nostre latitudini sembra funzionare. Quest’estate un gruppo di ricercatori della Tufts University lo ha ulteriormente confermato con un esperimento molto carino che dimostra come gli attori, nel pronunciare frasi con un tono triste, tendono a eseguire intervalli di terza minore (i più tristi di tutti). Lo studio è uscito su Emotion e qui potete sentire le registrazioni degli attori.  Ora un ricercatore dell’Università della Florida, ma di origini coreane, Jiyoun Kim, ha cercato di capire quale musica scelgono le persone tristi e se esiste una relazione con il loro stato d’animo. Per fare questo ha selezionato alcuni brani americani e altri coreani, li ha fatti classificare  in base al “mood” che ispirano agli ascoltatori (brani tristi e brani allegri) e li ha fatti ascoltare a un gruppo di persone felici e a uno di persone tristi. Ambedue i gruppi mostrano una lieve predilezione per la musica “triste” (effetto che il ricercatore spiega in base alla capacità della musica triste di evocare emozioni più intense rispetto a quella “felice”), ma la novità è che pare che le persone tristi scelgano la musica soprattutto in base alla familiarità. In pratica, anche quando la vita non ci sorride, preferiamo un’allegra canzonetta che ci evoca dei ricordi piuttosto che un brano adeguato al nostro umore ma del tutto avulso dal nostro paesaggio sonoro interiore, come potrebbe essere un brano coreano. I risultati del suo studio sono stati pubblicati ieri su un numero curioso dell’International Journal of Arts and Technology, interamente dedicato al tema del rapporto tra arte, intrattenimento ed emozioni. Lo studio di Kim non è particolarmente originale, ma è interessante perché conferma indirettamente altre ricerche recenti, alcune delle quali effettuate con l’aiuto del neuroimaging, e che dimostrano la potenza della musica nell’evocare emozioni. Una potenza in molti casi superiore a quella della parola scritta o alle arti visive e che viene usata anche in alcune forme di psicoterapia. Inoltre lo studio di Kim mi ha fatto venire in mente altri esperimenti molto semplici e più volte replicati (ne trovate uno descritto qui): se prendete una sala piena di persone, fate loro ascoltare dei brani musicali e chiedete loro di segnare su un foglio che tipo di emozione intende trasmettere il compositore, avrete un risultato molto congruente, con attribuzioni comuni intorno al 90 per cento dei casi. Se però chiedete alle stesse persone di indicare che emozione suscita in loro l’ascolto di una determinata musica, avrete risultati di poco superiori alla casualità (cioè non è detto che una musica triste evochi tristezza). Per quel che mi riguarda, il mood di una musica non dipende solo dalla tonalità, ma anche da altri elementi, come il ritmo. Da qualche parte ho letto un esempio nel quale mi riconosco appieno, ed è quello della notissima melodia ebraica Hava Naghila (qui cantata in una curiosa versione franco-ebraica nientemeno che da Dalida!) che è minore che più minore non si può ma in me evoca feste, matrimoni e danze sfrenate. Inoltre ho una predilezione per la musica vocale, e non riesco a prescindere dal significato delle parole. Quando sono triste ascolto soprattutto jazz o cantautori, a volte rock (perché c’è della rabbia in quasi tutti i momenti di tristezza) e molta, moltissima musica barocca. Il brano musicale più triste che io conosca è questo, ma se la gioca con quest'altro che è di tutt’altro genere (e con altre tonalità), a dimostrazione del fatto che i nostri cervelli, in fatto di arte ed emozioni, hanno gusti piuttosto eclettici."},
{"title": "Una giornata da dimenticare", "text": "Ho una certa allergia per le \"giornate\" di qualsivoglia genere: iniziative istituzionali il cui scopo è farci riflettere \"a comando\" su questo o quel problema. Quella di oggi, però, è una \"giornata\" che trovo particolarmente inutile, se non francamente dannosa per una causa che meriterebbe altre attenzioni.  Per decreto del nostro governo, oggi dovremmo rivolgere i nostri pensieri (e occupare le pagine dei giornali e le sale per conferenze) a chi è in stato vegetativo, in un coma che non è più acuto, ma cronico. Sia chiaro, quello degli stati vegetativi è un problema enorme, umano e scientifico. Umano perché queste persone, e i loro familiari, hanno bisogno di assistenza continua, di programmi di riabilitazione e di apparecchiature costose che il nostro sistema sanitario, specie in alcune regioni, non può assolutamente permettersi (e che tende a rifiutare, come ben sanno le associazioni di familiari che lottano quotidianamente contro la burocrazia). C'è poi un problema scientifico aperto, che è quello di imparare (ci stiamo arrivando piano piano e con fatica) a capire cos'è la coscienza e che grado di coscienza è presente in un paziente in coma: ogni caso è diverso dall'altro, come dimostrano recenti casi riportati in letteratura. Tra i pazienti apparentemente inconsapevoli di quello che accade intorno a loro, apparentemente incapaci di provare alcunché è possibile che vi siano anche persone parzialmente coscienti o addirittura persone totalmente coscienti ma impossibilitate a comunicare con l'esterno per via del danno cerebrale subito. È ovvio che è un dovere della scienza quello di scoprire chi si trova in questa tragica condizione, ed è esattamente quello che diversi progetti di ricerca nel mondo (anche in Italia) stanno cercando di fare. E gli strumenti a disposizione sono, al momento, sufficientemente evoluti da dare ai medici indicazioni affidabili in merito. In futuro, se arriveranno i fondi tante volte promessi per questi studi, i medici potranno dare ai familiari un'idea chiara dello stato in cui si trova il loro caro e della relativa prognosi. Certo, gli strumenti vanno affinati, e soprattutto applicati sulla base di protocolli diagnostici condivisi, ma non esiste un vero problema in merito né dal punto di vista delle tecnologie né da quello della teoria scientifica. Esiste, se vogliamo tornare al punto precedente, un problema di soldi, perché estendere a tutti i pazienti in coma determinati esami è costoso e implica, poi, di dover prendere decisioni difficili sulla base di ciò che gli esami stessi ci dicono. E c'è anche chi, tra i ricercatori, sostiene con una certa malizia che la simpatia che i sostenitori della \"vita a tutti i costi\" mostrano per le belle ricerche di scienziati come Steven Laureys (non a caso oggi ospite del convegno ufficiale promosso dal ministero della Salute) dipende dall'ignoranza della loro vera portata: leggendo l'attività cerebrale dei malati scopriremo e aiuteremo certamente coloro che sono ancora coscienti, ma scopriremo anche che altri non ci sono più e che, senza alcun ragionevole dubbio, il loro cervello è spento, finito. E il problema di cosa fare di queste persone, dal punto di vista etico, sarà sempre lì in tutta la sua drammaticità: come spesso accade, le scoperte della scienza potrebbero non spostare di un micron i termini del dibattito bioetico. Quindi diciamo che, in teoria, una giornata dedicata agli stati vegetativi ha un suo perché. Ma nella pratica non è così, e questo per le ragioni scellerate che hanno portato alla sua istituzione e che niente hanno a che vedere con le reali esigenze dei malati e delle famiglie. Non nel giorno dell'anniversario della morte di Eluana Englaro. Perché questo, permettetemi di dirlo forte e chiaro, è SCIACALLAGGIO. Il caso Englaro NON È un caso scientificamente dubbio, NON È un caso di stato vegetativo nel quale non si sapeva cosa fare dal punto di vista medico. È un caso che non ha nulla a che vedere in senso stretto col coma, con gli stati vegetativi, ma ha a che vedere con la libertà di scelta dell'individuo, col diritto (anche in senso legale) di disporre (o predisporre, nel senso di disporre con anticipo) del proprio corpo. I giudici che hanno finalmente consentito a Beppino Englaro di lasciare che la malattia della figlia facesse il suo corso naturale non hanno posto una diagnosi di tipo medico, né hanno disquisito delle nuove acquisizioni in tema di stati vegetativi. Hanno semplicemente stabilito che ognuno di noi ha diritto di rifiutare le cure, e che questo diritto è esplicitabile anche preventivamente. Leggo sull'Avvenire di ieri queste parole del sottosegretario Rocella: \"Questa  giornata sarà anche un appuntamento per fare il punto scientifico su tutte le  scoperte su queste situazioni di cui sappiamo ancora troppo poco. E  potrà rappresentare una finestra di visibilità per queste persone e le  famiglie che le accudiscono amorevolmente, troppo spesso coscientemente  accantonate dai media che si rivolgono al grande pubblico\". Cara signora Rocella, condivido pienamente. Ma se ci tiene davvero, faccia un regalo a tutti i pazienti che si trovano in questo stato disperato e alle loro famiglie: porti la loro \"giornata\" fuori dalle pastoie della politica e dell'opportunismo e la riporti nel suo alveo naturale, che è quello del dibattito sulla scienza e sulla qualità dell'assistenza. E, visto che c'è, faccia anche un grande regalo a questo Paese: permetta alla discussione bioetica, che è ineludibile e prioritaria, di diventare davvero matura, come accade altrove. Ci consenta di sederci intorno a un tavolo a parlare di questioni che toccano il destino di noi umani con spirito aperto e propositivo, in un'ottica di reciproca tolleranza e comprensione. Mi creda, né la scienza né la bioetica potranno progredire nella direzione giusta portando sulle spalle il peso del povero corpo di Eluana."},
{"title": "La vostra definizione di famiglia", "text": "Sono giorni che leggo e rileggo articoli e libri sul concetto di famiglia per un articolo che sto scrivendo e che uscirà su Mente e Cervello.Ho avuto conferma di quanto già in parte sapevo, ovvero che il concetto di famiglia è alquanto soggettivo e sfuggevole, anche se c'è chi tenta di imbrigliarlo in una definizione univoca, legale o emotiva.Allora vi chiedo aiuto: cos'è per voi una famiglia? Che cosa la differenzia un gruppo qualsiasi di persone?Rispondete liberamente, mi va bene tutto, dalla visione \"due cuori e una capanna\" a quella politica (la famiglia è l'avamposto del controllo borghese sulla società, si diceva negli anni '70   ). Sono curiosa di leggervi.   var _gaq = _gaq || [];  _gaq.push(['_setAccount', 'UA-21806849-1']);  _gaq.push(['_trackPageview']); (function() {    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);  })(); "},
{"title": "Segnalazioni per i prossimi giorni", "text": "Vi segnalo due iniziative interessanti. La prima: un convegno che si terrà a Trieste, presso la Scuola internazionale superiore di studi avanzati, organizzato dallo storico della scienza Stefano Canali e dal Laboratorio di studi interdisciplinari. Titolo: Il cervello, le emozioni e la morale. immagini, teorie e storie. Si parlerà di come le neuroscienze studiano, oggi, le emozioni, e di come è possibile comprenderne la natura alla luce dell'evoluzione e della biologia. Il convegno dura due giorni (l'8 e il 9 marzo) e potete scaricare qui il programma dettagliato. Inizia invece il 14 marzo prossimo, a Bologna, un interessante ciclo di sette incontri organizzato dall'Istituto di studi avanzati dell'Università Alma Mater: \" The human mind: a comprehensible machine, an unsolvable mystery or.... whatever works?\" I relatori sono tutti eccezionali (da Douglas Bruster, che apre con un incontro dedicato ai suoi curiosi studi sui processi mentali di Shakespeare, al cattedratico di Francia Pierre Rosanvallon, passando per Giacomo Rizzolatti, padre dei neuroni specchio, per finire in bellezza, a giugno, con John Searle e Douglas Hofstadter). Potete consultare il programma dettagliato, con date, orari e luogo, a questo indirizzo. "},
{"title": "Profonde paure", "text": "Negli ultimi tempi, lo studio delle emozioni con gli strumenti delle neuroscienze sta dando risultati interessanti e sorprendenti. Personalmente trovo questo uno dei filoni di ricerca più innovativi, perché fino a non molti anni fa le emozioni erano ritenute \"oggetti\" non adatti alla ricerca scientifica, per la loro natura sfuggevole, soggettiva e scarsamente definibile. Per esempio Iannis Vlachos e colleghi del Bernstein Center di Friburgo hanno appena pubblicato su Plos Computational Biology uno studio sulle paure nascoste, scoprendo che anche quelle che sembrano superate si annidano, in realtà, nelle parti più profonde del nostro cervello, nell'amigdala, non a caso uno dei nuclei cerebrali filogeneticamente più antichi. Un gruppo di ratti sottoposto a suoni spaventosi mostra ovviamente tutti i segnali di una grande paura. Se però lo stesso suono  viene ripetuto più volte in un contesto diverso da quello originale e in assenza di conseguenze negative, il ratto impara a non farci più caso. Basta però ripresentare il suono nuovamente nel contesto originario, oppure in un contesto totalmente diverso da quello utilizzato per il decondizionamento dallo stimolo spaventoso, perché la paura si ripresenti tal quale. Il fatto che la paura possa essere in qualche modo mascherata era già noto e due degli autori del presente studio avevano ipotizzato in precedenza la presenza di due diverse popolazioni di neuroni: una registrerebbe la natura dello stimolo pauroso, l'altra inibirebbe la trasmissione della paura dall'amigdala verso la corteccia cerebrale. Semplificando il concetto possiamo dire che se la paura non arriva alla corteccia, cioè ai centri cerebrali superiori, allora non vuol dire che non esiste: semplicemente non raggiunge la coscienza dell'individuo che quindi non ne è consapevole. La teoria della modulazione della paura nell'amigdala è stata dimostrata grazie all'uso di un modello computazionale verificato poi sperimentalmente nell'animale sulla base del comportamento.La scoperta dei circuiti neuronali di stoccaggio e inibizione della paura nei ratti è importante anche per noi esseri umani, soprattutto se pensiamo a patologie come gli attacchi di panico o le fobie. E questi studi sulle modalità di regolazione fine delle emozioni ci dicono molto anche su ciò che può essere efficace nel curare certi disturbi ma, soprattutto, su ciò che potrebbe non esserlo: alcune terapie cognitivo-comportamentali, per esempio, possono essere utilissime per far scomparire una paura irrazionale nella maggior parte delle situazioni sociali ma senza essere efficaci del tutto, come sanno bene i pazienti, che possono sperimentare ricadute nei momenti più inattesi.Perché, come ci dice questo studio, lo stimolo che genera la paura è importante, ma lo è ancor più il contesto nel quale si manifesta. Vlachos, I., Herry, C., Lüthi, A., Aertsen, A., &amp; Kumar, A. (2011). Context-Dependent Encoding of Fear and Extinction Memories in a Large-Scale Network Model of the Basal Amygdala PLoS Computational Biology, 7 (3) DOI: 10.1371/journal.pcbi.1001104"},
{"title": "L'omosessualità non è una patologia", "text": "Ricevo ora e diffondo con grande piacere questo comunicato stampa dell'Associazione italiana di psicologia. Con una doverosa premessa: non mi importa chi ha ha tirato di nuovo in ballo (e perché) la relazione inesistente tra omosessualità e pedofilia, ma ritengo assolutamente necessario che si ribadisca l'inesistenza di tale relazione anche sul piano scientifico, oltre che su quello sociale e politico.  COMUNICATO STAMPA L’Associazione Italiana di Psicologia (AIP), principale Società Scientifica rappresentativa della Psicologia e della ricerca psicologica italiana, apprese le parole pronunciate dal Cardinale Tarcisio Bertone durante la visita in Cile (“numerosi psichiatri e psicologi .... hanno dimostrato che esiste un legame tra omosessualità e pedofilia\", Corriere dellaSera.14.04.2010, p. 2), intende precisare che la letteratura scientifica sull’argomento non supporta in alcun modo quanto sostenuto dal Segretario di Stato della Santa Sede e che è anzi dimostrato che vittime di abuso sono tanto i bambini quanto le bambine. In particolare, in qualità di Psicologi, Psicoterapeuti e Ricercatori sentiamo il dovere di precisare che le parole pronunciate dal Cardinale, oltre che assolutamente prive di evidenza scientifica, paiono rilanciare una pericolosa reinterpretazione in chiave psicopatologica dell’omosessualità, condizione invece da anni esplicitamente esclusa dalla nosografia psichiatrica in uso. Pur prendendo atto delle successive dichiarazioni di smentita, l'AIP sente il dovere di precisare che \"patologizzare\" l’omosessualità, invocando in modo improprio il supporto della comunità scientifica, non fa che aumentare l’omofobia, che è la vera malattia da combattere. L’errato riferimento alla letteratura psicologica, sociologica o psichiatrica appare estremamente dannoso per lo sforzo che, da anni, clinici e ricercatori intraprendono a vantaggio della salute psichica della popolazione, sia dei minori (abusati e non) che degli abusanti (questi sì realmente malati). L'AIP pertanto invita tutti coloro che ricoprono importanti ruoli istituzionali, e che quindi hanno un peso incisivo sulla pubblica opinione, a prestare maggiore attenzione alla ricerca scientifica e a diffondere correttamente i suoi risultati conoscitivi e applicativi. prof. Roberto CubelliPresidente Associazione Italiana di Psicologia"},
{"title": "Madri naturali, madri culturali...", "text": "Sulla mia scrivania c'è da qualche giorno il numero di Time con in copertina la pillola anticoncezionale: un tributo ai 50 anni di questo farmaco (approvato dalla Food and Drug Administration per la prima volta nel maggio del 1960). L'articolo, bello e approfondito come sanno essere spesso le inchieste di Time, mette subito in chiaro la portata dell'evento: per la prima volta nella storia dell'umanità è stato possibile controllare con certezza la propria fecondità e, di riflesso, liberare dalla funzione procreativa l'esercizio della sessualità. Una rivoluzione tutta al femminile, visto che è alle donne che incombe di portare a termine le gravidanze. In questi stessi giorni sto leggendo il libro di Elisabeth Badinter, \"Le conflit, la femme et la mère\", appena uscito in Francia e che ha suscitato Oltralpe un dibattito notevole. Da noi ne ha parlato la Rodotà sul Corriere. Badinter, per chi non la conoscesse, è una filosofa femminista che da decenni ormai si interroga sul concetto di maternità e sulla relazione tra biologia e cultura nella cura della prole e nella creazione sociale del ruolo femminile. Con questo libro dà una bella scrollata alle donne della mia generazione: ci accusa, in sostanza, di aver barattato le conquiste delle nostre madri (in special modo quelle in materia di maternità e ruolo femminile) per inseguire un nuovo modello che riconosce a parole la parità tra i sessi ma riporta la donna al suo ruolo eminetemente materno in nome di una supposta \"naturalità\": è naturale essere madri, è naturale volere figli, è naturale volerli allattare, è naturale voler trascorrere più tempo possibile con loro... Anche nella maternità si riflette il mito del naturale che rende la nostra società fintamente ecologista (così dice lei), portata ad esaltare il cibo a chilometro zero e a bandire gli OGM in modo totalmente irrazionale. E nella maternità questo significa, per esempio, promuovere oltre ogni ragionevolezza l'allattamento al seno (demonizzando i latti artificiali più che dignitosi e nutrienti), e riportare il rapporto madre-figlio alla simbiosi fisica, alla corporeità di una \"madre scimpanzé\".  Riassumere qui le diverse sfaccettature della sua analisi è complesso, ma alcune cose mi hanno colpita: per esempio, l'accusa rivolta alla psicologia e alle neuroscienze di aver rafforzato lo stereotipo della naturalità dell'essere madre (contro l' \"innaturalità\" di non volerlo essere) attraverso studi che puntano a dimostrare quanto diverso sarebbe il funzionamento del cervello femminile rispetto a quello maschile (argomento lungo e complesso ma quanto mai attuale). Oppure il dito puntato contro la mitologia della genitorialità, secondo la quale non ci dovrebbe essere nulla di più bello nella vita dell'essere genitori (il che è spesso vero, ma insomma, mica sempre...). Secondo diversi sondaggi che la Badinter stessa cita nel libro, una percentuale variabile tra il 45 e il 70 per cento dei genitori non rifarebbe figli alla luce di ciò che ha appreso con l'esperienza: sondaggi ovviamente falsati dalla natura stessa della domanda (perché è ovvio che gli insoddisfatti sono più propensi a rispondere), ma che meritano comunque qualche riflessione. La battaglia della Badinter contro l'allattamento al seno le è costata gli strali della Lega del Latte francese, a dimostrazione che, almeno su questo aspetto, ha toccato una ferita aperta. È vero che il latte materno resta un alimento prezioso per la salute dei bambini, ma è anche vero che la pressione psicologica ad allattare è a volte insostenibile. Nel mio piccolo, ne sono stata vittima. Quando è nata la mia prima figlia, qui in Italia, la sola idea di non allattarla era semplicemente inconcepibile. Solo che per me è stata un'esperienza tremenda: dolorosa, faticosa, umiliante persino. Proprio non fa per me: ma pensare di rinunciare mi faceva sentire inadeguata al ruolo materno, quindi ho proseguito fino a stare davvero male, dopodiché ho dovuto interrompere. La pediatra di base che vide per la prima volta mia figlia decretò che avrebbe avuto un QI più basso della media, perché \"è scientificamente dimostrato che il latte materno migliora in quoziente intellettivo dei piccoli\". Certo, è vero, lo aumenta tanto quanto l'affetto dei genitori, gli stimoli intellettuali, la cura per l'educazione e per l'alimentazione nel corso di tutta l'infanzia e tante altre cose che mi sono premurata di darle... Qualche tempo fa, intorno alla questione dell'allattamento e del ritorno al lavoro si è scatenata su Facebook una discussione accesa tra colleghe: c'era chi rivendicava il diritto-dovere di stare a casa con i figli nel primo anno di vita (o per dirla col nostro ministro dell'Istruzione, il privilegio    ) e chi, come me (ma anche come molte altre) riteneva di avere un  diritto uguale e contrario di tornare al lavoro, dal momento che per fortuna non facciamo lavori usuranti e siamo fisicamente in grado di essere al contempo madri e lavoratrici. Anche in questo caso si è sviluppato un dibattito tra le \"naturaliste\", per dirla con la Badinter, che ritenevano di avere la biologia dalla loro parte (i piccoli umani sono la specie più bisognosa di cure parentali) e le \"culturaliste\", che sostenevano la possibilità di delegare in parte questi compiti pur rimanendo \"buone madri\". E dalla nostra parte c'è la storia, giacché il modello di accudimento esclusivamente materno prima, e oggi (seppure con qualche limite) più generalmente parentale è assai recente, e coincide con la nascita della classe borghese. Ad ascoltare le donne, quindi, come dice anche la filosofa francese, non esiste un solo modo di essere madri (né tantomeno la maternità è identificabile con l'iconografia della \"mamma italiana\"), ma ne esistono molti, compreso quello in negativo, cioè quello delle donne che per scelta non sanno cosa farsene della maternità e non vivono certo questo fatto con tormento. Eppure la società e la politica sembrano proporre un modello univoco, e in questo c'è ancora molto da lavorare, in effetti. Per non parlare della psicologia, che ha pure la sua quota di colpe, e che porta avanti un'idea di maternità \"sana\" che è frutto, anche storicamente,  di una visione maschile della psiche femminile. Su una cosa, però, non concordo con la Badinter: secondo lei noi madri di oggi vogliamo troppo per i nostri figli e, di conseguenza, cerchiamo di essere perfette, qualunque sia il nostro modello educativo: un \"effetto collaterale\" dell'acquisito controllo della fertilità. Le nostre nonne, e in gran parte anche le nostre madri, hanno vissuto l'arrivo dei figli come un evento ineluttabile; noi spesso aspettiamo il momento giusto, il compagno giusto. I nostri sono figli scelti, voluti, e questo ci porta a sentirci maggiormente responsabili nei loro confronti. Il più delle volte non ci cascano addosso all'improvviso, ma sono una tappa fondante della nostra identità, del nostro modo di stare a mondo e di lasciare traccia di noi stessi. Ora, leggendo \"Le conflit\" sembrerebbe che il bisogno di dare il meglio a questa prole così preziosa sia una forma di schiavitù, ma su questo dissento. Penso invece che, dal punto di vista etico, l'aver scelto di mettere al mondo nuovi esseri umani ci renda davvero più responsabili di ciò che offriamo loro di quanto non fossero i nostri nonni, che i figli li accoglievano come \"dono del Signore\", salvo chiedersi, a un certo punto, se non c'era modo di mettere un freno a cotanta grazia."},
{"title": "Psicopatici, criminali e conflitti d'interesse", "text": "Cosa accade quando gli scienziati si pongono in una posizione di potenziale conflitto di interesse e agiscono legalmente contro altri scienziati a difesa delle proprie teorie? Un'idea possiamo averla dalla vicenda che appassiona da ormai tre anni gli esperti di psicologia e psichiatria forense, che hanno seguito con attenzione la controversia che oppone Robert Hare a due colleghi, Jennifer Skeem e David Cooke. Robert Hare è un criminologo dell’Università della British Columbia, autore della Psychopathy Checklist Revised (PCL-R), un test ampiamente usato, specie nelle corti statunitensi, per valutare se un criminale che ha commesso atti violenti come omicidi o violenze sessuali (ma anche rapine ripetute) è anche uno psicopatico. La psicopatia è infatti, secondo Hare, una forma particolarmente grave di disturbo antisociale di personalità caratterizzato, oltre che da una generale mancanza di empatia verso gli altri, da capacità manipolatorie e da spregio delle regole, anche da una tendenza alla condotta criminosa recidivante. È importante ricordare che la psicopatia non fa ancora parte della classificazione ufficiale delle malattie psichiatriche raccolta nel DSM-IV-TR, dove trova posto soltanto il disturbo antisociale di personalità. Il test messo a punto da Hare è molto utilizzato soprattutto negli Stati UNiti e nei casi in cui c’è in ballo la pena di morte, poiché le giurie sembrano negativamente influenzate dalla presenza di una diagnosi di psicopatia che, secondo gli studi effettuati da Hare, si correla significativamente con un’elevata tendenza alla recidiva. In sostanza, è più facile che uno psicopatico sia condannato a morte. Somministrare il test di Hare richiede però un training particolare, poiché si tratta di uno strumento diagnostico sottoposto a copyright. I corsi che abilitano alla valutazione degli psicopatici sono diventati un discreto business, dal momento che spesso il test fa parte di una perizia sottoposta a cross-examination (cioè all’interrogatorio incrociato di accusa e difesa che spesso si vede nei telefilm americani). Formarsi presso la scuola del creatore del test (l'unica ufficialmente riconosciuta) è quindi considerato un elemento importante di accredito del perito stesso.  Il problema è che l'esistenza stessa della psicopatia come entità nosologica è molto controversa, poiché costituisce una diagnosi di potenziale pericolosità sociale non recuperabile. Nel 2007 due psicologi dell'Università della California, Skeem e Cooke, hanno somministrato a un gruppo di criminali una checklist alternativa a quella di Hare e sono giunti alla conclusione che psicopatia e criminalità non vanno necessariamente a braccetto. I loro risultati sono frutto di uno studio effettuato su oltre 1000 soggetti adulti. Infatti affermano: “Psicopatia e comportamento criminale sono due costrutti distinti. Se dobbiamo comprendere in che relazione si pongono, seppure esiste tale relazione, è essenziale che questi costrutti siano misurati separatamente. Questo è particolarmente importante nel contesto di progetti come il Dangerous and Severe Personality Disorders Project, nel quale gli individui sono soggetti a limitazioni della libertà in base all’assunto che esiste un collegamento funzionale tra il loro disturbo di personalità e il rischio che presentano”. Il progetto citato da Cooke è descritto in questo sito: si tratta in sostanza di un’iniziativa sperimentale di confinamento in luoghi protetti per i soggetti che risultano postivi al test di Hare. Come ho detto prima, lo studio di Skeem e Cooke è del 2007 - ed è stato sottoposto a regolare processo di peer-review e accettazione in quell’anno, ma è stato pubblicato solo oggi su Psychological Assessment e corredato da un commento firmato dallo stesso Hare. Questo perché, al momento della sua accettazione, Robert Hare ha minacciato gli autori e l’editore, anticipando la richiesta di eventuali danni economici derivanti dalla pubblicazione di un lavoro che mette in dubbio la teoria alla base del suo test. Tra gli addetti ai lavori la storia è ben nota, anche perché Cooke ha in parte raccontato i risultati dello studio censurato in un libro uscito l’anno scorso, ma è probabile che il lavoro originale abbia visto finalmente la luce anche grazie al lavoro di denuncia di John Travis, un giornalista scientifico che scrive su Science. Qui trovate l’articolo che ha scritto oggi sulla conclusione della vicenda. Certamente si tratta di una pagina buia di scienza: se i ricercatori cominciano a farsi causa per bloccare la pubblicazione di studi regolarmente sottoposti a peer-review, che fine fa la libertà di ricerca? In questa storia, tra l’altro, fa una pessima figura anche l’American Psychological Association, ovvero l’editore di Psychological Assessment, poiché ha ceduto al ricatto. Io credo, però, che la vicenda costituisca anche un campanello d’allarme per un settore importante della ricerca neuroscientifica, che è quello della criminologia. Ieri, a Milano, ho partecipato a bellissimo un corso di formazione in neuroscienze per magistrati organizzato dallo European Center for Life Sciences and the Courts dell’Università di Pavia in cui è stato presentato uno studio di neuroimmagine condotto da un gruppo finlandese insieme ad alcuni ricercatori di Brescia e pubblicato su Human Brain Mapping.  Basandosi sulla checklist di Hare, questi psicologi hanno esaminato con la risonanza magnetica i cervelli degli psicopatici riscontrando una diversa struttura degli ippocampi. Lo studio in questione, che non ho avuto modo di analizzare nel dettaglio, presenta, come peraltro altri analoghi, molti aspetti interessanti ma altri inquietanti, che sollevano interrogativi sull’impatto sociale di certe scoperte. È quindi particolarmente importante che il dibattito sugli strumenti di valutazione delle alterazioni della personalità nei criminali - proprio per le ricadute che la diagnosi ha sul soggetto psicopatico ma anche sulla società intera - sia il più libero e trasparente possibile. Questo significa che va condotto in assenza di conflitti di interesse economici ma anche di pregiudizi ideologici nei confronti dei criminali, anche dei più efferati.  Altrimenti vuol dire che non abbiamo imparato nulla dalla vicenda umana e scientifica di personaggi come Cesare Lombroso, che hanno applicato il metodo scientifico ma hanno letto i risultati ottenuti attraverso la lente deformata della loro concezione morale e sociale."},
{"title": "Il gioco dei libri", "text": "Reduce da un certo numero di giorni di malattia che mi hanno costretta a casa, ho fatto un po' il punto sulle riserve letterarie per l'estate, scoprendo che il riposo forzato le ha abbondantemente consumate. Mi tocca quindi fare un giro tra libreria e biblioteca comunale in vista delle vacanze. Così ho pensato di proporvi un gioco utile e dilettevole al contempo: creiamo insieme la nostra biblioteca ideale di Mente e Psiche. Il meccanismo è semplice e non l'ho certo inventato io: un paio d'anni fa sul blog Letteratitudine si è parlato, per esempio, di letteratura della follia. Ma io vorrei ampliare gli orizzonti. Qualsiasi libro riguardi il cervello, il suo funzionamento, la mente, la psicologia, la malattia psichiatrica eccetera è ammesso. Vanno bene le novità come i superclassici. Non è necessario che sia un saggio, anzi, sono benvenuti romanzi e poesie. Per ogni titolo mettete tre righe di spiegazione per chi non conosce l'opera (e magari la ragione per cui la consigliate). Se vi vengono in mente più libri, fate un post distinto per ciascuno di essi (così magari potete anche mettere un link senza essere bloccati dall'antispam che diffida dei post con molti link). Sono ammessi anche i libri non in italiano, se si tratta di opere non tradotte.  Ovviamente comincio io: Categoria saggi: Inventare la mente di Chris Frith. Un bellissimo saggio che racconta, con un linguaggio chiaro e una prosa gradevole, perché secondo l'autore le neuroscienze hanno determinato la fine della dicotomia mente-cervello. Attraverso una serie di esempi clinici in cui è compromessa la coscienza (del corpo, di sé, dello spazio) Frith porta naturalmente il lettore alla conclusione che siamo il prodotto del nostro cervello. The body has a mind of its own di Sandra and Matthew Blakeslee: un saggio bellissimo, scritto da una giornalista scientifica americana e dal figlio che si occupa di neuroscienze cognitive. Fa il punto sul rapporto mente-corpo visto dalle neuroscienze. Un'occasione unica per conoscere, grazie al linguaggio semplice, le scoperte delle neuroscienze su come cervello e corpo interagiscono per consentirci l'interazione con l'ambiente (in inglese). Categoria romanzo (psicologia e psichiatria): Follia di Patrick McGrath. Non è un libro recente, ma rimane uno dei romanzi più intensi che io abbia mai letto sulla malattia mentale, ma anche sulla passione. La campana di vetro di Sylvia Plath. L'unico romanzo della poetessa americana narra la storia di una donna affetta, come lei, da psicosi maniaco-depressiva. La trama segue la lenta discesa della protagonista nella malattia mentale, con un parallelo evidente con la vita della scrittrice, morta suicida all'età di 31 anni. L'amore fatale di Ian McEwan. Uno dei romanzi più noti di McEwan, il cui protagonista diventa l'oggetto delle ossessioni amorose di uno sconosciuto incontrato per caso durante un rocambolesco salvataggio. Questo sconosciuto è però affetto da sindrome di de Clérambault, una forma di erotomania spesso associata alla schizofrenia. Chi ne è affetto è convinto di provare un amore intenso e ricambiato per un'altra persona, indipendentemente dall'atteggiamento di questa. Categoria romanzo (neuroscienze) Brooklyn senza madre di Jonathan Lethem. Una detective novel il cui protagonista è affetto da sindrome di Tourette, il che ne fa un investigatore alquanto atipico. Il fabbricante di eco di Richard Powers. Al risveglio dal coma, provocato da un incidente automobilistico, il giovane Mark è affetto da sindrome di Capgras, una rara alterazione neurologica che impedisce di associare il volto di una persona nota ai sentimenti e alle emozioni che questa persona suscita in noi. Metafora dell'incomunicabilità affettiva, la sindrome di Capgras diventa protagonista di questo intenso romanzo. Non ho selezionato necessariamente i libri migliori, ma solo i primi che mi sono venuti in mente. A questo punto la palla passa a voi. Attendo curiosa i vostri suggerimenti prima di dare il via allo shopping libresco."},
{"title": "Anche i moralisti frodano il prossimo", "text": "La notizia non ha avuto molta eco sulle pagine dei giornali italiani, ma negli Stati Uniti è stata ripresa addirittura dal New York Times, per ben due volte. Marc Hauser, professore di psicologia all'Università di Harvard e biologo evoluzionista, uno dei massimi esperti di neuropsicologia cognitiva comparata (la disciplina che confronta le prestazioni cognitive di umani e animali) avrebbe falsificato i dati di uno studio pubblicato nel 2002 sulla rivista Cognition nel quale dimostrava come le scimmie siano in grado di comprendere semplici regole algebriche e di discriminare tra diversi tipi di operazioni. Lo ha dichiarato pochi giorni fa il direttore di Cognition, lo psicolinguista Gerry Altman, che sul suo blog spiega nel dettaglio come è giunto a queste conclusioni e fa anche un interessante distinguo tra il concretto di frode scientifica e di errore nel disegno sperimentale, esprimendo pubblicamente la difficile situazione in cui si è trovato nel momento in cui ha dovuto prendere una decisione sul caso in questione. La colpa di Hauser sarebbe quella di aver falsificato l'esperimento di controllo. Gli studi di psicologia cognitiva prevedono infatti che gli animali siano sottoposti a uno stimolo test il cui risultato deve essere confrontato con quello ottenuto in risposta a uno stimolo non significativo. Questo disegno sperimentale è simile a quello che si utilizza in altre situazioni, come nella ricerca farmacologica, dove l'efficacia di una nuova molecola viene messa alla prova confrontandola con gli effetti di una sostanza inerte, il placebo. Lo scopo di tutto ciò è di essere certi che il processo analizzato, o l'effetto riscontrato, non siano frutto del caso. Come mai nessuno se ne è accorto prima e perché ci sono voluti otto anni perché Cognition, una rivista tra le più serie e sottoposta a processo di revisione tra pari, annunciasse ufficialmente il ritiro dell'articolo? Semplicemente perché nessun esperto aveva pensato di ripercorrere passo dopo passo l'analisi statistica che aveva dato luogo ai risultati: mentre le risposte allo stimolo test hanno una distribuzione sufficientemente casuale da apparire reale, le risposte alla prova di controllo mostrano un'anomala regolarità che il più delle volte è data dal fatto che sono inventate di sana pianta (oppure, nell'interpretazione più benigna, che sono frutto di un esperimento disegnato molto male). Il ricontrollo è stato fatto solo perché, nel 2007, alcuni studenti di Hauser lo hanno denunciato al rettore di Harvard ritenendo di essere stati spinti a trarre conclusioni che non convidevano al solo scopo di pubblicare su riviste più prestigiose. Dopo tre anni di indagine, l'Università ha deciso, alla fine di agosto, di sospenderlo per un anno, poiché ha riscontrato \"otto gravi esempi di cattiva condotta scientifica\". Cosa si celi esattamente dietro questa formula sibillina al momento non è dato sapere, perché l'accusato ammette di aver commesso \"errori\" ma non di aver frodato la comunità scientifica.  Qualche avvisaglia che avrebbe dovuto destare sospetti, a dire il vero, c'era stata. Nel 1995 Hauser aveva pubblicato uno studio sui tamarini di Edipo, un primate sul quale ha fatto la maggior parte dei suoi esperimenti, nel quale affermava che questi erano in grado di riconoscersi allo specchio. Un altro psicologo, Gordon Gallup, aveva chiesto una videoregistrazione e dopo un po' Hauser aveva ammesso di non essere riuscito a replicare i risultati. Anche nel caso dello studio di Cognition ci sono dei videotape nei quali si vedono le scimmie impegnate solo nella condizione di test e non in quella di controllo. D'altronde chi va a guardare i video dei controlli? La parte interessante di uno studio è sempre quella in cui si trova ciò che si cerca! E poiché non sono molti gli studiosi che nel mondo hanno a disposizione colonie di primati per studi di questo genere, Hauser ha potuto pubblicare i suoi articoli senza temere troppo che ad altri venisse in mente di rifare tal quale il suo lavoro. Ora, poiché molti dei suoi esperimenti arrivavano alla conclusione che le scimmie si comportano spesso come i piccoli umani, se si dimostrerà definitivamente che la pratica della falsificazione è una costante del suo lavoro, bisognerà anche rivedere molte convinzioni sulle funzioni cognitive dei primati. Nella storia di Hauser ci sono diversi elementi che, umanamente parlando, lasciano attoniti (in primo luogo, per chi legge la vicenda dall'Italia, l'idea che la denuncia sia partita dai suoi stessi dottorandi!). Si tratta infatti di uno studioso di primo piano, uno dei padri della teoria della morale naturale. Per molti anni, infatti, si è dedicato allo studio delle basi della morale negli animali e negli uomini, riscontrando nella nostra specie una sorta di \"codice innato\" (che lui chiama grammatica morale, per analogia col concetto di grammatica generativa di Noam Chomsky) condiviso a tutte le latitudini e indipendentemente dagli aspetti culturali. Tra queste regole di base si annoverano, per esempio, la riprovazione per l'omicidio, la distinzione tra danno intenzionale e accidentale e quella tra danno per azione (più grave) e danno per omissione (meno grave). Il suo pensiero sulla morale naturale è ben riassunto in un libro di grande successo, Menti morali, edito in Italia da Il Saggiatore, nel quel spiega benissimo anche la distinzione tra conoscenza della morale e rispetto delle regole morali. In sostanza, tutti sappiamo che frodare il prossimo non è un gesto elegante, ma alcuni lo fanno anche se sanno di compiere un atto riprovevole e di rischiare l'espulsione dalla comunità scientifica."},
{"title": "Test genetici: aspetti sociali e percezione del rischio", "text": "Forse chi mi legge da un po' ricorderà un'iniziativa di cui ho parlato l'anno scorso, il progetto Brain in Dialogue coordinato dal Laboratorio interdisciplinare della SISSA di Trieste. Questa settimana, presso l'ICTP di Trieste, si è tenuto il secondo workshop internazionale: una sessantina di partecipanti hanno discusso degli aspetti scientifici, etici e sociali legati all'introduzione dei test genetici per la diagnosi delle malattie neurologiche. Ormai da alcuni anni si sa che malattie come l'Alzheimer e il Parkinson hanno una base genetica, specie quando ricorrono frequentemente nella stessa famiglia ma non solo. Non tutti i geni coinvolti sono noti e, nel caso delle due malattie di cui sopra, (come di tutte quelle multifattoriali) quelli già identificati non determinano con certezza la comparsa della malattia ma solo una maggiore predisposizione a svilupparla. Diverso è il caso di patologie a trasmissione mendeliana (cioè determinate da un unico gene e in cui i figli di una persona portarice hanno il 50 per cento di probabilità di ammalarsi): in quel caso il test permette, nella stragrande maggioranza dei casi, di conoscere con certezza il proprio destino e, da alcuni anni, anche la fascia di età in cui compariranno i primi sintomi. Genetisti, medici, sociologi e comunicatori di tutta Europa sono stati inviati a Trieste per chiarire i punti controversi di queste nuove tecniche diagnostiche e, soprattutto, gli eventuali limiti che le istituzioni devono imporre per regolamentarle. Ormai la tecnologia di decodifica del DNA ha fatto passi da gigante, e un'analisi mirata alla ricerca di un gene specifico costa poche centinaia di euro. L'analisi dell'intero genoma di un individuo dovrebbe venire a costare, nel giro di un paio d'anni, al massimo 2000 euro. È facile capire che questi test costituiscono un business lucroso, tanto che già proliferano i laboratori che offrono le analisi più disparate mentre sul web alcune compagnie commercialmente molto aggressive come la DecodeMe e la 23andMe (ma anche molte altre) promettono di svelare, attraverso il DNA, quale sarà il nostro destino non solo per via delle patologie future, ma persino dei talenti individuali.  I punti critici di questa pratica sono molti: innanzitutto la qualità dei test offerti non è sempre verificabile, né lo sono i database in base ai quali ogni singola compagnia afferma che il campione che ha tra le mani ha un rischio più elevato della media di sviluppare una determinata malattia. Hans Scheffer, genetista olandese responsabile del progetto europeo Techgene, ha ricordato che anche nei test genetici si riscontrano risultati falsamente positivi e che la prima versione del genoma umano pubblicata nel 2003 conteneva lo 0,1 per cento di errori: sembra poco, ma sono circa 6 milioni di basi, e se si considera che basta una sola mutazione puntiforme per indurre la comparsa di una malattia, i limiti di questi test appaiono chiari. Inoltre, almeno per quanto riguarda le patologie neurodegenerative di cui sopra, una volta che un individuo ha scoperto di essere più a rischio della media della popolazione non può fare molto né per prevenire l'insorgenza della malattia né per rallentarne il decorso. Si tratta infatti, nella stragrande maggioranza dei casi, di patologie a comparsa tardiva: tra il risultato del test e le prime manifestazioni del male possono passare anche decenni. La genetista italiana Marina Frontali, una delle massime esperte di corea di Huntington, ha raccontato il lungo lavoro di counselling che deve accompagnare la decisione di sottoporsi a questo genere di esame, un lavoro che certo le compagnie che forniscono test \"a distanza\" non sono in grado di fare. Ricevere una diagnosi come quella di Huntington non è certo facile, eppure c'è chi ha preferito sapere, per organizzare la propria vita di conseguenza (per esempio decidendo di non avere figli per non perpetuare una malattia che spesso è all'origine di tragiche storie familiari). Altri, invece, hanno preferito non conoscere con certezza il proprio destino, per timore che la diagnosi impedisse loro di fare una vita normale finché possibile. Tra le persone intervenute al workshop c'era anche Verena Schmoker, una paziente con una forma familiare di Parkinson a insorgenza precoce che non ha mai voluto fare i test genetici, così come non desiderano farli le sue due figlie, una delle quali, ventenne, era presente tra il pubblico. Secondo la signora Schmoker, - che in Svizzera ha fondato un'associazione per il sostegno dei pazienti con forme giovanili di Parkinson - dal momento che non si può fare nulla non ha senso sottoporsi a un test. L'informazione relativa al proprio rischio aumentato di ammalarsi può essere accessibile ad altri, per esempio alle assicurazioni sanitarie o ai datori di lavori, determinando situazioni di discriminazione o, peggio, di esclusione sociale. Non a caso molti relatori hanno citato opere letterarie come il Mondo nuovo di Aldous Huxley o film come Gattaca per evocare il rischio di una società in cui il destino dell'individuo risulti predeterminato dalla sequenza di basi contenute nel suo genoma. La questione non interessa però solo l'individuo, ma anche la società tutta: a parte gli aspetti legati alla privacy, vi sono anche questioni etiche e legali sulle quali i governi sono chiamati a legiferare. Ne ha parlato a lungo la bioeticista ungherese Judith Sandor: è giusto, per esempio, sottoporre a questi test un minore se si tratta di individuare un rischio che non è in alcun modo riducibile? In che modo il fatto di sapere che il proprio figlio è portatore di un gene che lo predispone a una malattia neurologica influenzerà l'atteggiamento dei genitori e il tipo di educazione che è destinato a ricevere? È lecito chiedere di sottoporre un embrione a una diagnosi preimpianto in caso di malattia multifattoriale, in cui il test non può dare alcuna certezza sul fatto che la mutazione indurrà effettivamente la patologia? Non si rischia di reintrodurre in Europa lo spettro dell'eugenetica, questa volta non su base coercitiva ma volontaria? E quando la società tutta spinge l'individuo portatore di una mutazione potenzialmente patogena a sottoporsi all'esame e a comportarsi di conseguenza, qual è il confine tra decisione autonoma e coercizione sociale? Come vedete, le questioni in ballo sono tante e complesse. Una però, mi pare molto chiara: non è possibile decidere con sicurezza se sottoporsi o meno a questo tipo di esame, quando è indicato sulla base del proprio profilo genealogico, se non si comprende appieno il concetto di \"rischio\". Su questo punto tutti gli esperti sono stati concordi: la società in cui viviamo non è abituata a fare i conti con il concetto di rischio né a stimarne l'entità reale. Abbiamo paura di molte cose che statisticamente ci colpiranno raramente mentre non siamo capaci di vedere il rischio che corriamo tutti i giorni. Con i test genetici, il problema è lo stesso: se anche abbiamo un rischio cinquanta volte maggiore della media di ammalarci di una patologia rara, in termini assoluti questa resterà un'evenienza remota. E anche quando ne capiamo la portata, c'è una dicotomia tra la valutazione razionale e quella emotiva, come spiega bene un filmato che proprio oggi mi ha segnalato un amico e che, anche se non parla nello specifico di test genetici, rende bene l'idea di quanto sia complesso fare i conti con l'incertezza.  Dopo questa tre giorni intensa, io ho più dubbi di prima (il che significa che discutere serve sempre!): penso che dal punto di vista scientifico i test genetici sono uno strumento formidabile per conoscere i meccanismi molecolari che determinano la comparsa di una malattia. Ma penso anche che possono essere strumenti inutili, se non pericolosi, quando vengono usati da un individuo per ottenere un'informazioni fine a se stessa. Vi sono però delle eccezioni: gli interventi di prevenzione, nel caso di malattie che porteranno alla perdita dell'autonomia o della memoria, non sono solo farmacologici o sugli stili di vita. A volte può essere utile prepararsi psicologicamente a un tale destino, sempre che si sia in grado di gestire l'ansia che inevitabilmente accompagna certe consapevolezze."},
{"title": "Stupore e tremori", "text": "Ebbene sì, ho rubato il titolo a Amelie Nothomb, ma mi è venuto spontaneo leggendo un articolo uscito un paio di giorni fa sul New York Times che racconta la ricerca di due psicologi sociali dell'Università della Pennsylvania, Jonah Berger e Katherine A. Milkman (dalla pagina del NYT potete scaricare il pdf che purtroppo io non ho modo di allegare). Analizzando la lista degli articoli più \"inviati\" dal sito del NYT (in sostanza quelli che le persone spediscono più di frequente ad amici e conoscenti per condividerne i contenuti) hanno scoperto che, contrariamente a quanto immaginavano, non sono quelli più \"utili\", cioè quelli con un contenuto pratico o informativo immediato, ma quelli che suscitano più stupore. In realtà il termine che hanno usato è \"awe\", intraducibile in italiano perché comprende sia il concetto di stupore sia quello di timore e paura.  L'idea di valutare quanto \"stupefacenti\" fossero gli articoli più condivisi è nata quando si sono accorti che se il 20 per cento dei pezzi più inoltrati ad altri faceva parte di quelli pubblicati in home page, questa percentuale saliva al 30 per cento per gli articoli di scienza, compresi alcuni con titoli che gli stessi ricercatori definiscono \"poco empatici\" (per esempio \"Le promesse e le potenzialità dell'RNA\"). Un campione di 3.000 articoli è stato analizzato da lettori indipendenti che ne hanno valutato la carica di stupore del contenuto, ma anche da un algoritmo computerizzato che ha valutato il rapporto tra numero di parole e parole con contenuto emozionale. Dal canto mio, sono contenta che la scienza, almeno negli USA, riesca ancora a sucitare tali emozioni. I due psicologi sono peralto esperti di \"stupefazione\". Nel 2003 hanno scritto una bella revisione su Cognition and emotion intitolata \"Approching awe, a moral, spiritual and aesthetic emotion\". Definiscono così lo stupore: \"Due elementi sono centrali e presenti in tutti i casi di stupore: la percezione della vastità, definita come incapacità di assimilare un'esperienza sulla base delle proprie strutture mentali attuali e il bisogno di trovare il modo adattarvisi. Cinque ulteriori elementi danno conto della variazione del tono edonico delle esperienze di stupore: timore, bellezza, abilità ecezionali, virtù e attribuzioni sovranaturali\". Condividere le ricette di cucina, i suggerimenti finanziari o i consigli medici è sensato in base alle teorie classiche sull'utilità economica, ma fare altrettanto con articoli che ispirano stupore, fanno sentire piccoli o, viceversa, immensamente speranzosi per il futuro è meno spiegabile, dice Berger, che ha però una sua interpretazione: le emozioni, come dimostrano diversi studi di psicologia, tendono a propagarsi (ne abbiamo parlato anche qui). E il web è un formidabile strumento di propagazione, tutt'altro che emotivamente asettico. Se volete discutere con il giornalista John Tierney del modo col quale condividete le informazioni (e con chi), potete farlo sul suo bellissimo blog. Ma prima, ovviamente, passate di qui"},
{"title": "L'elenco delle malattie psichiatriche si apre ai commenti", "text": "Il 10 febbraio scorso l'American Psychiatric Association (APA) ha reso pubblica la bozza di revisione della quinta edizione del DSM (Diagnostic and statistical manual of mental disorders), il manuale diagnostico delle malattie psichiatriche e mentali. Per chi non lo sapesse, il DSM, la cui prima edizione risale al 1952, è un corposo volume che raggruppa e classifica tutti i disturbi mentali, indicando per ciascuno di essi i segni e sintomi che ne consentono la diagnosi. L'American Psychiatric Association, che da sempre si occupa della sua compilazione e revisione, ne promosse la stesura per cercare di dare alla psichiatria gli stessi criteri di certezza diagnostica su base scientifica su cui può contare in genere la medicina. Le malattie mentali, infatti, non essendo quasi mai diagnosticabili sulla base di alterazioni di parametri fisiologici documentabili con un semplice esame del sangue o una radiografia, erano state, fino a quel momento, in balia della \"mania definitoria\" dei singoli medici. Il tentativo di fare ordine e di fornire parametri oggettivi era (ed è) quindi un'iniziativa assolutamente necessaria, anche se il principale criterio utilizzato per creare le liste di sintomi in base ai quali determinare la diagnosi era (ed è ancora oggi, per gran parte delle patologie) quello statistico \"a posteriori\": si valuta, in sostanza, di cosa soffrono i pazienti che già hanno ricevuto una diagnosi e, soprattutto, quali sono state le manifestazioni di esordio della malattia. Nel corso degli anni, il DSM è stato più volte riveduto e corretto, perché risente ovviamente non solo delle scoperte della scienza ma anche dell'evoluzione del costume. Basti pensare che l'omosessualità è stata a lungo annoverata tra le parafilie (i disturbi della sessualità) e che solo nel 1974, grazie a una votazione dei membri dell'APA, è stata esclusa dall'elenco delle patologie mentali. Ma non sono solo questi i limiti dello strumento: essendo compilato in base a dati provenienti da pazienti americani o comunque occidentali, non è adatto a rilevare sintomi di diasgio fortemente connotati dall'appartenenza a culture \"altre\", che rimangono oggetto di studio della cosiddetta etnopsichiatria, benché i fenomeni migratori abbiano in parte modificato la loro incidenza anche in Occidente. Un altro punto critico è quello dei test diagnostici e delle scale di valutazione psichiatrica, che vengono modellate sulla base dell'elenco di sintomi presenti nel DSM e, poiché servono a diagnosticare i malati del futuro, contribuiscono alla cristallizzazione dei criteri diagnostici in una sorta di loop tautologico. Nel 2013 vedrà quindi la luce la quinta versione del DSM: sarà molto simile alla bozza resa pubblica in questi giorni, ma potrà ancora essere modificata sulla base dei commenti che arriveranno ai compilatori. Non ho avuto modo di guardarla con attenzione, ma tra i cambiamenti maggiori si segnalano l'abolizione delle diverse forme di autismo (che vengono raggruppate sotto un'unica definizione di sindromi dello spettro autistico) e la generale diminuzione delle patologie, accorpate sotto ombrelli più ampi di quelli attuali. È il caso, per esempio, delle dipendenze, un tempo categorizzate in base all'oggetto della dipendenza (alcol, droga, sesso, gioco d'azzardo) mentre nella nuova versione si privilegia il concetto stesso di dipendenza, dal momento che ora è chiaro che i confini tra l'una e l'altra sono spesso labili. In questo caso, tra l'altro, le scoperte delle neuroscienze hanno contribuito al cambiamento, dimostrando che i circuiti cerebrali che attivano e mantengono una dipendenza sono fondamentalmente gli stessi a prescindere dall'oggetto da cui si dipende. Perché aprire a tutti i commenti al DSM invece di lasciarne l'onere e l'onore agli specialisti? La ragione principale è che le malattie psichiatriche non sono come le altre, e le etichette che vengono attribuite alle persone che ne soffrono possono avere un peso determinante sul destino di un individuo. Una settimana fa, per esempio, l'associazione statunitense dei pazienti con sindrome di Asperger (che semplificando si può definire una forma lieve di autismo) ha protestato per la cancellazione della propria patologia come entità autonoma, principalmente a causa dello stigma legato alla diagnosi di autismo tout court. Ugualmente ci sono proteste per l'introduzione di una nuova categoria che raggruppa le persone \"a rischio\" di sviluppare un disturbo mentale: molto utile per facilitare la diagnosi precoce, ma socialmente piuttosto pericolosa. È però necessario che una malattia venga definita come tale perché possa essere presa in carico dai sistemi sanitari o dalle assicurazioni private. Per questo esiste una forte spinta (spesso pilotata dalle farmaceutiche) a includere nel DSM disturbi di natura ancora non del tutto chiara, come alcune forme lievi di depressione (fece un discreto scalpore a suo tempo l'inclusione nel DSM IV della distimia, una forma di alterazione dell'umore di tipo depressivo che è difficile distinguere da un periodo di difficoltà \"fisiologica\") oppure come i disturbi della libido femminile, intorno ai quali si gioca una partita economicamente interessante. Molte farmaceutiche hanno infatti cercato il cosiddetto \"viagra rosa\", un farmaco in grado di bissare il successo di quello maschile: per ora la maggior parte delle sostanze identificate si sono dimostrate di scarsa efficacia, ma se qualcuno riuscisse finalmente a governare il desiderio femminile sarebbe difficile riuscire a vendere un farmaco per un disturbo \"non ufficiale\". Anche per questo coloro che stanno lavorando alla stesura del nuovo DSM hanno accettato di limitare le proprie entrate provenienti da industrie del farmaco a soli 10.000 dollari l'anno per tutta la durata della compilazione (oltre dieci anni): può sembrare molto, ma non è così, se si considera che in questa somma sono compresi anche i finanziamenti per la ricerca. Insomma, fino al 20 aprile tutti possono dire la loro. Sarà interessante scoprire come internet modificherà la modalità di compilazione della bibbia della psichiatria. A modo suo, questo è anche uno dei primi esperimenti di democrazia partecipata nella scienza."},
{"title": "Chi perde il lavoro perde se stesso?", "text": "Questa mattina un'amica psicologa incontrata per caso aveva in mano Mente e Cervello dello scorso mese, che in copertina riportava il un mio articolo sugli effetti psicologici della disoccupazione, in particolare per quanto riguarda la perdita del proprio ruolo sociale. Non si era accorta che l'articolo era mio: era solo rimasta colpita dal testo perché proprio in questo periodo deve affrontare la questione con due suoi pazienti che hanno perso il posto di lavoro. \"Bisogna parlarne di più\", mi ha detto, confermando il senso di vergogna che le persone licenziate o senza reddito provano per la propria condizione e gli effetti negativi che questo tormento non condiviso ha sulla loro stabilità. Beh, se bisogna parlarne, allora questo è il posto giusto, meglio delle pagine di una rivista. Nel 2009 il tasso di disoccupazione, in Italia, ha sfiorato i 9 punti percentuale e nel primo trimestre dell’anno l’INPS ha visto crescere del 45 per cento le domande di indennità di disoccupazione. Una fotografia per difetto, se si considera che molte persone che lavorano come liberi professionisti, con contratti a termine o con altre forme di contrattualizzazione atipica possono non rientrare nelle statistiche. A trovarsi a casa, da un giorno all’altro, sono soprattutto i giovanissimi (sotto i 25 anni) e la fascia di età tra i 40 e i 55 anni, i più penalizzati nel momento in cui desiderano reinserirsi nel mondo del lavoro. Il primo pensiero, parlando di disoccupazione, va alla mancanza di una fonte sicura di reddito ma non è questa l’unica conseguenza: la perdita del proprio ruolo di elemento attivo della società ha ricadute importanti e a volte superiori a quelle della mancanza di denaro, come dimostrano le più recenti ricerche in materia. \"I primi studi sugli effetti sociali e psicologici della disoccupazione sono stati condotti tra le due guerre mondiali del secolo scorso e il loro focus era centrato solo sul rischio di povertà, specie di povertà estrema: si dava poco peso all’aspetto identitario del lavoro” spiega Duncan Gallie, docente di sociologia all’Università di Oxford e curatore del volume “Resisting marginalization - Unemployment experience and social policy in the European Union”, frutto di una ricerca multicentrica effettuata in diversi Paesi della EU per misurare gli effetti della perdita del posto. Oggi sappiamo che gli aspetti psicologici e sociali sono fattori che determinano, a volte in modo assolutamente consequenziale, la possibilità di rientrare nel ciclo produttivo. Dal punto di vista della psicologia sociale, l’individuo tende a costruire una rappresentazione di sé basata sui ruoli che sente propri e, in base a questi, sviluppa la sicurezza che gli consente la corretta integrazione sociale. La perdita del lavoro inciderà quindi su ambedue gli aspetti: il ruolo sociale e l’autostima. Non è difficile trovare on line gli sfoghi di chi si ritrova, da un giorno all'altro, senza nulla da fare. Ci sono operai, per esempio, che per un po' tornano tutte le mattine al bar davanti alla fabbrica, salvo gettare la spugna quando il rituale diventa troppo carico di rimpianti. Il sindacato e i comuni, a volte, mettono in piedi servizi di supporto psicologico per affrontare la precarietà, ma anche il vuoto e la perdita di ruolo. L'ha fatto, per esempio, il comune di Parma con un ciclo di incontri intitolati “Affrontare i risvolti psicologici della crisi”. Il sintomo comune a tutti coloro che perdono il lavoro, dicono gli esperti, è la concomitante perdita dell’autostima. E questa perdita inficia anche le capacità dell'individuo di cercare una via d'uscita, specie in frangenti economicamente difficili come quelli che stiamo vivendo. Ciò non dipende necessariamente dall’incarico che ricoprivano in precedenza, perché accade all’imprenditore che deve chiudere l’azienda come all’operaio o al commerciante. Anche se, ovviamente, la componente identitaria è maggiore in chi non riesce a immaginarsi in un altro ruolo se non in quello che ha perso. E questa sorta di \"immobilismo\" del ruolo lavorativo, l'incapacità di immaginarsi impegnati in qualcosa di totalmente diverso e nuovo è uno degli ostacoli che gli psicologi del lavoro tentano di rimuovere quando una persona fatica troppo a reinserirsi. È il caso, soprattutto, di dirigenti e liberi professionisti che, dicono le statistiche, non riescono ad accettare di cambiare tipo di lavoro e sono più portati a vivere con depressione l’improvviso vuoto quotidiano, fino a negarlo, con se stessi come con gli altri. Nel 1997 il regista Peter Cattaneo mise in scena gli effetti della crisi economica in Gran Bretagna nel film ‘Full monthy’. E con sensibilità di artista colse un elemento importante: mentre gli operai si disperavano per il quotidiano, ma in qualche modo facevano resistenza alla depressione, il loro capo, un colletto bianco, usciva ogni giorno di casa in giacca e cravatta e non osava dire alla moglie che non aveva più un lavoro. Se ne vergognava, come se fosse colpa sua.  “Non esiste un legame deterministico tra disoccupazione e altre dimensioni dell’esclusione sociale” spiega Gallie, forte dei dati della sua ricerca provenienti da una decina di Paesi europei. “L’impatto sulla persona e sulla percezione di sé deriva da differenze culturali e istituzionali, in particolare dal supporto familiare e dal sistema di welfare esistente”. Secondo l’indagine condotta dal suo gruppo, nel Nord Europa, dove il welfare funziona, la marginalizzazione del disoccupato non deriva da improvvisa povertà (perché i sussidi pubblici lo sostengono) ma, appunto, dalla perdita del ruolo e dalla sensazione di essere un “peso” per gli altri. Nell’Europa del Sud, invece, il sistema familiare protegge maggiormente il benessere psicologico di chi ha perso il posto, ma la mancanza di paracaduti materiali rende le preoccupazioni quotidiane paralizzanti. Che la società in cui si vive determini anche il vissuto in relazione alla perdita del lavoro lo conferma una ricerca condotta dal sociologo di origine israeliana Ofer Sharone, oggi al Massachusetts Institute of Technology di Boston. Nel 2005 Sharone ha studiato 100 colletti bianchi israeliani e statunitesi che avevano perso il posto. Si trattava soprattutto di manager e lavoratori del settore high-tech, persone che mai avrebbero pensato, negli anni del boom delle nuove tecnologie e degli stipendi d’oro, di trovarsi per strada. “Ho notato che c’erano differenze notevoli tra i due gruppi” spiega Sharone. “Gli israeliani, quando non trovano lavoro, tendono ad attribuirne la colpa alle istituzioni e al sistema, mentre gli americani incolpano se stessi. Questo fa sì che i primi reagiscano con più rabbia, ma anche con più energia, mentre i secondi tendono a perdere sempre più fiducia in se stessi. In sostanza, si dicono che se non trovano lavoro è perché c’è qualcosa di sbagliato in loro”. Sharone afferma che non si tratta di un effetto della cultura individualista che caratterizza gli  Stati Uniti: “In parte può darsi, ma non è solo questo. È anche un problema di struttura sociale. In Israele esiste una tutela del lavoro centralizzata, figlia degli anni della fondazione dello Stato, che aveva un impianto socialista. Ciò significa che se un’azienda ha bisogno di personale, si rivolgerà all’ufficio di collocamento che smisterà le proposte e favorirà l’assunzione. Questo, se da un lato può inibire l’iniziativa personale del lavoratore, dall’altra non lo priva del proprio ruolo. È sempre un lavoratore, solo che è momentaneamente senza impiego e tocca all’ufficio preposto aiutarlo a ricollocarsi. Negli Stati Uniti, invece, tutti dicono al disoccupato: devi avere più fiducia in te stesso, devi imparare a ricollocarti, devi parlare così, devi vestirti cosà. La manualistica fai-da-te, in questo campo, vende milioni di copie l’anno. Per non parlare dei corsi di self-help e dei fenomeni come il coaching che pretendono di “allenare” la persona ad affrontare le difficoltà. Quindi il lavoratore senza lavoro è un individuo senza ruolo, che si deve ricostruire, e per di più, se non riesce a trovare un nuovo impiego, evidentemente è perché non fa le cose giuste. Ma questo non è vero, perché ci sono anche le condizioni economiche esterne, che non sono modificabili a volontà”. Anche con la crisi globale, questo atteggiamento di auto-colpevolizzazione è duro a morire: “Se chiude una fabbrica o un’azienda, in Europa, i lavoratori protestano, bloccano il traffico, piantonano i cancelli… Negli USA non accade nulla di ciò. E non sono aumentati nemmeno gli iscritti ai sindacati. Un segno chiaro di questa visione totalmente individualista del lavoro, che è l’altra faccia del sogno americano”. Una rete di welfare efficiente consente anche di fare i conti con la realtà. “Uno dei problemi che hanno i disoccupati che ricoprivano incarichi prestigiosi o che godevano di autonomia intellettuale, è la difficoltà di adattare le proprie aspettative alle mutate condizioni economiche” spiega ancora Sharone. “Chi fa tutto da sé può trovare frustrante, alla fine, scoprire che un posto come quello che ha lasciato non esiste più e che quindi bisogna accettare di puntare più in basso o di cambiare completamente ambito lavorativo. Un buon ufficio di collocamento aiuta a fare questo percorso senza sentirsi sminuiti”. Si può essere felici accettando un lavoro al di sotto dei traguardi raggiunti in precedenza? Sharone pensa di sì, purché si viva ciò come un’opportunità di risalita oppure, semplicemente, come un passaggio, in attesa di tempi migliori. “Quel che non si deve fare è stare a casa se c’è l’opportunità di lavorare”. Già nel 1938 due  noti psicologi, Philip Eisenberg e Paul F. Lazarsfeld, descrissero la  situazione psicologica del disoccupato, individuando tre fasi. Nel primo  periodo si manifesta un rifiuto della nuova realtà: l’individuo stenta a  credere di aver perso il posto e si dice che, in un modo o nell’altro,  ne verrà fuori. Segue un periodo di pessimismo, quando dopo vari  tentativi non compare all’orizzonte nessuna nuova possibilità di lavoro.  È in questo momento che l’individuo comincia a dubitare: forse non ne  verrà mai fuori. Infine compaiono rassegnazione e ripiegamento su se  stessi, chiari sintomi depressivi. L’individuo si riconosce nel ruolo di  disoccupato cronico e si sente spacciato: è finita, non ne verrà mai  più fuori. Questa fase si instaura in genere dopo una media di nove mesi  di disoccupazione e richiede l’intervento di uno psicologo del lavoro o  la partecipazione a un corso di formazione professionale il cui scopo,  in genere, è quello di facilitare la conoscenza di se stessi e delle  proprie aspirazioni, acquisire nuove competenze professionali e imparare  a elaborare un progetto professionale efficace. Dicono le statistiche che, in tutta Europa, l’ultima ondata di licenziamenti ha colpito più gli uomini delle donne, perché queste ultime, in media, pesano meno sui bilanci delle società e ricoprono ruoli intermedi ed esecutivi. Questo ha fatto crescere le coppie in cui i classici ruoli di genere sono invertiti: lei manda avanti la famiglia, lui è a casa. Il giornalista di Economywatch.com, Vladimir Gonzales, ha chiamato questo fenomeno he-cession, con un gioco di parole tra il termine “he”, lui, e il termine “recession”, recessione. In questo caso, oltre alla perdita del ruolo di lavoratore, è anche l’identità di genere a entrare in crisi. Poiché mediamente l’uomo guadagna più della donna, la mancanza del suo stipendio si fa sentire anche nei casi più fortunati, e richiede sacrifici come la disdetta dell’asilo nido o della baby sitter per i figli, oppure la rinuncia alla badante per i genitori. L’uomo, di colpo, si trova a dover gestire la casa e la famiglia, senza essere preparato. E non può neanche contare sulla comprensione della compagna, che spesso non vede dove sta il problema: quando accade l’opposto, pare normale a tutti che la donna stia a casa per favorire l’equilibrio economico della famiglia. Il peso della “casalinghitudine”, però, esiste davvero, dal momento che le statistiche degli uffici collocamento italiani dicono che il 20 per cento delle donne che lavorano smette dopo la nascita dei figli e raramente riprende: non sempre perché non desidera rientrare nel mondo del lavoro, ma più spesso perché perde gli strumenti necessari, compresa l’abitudine a mostrarsi sotto la luce migliore, necessaria a fare bella figura nei colloqui di lavoro. Portare il denaro a casa è un elemento fondante dell’identità maschile, che anni di educazione alla parità dei ruoli hanno scalfito solo in parte: va bene condividere le responsabilità lavorative e persino quelle casalinghe, ma invertire i ruoli è ancora molto conflittuale. In Estonia, paese che, con la Finlandia e l’Islanda, ha visto crescere negli ultimi due anni la disoccupazione maschile a fronte di un maggior impiego di donne, è cresciuta parimenti la violenza sessuale e domestica. Un fenomeno che i Governi locali guardano con preoccupazione e per scongiurare il quale hanno messo in piedi costose campagne di sensibilizzazione. La disoccupazione è un dramma per tutti, non solo dal punto di vista economico ma anche psicologico. Il lavoro è correlato allo status sociale di una persona e costituisce la base sulla quale immaginare il proprio futuro. Le donne sono forse più protette dalla depressione e dalla perdita di autostima grazie alla possibilità di definirsi casalinghe, ma questo non cambia la realtà. Se non è una scelta volontaria, stare a casa, per un uomo come per una donna, è perdere una parte di sé. Una versione più lunga di questo testo è uscita sul numero di Febbraio 2010 di Mente e Psiche"},
{"title": "Giù le mani dalla mia maglietta!", "text": "Fratelli e sorelle possono essere una risorsa preziosa nella vita di un individuo oppure una vera e propria condanna, una fonte continua di stress e conflitti. Chiunque abbia più di un figlio si è chiesto se esiste una ricetta per rafforzare il legame tra fratelli, spesso senza trovare risposta. Conosco famiglie in balia di liti e malumori costanti, altre che invece hanno figli che vanno d'amore a d'accordo senza che vi sia una spiegazione apparente per queste differenze: non dipendono dalla differenza di età né dal sesso dei bambini. Per questo sono rimasta colpita da un piccolo studio pubblicato sulla rivista Child Development e condotto da due psicologhe dell'Università del Michigan.  Secondo la loro analisi, esistono litigi che sono indice di un futuro rapporto conflittuale e altri che invece sono parte della normale dialettica familiare. Analizzando il comportamento di un gruppo di 330 adolescenti, hanno scoperto che i conflitti che riguardano lo spazio personale (reale ma anche emotivo), come per esempio l'uso della camera, dei vestiti, l'intromissione di un fratello nella vita dell'altro, specie in presenza di amici o, peggio, di fidanzate o fidanzati, si accompagna a una cattiva capacità di comunicazione e pessimi rapporti nell'età adulta. Viceversa le liti per questioni di equità o di condivisione delle responsabilità (chi deve pulire o riordinare, chi ha avuto più soldi eccetera) non influenzano la profondità della comunicazione tra fratelli, né il loro affetto o la loro fiducia reciproca. Sono spesso i fratelli maggiori a lamentarsi di un'invasione dei propri spazi da parte del fratello minore, probabilmente perché il bisogno di autonomia, anche emotiva, è un elemento che caratterizza la crescita dell'individuo. Il consiglio delle autrici mi pare molto sensato: genitori ed educatori devono crescere i ragazzi nel rispetto degli spazi fisici e mentali degli altri membri della famiglia. Anche il senso dell proprietà ha un ruolo nella definizione di sé come individuo indipendente e sicuro, quindi è bene che ciascuno possieda i propri vestiti e i propri beni e che gli scambi vengano concordati.Altro punto importante è insegnare fin da piccoli l'arte della negoziazione dei conflitti:sedersi intorno a un tavolo e giungere a un compromesso è una cosa che diventa naturale solo se praticata fin da giovanissimi, anche perché, dopo una certa età, è meglio che i genitori si astengano dall'intervenire nelle liti dei figli perché, dicono le esperte, fanno più male che bene. C'è da dire che l'idea che il conflitto sia la condizione \"naturale\" della relazione fraterna è molto radicata in psicologia, a partire dalle numerose analisi del mito di Caino e Abele. Gli studi sull'attaccamento (in particolare quelli di Judy Dunn che hanno influenzato anche i lavori successivi sull'argomento) dicono poi che bambini con un attaccamento insicuro alla madre tendono ad avere una relazione più conflittuale con i fratelli, così come quelli esposti continuamente a valutazioni e paragoni tra loro. Nonostante ciò, anche nelle famiglie più sane la competizione per l'accudimento genitoriale, la lotta per avere tempo e attenzioni sono considerate situazioni inevitabili che modellano fin dalla più tenera età la relazione di fratellanza. Come ho detto, quello delle due psicologhe dell'università del Michigan è solo un piccolo studio curioso, ma offre uno spunto di riflessione interessante sia sull'inevitabilità del conflitto sia sul concetto di famiglia tradizionale, secondo il quale \"ciò che è mio è tuo e ciò che è tuo e mio\": suggerisce infatti un modello nuovo, in cui si rispetta il bisogno di ciascuno dei membri, anche di quelli più giovani, di essere individui autonomi. Anche nella famiglia più unita (anzi, proprio in quella che ha più chances di restare unita) ci sono silenzi che vanno rispettati e porte che devono restare chiuse."},
{"title": "Il risveglio dal coma e le tavolette dei medium", "text": "La notizia del paziente belga che, dopo 23 anni in stato vegetativo, si scopre essere pienamente cosciente grazie a una nuova valutazione neurologica ha fatto il giro del mondo. Il primo a pubblicarne la storia è stato Der Spiegel, ma trovate più o meno la stessa descrizione sul New York Times e su numerosi giornali italiani (vi cito solo La Stampa e Repubblica). Il documento certamente più impressionante è questa videointervista di BBC news. In realtà la scoperta del suo reale stato di coscienza è di tre anni fa, periodo durante il quale è stato sottoposto a intensa riabilitazione. Ho cercato quindi lo studio originale dove il suo caso sarebbe descritto, che tutti citano come pubblicato sulla rivista open acces BioMedCentral Neurology, dove in effetti si trova uno studio di coorte del luglio scorso che raggruppa 107 casi di pazienti con diagnosi di stato vegetativo e stato di minima coscienza firmata da Steven Laureys, direttore del Coma Science Group dell'Università di Liegi, in Belgio. Laureys è un esperto di neurimaging di chiara fama, che da anni porta avanti ricerche con l'obiettivo di elaborare criteri diagnostici più efficaci di quelli attuali per determinare cosa accade nella testa dei pazienti in coma acuto o in stato vegetativo. È anche il medico che, a detta di Rom Houben, il paziente in questione, gli ha ridato la vita.  Purtroppo il lavoro che è citato da tutti come referenza per la descrizione del caso non contiene, in realtà, alcun dato che permetta di conoscere realmente lo stato di salute, gli esami e la diagnosi dei singoli pazienti coinvolti, e infatti nessuno dei soggetti esaminati è riconoscibile. Le conclusioni, però, sono chiare: utilizzando un protocollo di valutazione più attento di quello attuale (che si basa sostanzialmente sulla Scala di Glasgow) combinato con le tecniche di neuroimaging i ricercatori avrebbero scoperto che il 27 per cento dei pazienti in acuto e il 44 per cento di quelli in stato di coma cronico sarebbero oggetto di diagnosi errate. Con una particolarità: l'89 per cento delle diagnosi sbagliate nei pazienti cronici riguarda persone in stato di minima coscienza e non in stato vegetativo (una distinzione recente che però è essenziale anche per la determinazione della prognosi e per le ricadute etiche e legali). Data la serietà del ricercatore, non vedo perché dubitare di questo studio, che tra l'altro conferma l'impressione di altri esperti. È ormai chiaro, comunque la si pensi sulle questioni etiche legate agli stati vegetativi, che con le nuove tecnologie è possibile perfezionare le diagnosi e che giunto il momento di stabilire nuovi protocolli che aiutino medici e familiari a valutare il reale stato delle persone in coma, in termini di prognosi e di assistenza. Il caso di Houben solleva però alcuni interrogativi per i quali non ho ancora trovato risposte, e che sono gli stessi che lasciano perplessi diversi bioeticisti, tra cui Jacob Appel sull'Huffington Post e Arthur Caplan (di cui riferisce il Times). Innanzitutto la maggior parte degli studi effettuati su pazienti con sindromi che impediscono la comunicazione con l'esterno mostra che col tempo c'è un decadimento inevitabile delle funzioni cognitive. Uno dei massimi esperti di brain-computer interface, cioè quegli apparecchi che consentono, dopo un adeguato training, di leggere le onde cerebrali dei pazienti totalmente paralizzati e quindi di scrivere al computer, il tedesco Niels Birbaumer, mi raccontava in un'intervista della necessità di agire in fretta perché pochi mesi (a volte poche settimane) di interruzioni delle comunicazioni con l'esterno deteriorano inesorabilmente le capacità cognitive dei malati. Come è possibile che quest'uomo abbia \"parlato\" solo con se stesso per 23 anni e ne sia uscito cognitivamente intatto rimane un fatto incomprensibile (il che, ovviamente, non esclude che sia possibile). L'altro punto molto critico è il metodo scelto per far parlare Houben, che si vede bene nel video della BBC. Si tratta della cosiddetta comunicazione facilitata, un sistema per il quale un operatore impara a riconoscere minime contrazioni o \"tensioni\" volontarie del paziente e quindi a orientare il dito di conseguenza. Inutile dire che la comunicazione facilitata è considerata poco attendibile e molto manipolabile, come ricorda anche Giuseppe Regalzi nel suo blog Bioetica e come potete vedere da questa revisione sul suo uso nell'autismo. Qualcuno l'ha assimilata persino ai trucchi dei medium. Pare che Laureys abbia affermato (lo dice l'articolo del Times linkato più sopra) di aver fatto delle prove mostrando a Houben degli oggetti in assenza della facilitatrice e ottenendo poi delle risposte corrette sulla denominazione degli stessi. Certamente la velocità con cui il paziente scrive solleva qualche dubbio. Infine rimane il nodo cruciale della diagnosi: di cosa soffre realmente Rom Houben se non è in stato vegetativo? Ora qualcuno cita la sindrome locked-in, ma a vedere alcune caratteristiche dei suoi movimenti nei video, anche questa non sembra essere una diagnosi corretta (anche perché, se così fosse, non si capirebbe per quale meccanismo neurologico stranamente conservato quest'uomo sarebbe in grado di segnalare con contrazioni muscolari le sue volontà alla facilitatrice). Insomma, mi pare che ci siano diverse questioni tutt'altro che secondarie da capire prima di gridare al miracolo ed è ciò che sto cercando di fare da ieri. Purtroppo non riesco a mettermi in contatto con Laureys, che evita le interviste, ma se riesco ad avere informazioni aggiornate ve le trasmetterò. Quel che è certo è che è una storia particolare e che non mi pare il caso di scomodare il testamento biologico e l'eutanasia, come sento fare in queste ore. Comunque vada a finire, un miglioramento nella capacità di diagnosi di queste situazioni va a favore dei sostenitori del testamento biologico, non certo contro."},
{"title": "Generazione 2, identità e violenza", "text": "In questi giorni me ne sto, un po' basita, a sentire di vicini che vietano per legge i minareti, di quote stranieri nelle scuole, di genitori che non vogliono i cinesi in aula \"perché puzzano\", di partiti che propongono croci sulle bandiere come sugli stendardi di Giovanna D'Arco nei libri di Valerio Evangelisti. E penso. Penso all'invito che ho ricevuto, poche settimane fa, per partecipare a un convegno di G2 - generazione seconda - figli di immigrati nati e cresciuti \"per sbaglio\" in questo Paese del quale, però, si sentono ormai cittadini, con o senza diritti riconosciuti. Mi hanno chiamato perché sono anch'io una G2: né i miei genitori né i loro parenti più prossimi sono nati qui, ma vengono dall'Egitto e da altri ameni luoghi del Medio Oriente. Quando sono nata io, mia madre abitava a Milano da tre anni, a casa si parlava il francese, l'arabo (i nonni, non io, purtroppo), il greco, il giudeo-spagnolo e qualche altra lingua più o meno raffazzonata. Altri tempi: tempi in cui la condizione di apolide di mio nonno - sposato con una donna che rivendicava la cittadinanza italiana grazie al passaporto di una lontana ava suddita del Granducato di Toscana - era un problema umanitario da risolvere con italica lentezza, ma con, credo, un discreto spirito di accoglienza.  Forse per tutto questo non mi sono mai sentita G2, se non negli ultimi anni. Un po' diversa dagli altri, forse. Priva di radici sul territorio, sicuramente. Incapace di comprendere la lattaia che mi parlava in milanese e poco avvezza all'ossobuco con risotto, certamente. Ma pur sempre culturalmente una figlia d'Italia, una che a scuola imparava le poesie di Rodari e le parole di Bella Ciao (sì, sì, era nei sussidiari, allora...).Se fossi nata oggi, quale sarebbe la mia posizione? Figlia di immigrati dall'Africa, mica tanto benestanti, incapaci di parlare l'italiano - almeno all'inizio (poi si sono pure laureati) - e persino di religione diversa da quella della maggioranza. Se tutto ciò non mi è pesato più di tanto, se non ha contribuito a forgiare in negativo la percezione che avevo di me stessa è solo perché ho avuto la \"fortuna\" di appartenere all'immigrazione release 1.0, mentre ora siamo alla 2.3 o .4, almeno. Certo, ho anch'io la mia piccola collezione di aneddoti discriminatori: qualche vicino di casa bonariamente razzista (già allora gli odori della cucina altrui non erano apprezzati come il soffritto nostrano), un compagno di classe che si sentiva in dovere di imbrattarmi il diario di svastiche, un professore del blasonato liceo classico Berchet (supplente, per fortuna) che disse \"quelli come te dovrebbero tornarsene a casa loro\". Mi colpì, allora, questa frase. Quale avrebbe dovuto essere casa mia? Io in Egitto ci ho messo piede solo in viaggio di nozze e, peraltro, la mia famiglia da lì era stata cacciata. Oggi, mi dicono i ragazzi G2, questa frase punteggia la loro vita quotidiana, e pazienza se quando se ne tornano in Senegal o in Perù si prendono la stessa infezione intestinale che ci prendiamo noi, il che significa anche che non possono mangiare nel piatto in cui mangiano i cugini. Ditemi voi se c'è un simbolo più pregnante di questo per segnare la differenza... Il problema, per loro come per me, non è tanto quello di definirsi dal punto di vista identitario (ognuno di noi sa bene cos'è, nell'intimo) ma di veder riconosciuta questa identità, che è meticcia per definizione. E il meticciato - grande risorsa, come ci insegna la biologia - è visto con estremo sospetto di questi tempi. L'altra sera ho ripreso in mano il bel libro di Amartya Sen, \"Identità e violenza\", alla ricerca di ispirazione per questo post e per comprendere il clima di questi giorni. Nel prologo lui scrive: \"La concezione dell’identita influenza, sotto molti e diversi aspetti, il nostro pensiero e le nostre azioni. La suddivisione della popolazione mondiale secondo le civiltà o secondo le religioni produce un approccio [...] ’solitarista’ all’identità umana, approccio che considera gli esseri umani membri soltanto di un gruppo ben preciso. Un buon metodo per interpretare in modo sbagliato praticamente qualsiasi abitante del pianeta [....]. L’imposizione di una presunta identità unica spesso è una componente fondamentale di quell’arte marziale che consiste nel fomentare conflitti settari. Il mondo suddiviso secondo un unico criterio di ripartizione è molto più conflittuale dell’universo di categorie plurali e distinte che plasma il mondo in cui viviamo [...]. L’illusione del destino, in particolare quando è legata a determinate identità uniche, alimenta la violenza [....]. Dobbiamo avere piena consapevolezza di possedere molte e distinte affiliazioni\". E poi, nell'introduzione, dice anche una frase che ho molto amato, perché la sento vicina: \"Sarebbe una vittoria a distanza per il nazismo se le atrocità degli anni Trenta avessero precluso per sempre a un ebreo la libertà e la facoltà di invocare qualsiasi identità diversa dall’ebraismo\". La chiave mi pare sia proprio nella sfida che le identità al contempo minoritarie e meticce lanciano alle identità maggioritarie. Nella faccenda del crocifisso, o dei minareti, per esempio, vedo esattamente questo meccanismo, una volta che lo si depura delle manipolazioni demagogiche di alcuni politici: tanti italiani non si sono mai posti il problema del loro rapporto con questi (e altri) simboli, se non nel momento in cui qualcuno li ha messi in discussione. E allora la rivendicazione della loro importanza ha richiesto una scelta attiva, da parte di individui che li ritenevano, diciamo così, parte del paesaggio. E non a tutti piace dover fare scelte consapevoli, perché significa che i simboli, a quel punto, devo essere riempiti di un significato, non possono essere lasciati lì, vuoti, a prendere le ragnatele, altrimenti arriverà qualcun altro, che ai propri simboli (o assenza di simboli, che è altrettanto significativa, in certi contesti come quello religioso) dà un significato pregnante, identitario appunto, e gli sarà facile prendere il sopravvento. Questa è una cosa che le minoranze - di qualsiasi genere, religiose, linguistiche - conoscono bene, perché la conservazione di ciò che sono è un processo attivo, una scelta che passa dalla trasmissione volontaria di valori, abitudini, significati, odori, piatti eccetera; da un continuo compromesso con il mondo circostante. Una fatica che chi è sempre stato maggioranza non conosce neppure. Ecco perché io, che ero G2 in un periodo in cui noi G2 eravamo pochi, non sono mai stata vissuta, se non marginalmente, come un pericolo, anzi: diciamo che piaceva pure, questa mia capacità di leggere Dante mentre friggevo falafel, era sufficiente a decretare la mia completa integrazione. Ma solo perché ero una mosca bianca, non facevo massa. Anche se ogni tanto ho invidiato chi era in grado di mostrarmi dove era nato il suo bisnonno, ho presto capito che l'identità plurale è una ricchezza, che mi fa sentire a casa non solo a Milano, ma anche a Palermo, a Lisbona e a Istambul. Oggi, invece, è proprio questo mescolamento a essere percepito come minaccia. Sia mai che i minareti deturpino il profilo delle Alpi, o che i nostri figli imparino ad apprezzare la cucina cingalese del compagnetto di classe e dimentichino il campanile e la cassoeula. E allora è su questo meccnismo atavico, sulla paura di perdere l'idea di sé (nel senso dell'archetipo platonico, non di ciò che si è veramente), che giocano certi politici beceri e che spiega perché 59 elettori elvetici su 100 abbiano potuto votare una legge non solo intrensicamente razzista, ma anche antistorica. PS: quella bella signora in costume ottomano è la mia bisnonna, intorno agli anni '30, credo."},
{"title": "Buone azioni e tendine da cucina", "text": "Mentre cercavo un argomento per l'ormai tradizionale post natalizio mi è capitato quanto segue. Sabato pomeriggio, verso le 17,30, mentre Milano impazziva per il traffico di chi cercava regali, ho deciso che era ora di cambiare le tende della cucina, operazione che agogno ormai da mesi senza trovare il tempo per andare a cercarle. Mi sono messa in macchina e ho cominciato a percorrere il Naviglio verso il centro, in fila e a passo d'uomo. Ad un certo punto, lungo il muro di una ex frabbrica, vedo steso per terra un barbone, col capo appoggiato a una coperta ma con addosso solo un semplice giaccone. Fuori c'erano meno cinque gradi, certo non una temperatura adatta a stendersi sull'asfalto. Intanto la fila procedeva lenta, il tempo scorreva, le mie tende si facevano sempre più lontane. Accanto all'uomo c'erano due ragazzi giovani. \"Se mi fermo, addio tendine. Ma se non mi fermo (come peraltro tutti gli altri automobilisti in fila) magari questo ci lascia le penne, magari i ragazzi sono troppo giovani o non sono abbastanza svegli da chiamare il 118...\" Ormai mi ero allontanata ma la cosa, diciamocelo, continuava a tormentarmi, per cui ho deciso di fare inversione a U. Ci ho messo 15 minuti a tornare e i ragazzi avevano già chiamato l'ambulanza, perché l'uomo era ubriaco e non si riusciva a svegliarlo. A questo punto mi sono reimmessa nella fila e mi sono fatta un'altra mezz'ora di coda, ma con l'animo più sollevato. Questo banale episodio (e il fatto che nessun altro automobilista si fosse fermato) mi ha fatto tornare alla mente un famoso esperimento di psicologia sociale (conosciuto col nome del \"test del Buon Samaritano\") di cui si discuteva recentemente in un blog americano. Nel 1973, due psicologi della facoltà di teologia dell'Università di Princeton, John Darley e Daniel Bateson, inventarono un test semplice ma geniale. Presero un gruppo di studenti di teologia e annunciarono loro che sarebbero stati inviati a fare un breve discorso davanti a una commissione di valutazione. A metà di loro venne assegnato come tema la discussione della parabola del Buon Samaritano, all'altra metà una presentazione sul proprio curriculum studiorum. Inoltre ogni gruppo fu ulteriormente suddiviso in tre sottogruppi: al primo si disse che aveva accumulato molto ritardo, al secondo un ritardo lieve e al terzo che c'era tutto il tempo. Per raggiungere l'aula della presentazione i soggetti testati dovevano attraversare un parco e, lungo la strada, un attore steso a terra fingeva di essere un passante in difficoltà. Risultato: si fermava il 63 per cento di chi non era in ritardo, il 45 per cento di chi era un poco in ritardo e solo il 10 di coloro che erano molto in ritardo. Il tempo è quindi il fattore determinante, in questo caso, ma l'altruismo dipende anche, seppure in modo meno pregnante, dall'argomento sul quale stavano lavorando: infatti si fermava solo il 29 per cento di coloro che dovevano parlare di se stessi contro il 53 per cento di chi aveva lavorato sul tema del Buon Samaritano.  In sostanza, dice questo grande classico della psicologia sociale, la capacità di mettere in secondo piano i propri bisogni rispetto a quelli di un altro essere umano col quale non vi sono rapporti di affettivi o di conoscenza dipende da fattori oggettivi (il tempo) ma anche soggettivi (nel caso specifico, il \"mood\" nel quale i soggetti reclutati venivano a trovarsi). Ne deriva, dicono, Darley e Batson (che avevano sottoposto i ragazzi reclutati anche a test di personalità e alla misurazione del parametro di \"religiosità\") che il contesto in cui si vive, ciò che si ascolta, gli esempi di comportamento che si hanno attorno nei momenti che precedono di poco una scelta di tipo empatico o altruistico possono essere persino più importanti di elementi come l'educazione o la fede. Poiché è Natale e la filosofia va forte nei blog di psicologia e neuroscienze, vi cito anche un bel post di Jonah Lehrer su Frontal Cortex. Lehrer si chiede se tutto questo parlare di determinismo cerebrale del comportamento non possa avere un effetto nefasto sulle persone, inducendole a pensare di non essere pienamente responsabili delle proprie azioni. E per capirci qualcosa è andato a cercare alcuni importanti esperimenti sulla percezione del libero arbitrio tra individui esposti a informazioni sul funzionamento cerebrale. Il primo è un esperimento recentissimo condotto da Kathleen Vohs e Jonathan Schooler che hanno fatto leggere ad alcuni studenti un brano del premio Nobel Francis Crick: Tu, le tue gioie e i tuoi dolori, le tue memorie e le tue ambizioni, il senso di identità personale e il tuo libero arbitrio siete di fatto niente più che il risultato del comportamento di una gran quantità di cellule nervose e delle molecole a loro associate. Non sei nulla più che un paccheto di neuroni. Un secondo gruppo ha letto invece un estratto della stessa opera che parlava di coscienza senza però citare esplicitamente l'identità e il libero arbitrio. Dopodiché i partecipanti sono stati invitati a rispondere a un test computerizzato, con l'avvertenza che, per un difetto del software, bastava pigiare il tasto invio per ottenere la risposta corretta \"in anteprima\". L'esaminatore non avrebbe avuto modo di scoprire chi aveva barato, quindi il tutto si basava sull'onestà dei partecipanti. Il tasso di bari è risultato significativamente più elevato tra coloro ai quali era stata fornita una giustificazione scientifica del determinismo cerebrale sulle azioni, cioè tra coloro ai quali era stato detto che, di fatto, il libero arbitrio non esiste. Il secondo esperimento è stato condotto da Roy Baumeister all'Università della Florida. A un gruppo di studenti sono state fatte leggere frasi del tipo \"Siamo solo computer biologici disegnati dall'evoluzione e costruiti geneticamente\", mentre ad un altro venivano lette frasi inneggianti alla capacità umana di agire secondo coscienza persino malgrado il determinismo biologico. Dopodiché a entrambi i gruppi sono state presentate situazioni simulate che richiedevano una scelta di tipo empatico e altruistico (per esempio dare denaro ai poveri) oppure di pura cattiveria (per esempio aumentare la quantità di salsa piccante nel panino di un collega che non ama il piccante). Il risultato lo immaginate: pare che il primo gruppo si comportasse in modo decisamente più riprovevole del secondo. Anche in questo caso si può dire che il contesto determina il comportamento, esattamente come nell'esperimento del 1973. Lehrer sostiene che ciò possa dipendere dal fatto che le neuroscienze stanno minando il concetto stesso di libero arbitrio, ma su questo non sono d'accordo. In tutti gli esperimenti citati è l'aspetto emozionale, il senso di colpa, che viene modificato temporanenamente dall'esposizione a messaggi che solo apparentemente forniscono una buona scusa per essere dei pessimi esseri umani. Nel mio caso, per esempio, se non avessi riletto da poco l'esperimento del Buon Samaritano, forse non avrei fatto inversione a U, e non penso di essere più cattiva (né più altruista) della media. Buon Natale!"},
{"title": "Perché chiacchierate con me?", "text": "Lo ammetto, ultimamente trascuro questo blog. Colpa del lavoro ma anche, paradossalmente, del fatto che praticamente ogni giorno c'è un argomento che mi sembra degno di essere segnalato. Me lo appunto o metto la stampata sulla scrivania, ma scrivere un post richiede comunque un discreto lavoro di preparazione e, quando trovo il tempo, la notizia è già stata soppiantata da un'altra più fresca o più curiosa. Mi ha fatto pensare a tutto ciò un curioso studio uscito a fine dicembre sul BMJ secondo il quale i casi neurologici o neuropsichiatrici sono sovrarappresentati nelle riviste mediche rispetto alla loro reale incidenza. Gli autori hanno scoperto, per esempio, che costituiscono oltre un quarto dei casi clinici riportati dalla nota rivista medica Lancet e che in tutte le riviste mediche generaliste la percentuale risulta più o meno la stessa. Il fenomeno non colpisce solo la letteratura medica ma anche la fiction. Gli stessi esperti hanno riguardato tutti gli episodi di Dr House, calcolando che nel 27,5 per cento dei casi i pazienti di House avevano una malattia neurologica (subito dopo si piazzano le malattie infettive con il 16 per cento di presenze). La spiegazione addotta dagli autori mi trova perfettamente d'accordo, come potete immaginare: la neurologia, e le neuroscienze in generale, sono divertenti e intriganti; inoltre, in fondo, tutti noi soffriamo di quello che loro chiamano \"neurofobia\", la paura di essere colpiti nelle nostre facoltà mentali. \"Non sorprende affatto che il pubblico dimostri un tale appetito per storie come 'L'uomo che scambiò sua moglie per un cappello' di Oliver Sacks ma che non via sia molta richiesta per 'l'uomo affetto dalla strana diarrea ematica'\": non c'è che dire, anche le più serie riviste mediche britanniche non rinunciano a un tocco di humor  Mentre ridacchiavo su questa stupida battuta mi è arrivato un serissimo articolo uscito l'11 gennaio su Nature Review Neuroscience intitolato Neurotalk: improving the communication of neuroscience research. Tra le firme, molti importanti neuroscienziati USA ma anche una grande giornalista scientifica come Sandra Blakeslee, autrice di alcuni bei libri di divulgazione sulle ultime scoperte del settore. Il senso dell'articolo, in sostanza, è questo: dato che le neuroscienze studiano questioni che hanno a che fare con la natura stessa dell'uomo e con le ragioni del suo comportamento, non solo interessano il grande pubblico, ma dovrebbero interessarlo sempre più, a causa delle evidenti ricadute etiche e sociali di certe scoperte. Gli scienziati che si occupano di queste questioni devono quindi fare uno sforzo maggiore, rispetto ai colleghi di altre discipline, per comunicare e discutere i risultati delle loro ricerche. E poiché questo non è affatto semplice, ma richiede una formazione ad hoc, è necessario identificare dei mediatori, ovvero degli esperti di comunicazione che siano anche esperti di neuroscienze (lo ammetto, questa è la parte che mi è piaciuta di più. Di questio tempi, fa sempre piacere sapere che al mondo, c'è un posto dove il proprio lavoro ha mercato   ). L'articolo si sofferma poi sulle grandi potenzialità degli strumenti interattivi, come Twitter, Facebook e dei blog come questo. Sono un modo per abbattere le barriere e partecipare attivamente alla discussione anche quando non si è esperti nella materia. E non è un caso che i blog di neuroscienze siano numerosissimi (soprattutto in lingua inglese), anche se nella maggior parte dei casi sono tenuti da scienziati e solo in una piccola minoranza da giornalisti appassionati quale sono io. Il pubblico, invece, è molto più variegato e frequenta senza troppi complessi ambedue le tipologie di luoghi virtuali. Alla fine ho pensato che non ve l'ho mai chiesto: ma voi, perché venite a chiacchierare qui? Cosa vi interessa di più in questo ambito che è davvero immenso? Abbiamo parlato insieme di psicologia sociale e di reti neurali, di psicanalisi e di sinaptopatie... Raccontantemi un po' perché avete scelto questo salotto, così magari mi date anche qualche suggerimento nella scelta degli argomenti (anche se ci sono delle passioni personali alle quali non intendo rinunciare!)."},
{"title": "Incapaci di volere, capaci di far male", "text": "La vicenda di Luca Bianchini, presunto stupratore seriale che circa una quindicina di anni fa è stato giudicato non imputabile per un caso di tentata violenza in quanto incapace di intendere e di volere, mi ha fatto tornare in mente alcuni discorsi che ho ascoltato la settimana scorsa a Pavia, a un incontro tra giuristi e neuroscienziati sull'interazione tra le due discipline. La giornata era organizzata dal giudice Amedeo Santosuosso, direttore del Centro di ricerca interdisciplinare che fa capo alla Facoltà di giurisprudenza pavese, molto attivo nei campi della bioetica e della neuroetica.Ovviamente il tema dell'imputabilità è stato uno di quelli più dibattuti, anche perché giudici ed esperti di neuroscienze vedono spesso la faccenda da due angolazioni differenti. In particolare, ha affermato senza peli sulla lingua Luisella De Cataldo, avvocato e psicologa, presidente dell'Associazione di psicologia giuridica, quando le perizie si basano solo su interpretazioni psicodinamiche, vige una certa arbitrarietà nei giudizi. E comunque, ha detto rivolta agli esperti di mente e cervello presenti in sala, \"a sentir voi nessuno sarebbe imputabile, perché c'è sempre un evento traumatico che ne ha determinato le azioni\".  Per essere imputabile (e quindi, eventualmente, condannabile) un individuo deve essere in grado di intendere la gravità del gesto ha compiuto e di volere, cioè di porre un freno razionale all'impulso a compiere il male.\"Ancora prima che le neuroscienze svelassero l'esistenza di patologie in cui la capacità di intendere è mantenuta ma si perdono i freni inibitori, come per esempio alcune malattie psichiatriche e neurologiche che coinvolgono i lobi frontali, i legislatori avevano intuito la difficoltà di giudicare i cosiddetti delitti d'impulso, per i quali erano previste alcune attenuanti\" ha detto ancora De Cataldo. Oltre alla capacità di intendere e volere, per processare una persona bisogna anche che essa sia in grado di partecipare al processo stesso: se dal momento del delitto a quello del giudizio le condizioni mentali del soggetto si deteriorano (per esempio per via di una malattia degenerativa come l'Alzheimer), il giudice è obbligato a dichiararne la non imputabilità.Nel caso dello stupratore romano, se si rivelerà essere davvero colpevole come sembra dai primi riscontri, sono evidenti manifestazioni patologiche tali da far presupporre almeno l'incapacità di volere (per esempio il fatto che, a detta delle vittime, provava un senso di vergogna dopo la violenza). Ciò non significa che un individuo non possa essere pericoloso socialmente o persino per se stesso, o che non possa scivolare lentamente verso delitti più efferati, fino all'omicidio. Significa solo che, a rigore di legge, non è condannabile (ma ovviamente può e deve essere obbligato a curarsi ed essere sorvegliato con attenzione). L'Italia, tra l'altro, mantiene ancora aperti gli ospedali psichiatrici giudiziari, benché molti psichiatri ne chiedano la chiusura, ritenendoli non adatti al recupero dei pazienti. Se può accadere che anche uno stupratore seriale o un killer non venga condannato, è perché la legge, teoricamente, non ha una funzione punitiva ma riabilitativa (e comunque punire qualcuno che non è in grado di capire o di frenare i propri impulsi è ovviamente inutile dal punto di vista della deterrenza).Un perito del tribunale, in casi di manifesta patologia psichiatrica o neurologica, non può fare altro che dire la verità, cioè che l'imputato non è imputabile. È ciò che ha fatto il perito che anni fa scrisse il rapporto su Bianchini e, fino a prova contraria, dobbiamo credere che avesse elementi diagnostici a sostegno della propria perizia. Più difficile, invece, è pronunciarsi sulla probabilità che un individuo ripeta l'atto criminoso, poiché dipende ovviamente dalla diagnosi. Se una persona è affetta da una demenza frontale e spende senza ritegno, emettendo assegni a vuoto, continuerà a farlo, almeno fino a quando la malattia non comprometterà la sua autonomia tanto da rendere impossibile qualsiasi attività. Giudici e periti, hanno detto gli esperti riuniti a Pavia, devono per forza collaborare più strettamente, per comprendere meglio non solo il linguaggio dell'altro, ma anche i suoi strumenti di giudizio. Solo così chi commina le pene potrà davvero giovarsi del contributo di chi studia il cervello, anche quello degli uomini che compiono orrendi delitti. E la società non può restare a guardare, limitandosi protestare quando, in base a una legge che tutela i malati, uno di essi sfugge anche dalle maglie di chi lo dovrebbe curare per impedirgli di fare del male a degli innocenti. È evidente a tutti che quando questo accade, come nel caso in questione, c'è qualcosa che non va nel sistema. È altrettanto evidente, però, che gridare al \"chiudetelo in prigione e buttate la chiave\" non è etico (anche se a qualcuno può sembrare la soluzione giusta), né  lo è, a mio avviso, invocare pratiche come la castrazione chimica (applicata in alcuni Paesi come la Germania con una scarsa efficacia e molti dubbi). Scoprire che la mente umana può, se alterata, concepire abissi di perversione e cattiveria e stupirsene è da ingenui. Meglio chiedersi cosa deve farne, una società, di questi individui che sono, per l'appunto, incapaci di intendere e volere."},
{"title": "Naturalmente pentatonici", "text": "Aspettavo da qualche giorno che i responsabili del World Science Festival autorizzassero l'embedding di questo video nel quale il jazzista Bobby McFerrin dimostra quanto istintivo sia generare la scala pentatonica. Tra l'altro questo piccolo esperimento - che a quanto pare lui ha replicato in diversi paesi ottenendo sempre lo stesso risultato - va assolutamente nella direzione degli studi più recenti sulla percezione dell'altezza dei suoni: dipende da fattori cuturali, ma è modellato dalle caratteristiche del sistema uditivo e della corteccia uditiva (se il tema vi interessa, qui trovate il link all'abstract di una bella revisione sull'argomento). E se non vi interssa, avete comunque l'occasione per sentire uno dei miei miti musicali Buon ascolto e buone vacanze!  Bob McFerrin e la scala pentatonica"},
{"title": "30 milioni di dollari per il connettoma", "text": "Causa vacanze non abbiamo avuto modo di parlarne: il mese scorso i National Institutes of Health statunitensi hanno ufficialmente lanciato il Progetto Connettoma Umano, finanziandolo con 30 milioni di dollari. Obiettivo: tracciare, entro il 2015, una mappa delle connessioni tra le aree cerebrali nel cervello umano sano, per capire come vengono processate le informazioni. Tra gli altri obiettivi dichiarati anche quelli di mettere a punto nuovi farmaci per le malattie neurodegenerative e per il dolore: risultati pratici che suonano più come giustificativi per far digerire al contribuente americano l'entità dell'investimento su un progetto che ha invece tutte le caratteristiche della ricerca di base.  Ormai gli Stati Uniti sono convinti sostenitori della \"big science\", che richiede di unire gli sforzi su un obiettivo preciso da ottenere entro un termine prestabilito. E per far ciò, ovviamente, si mettono i soldi sui grandi progetti, magari a scapito di altri più modesti. Col genoma ha funzionato, con le staminali un po' meno: vedremo cosa accadrà col cervello. Nel caso specifico, la \"colpa\" è di Olaf Sporn, un neuroscienziato dell'università dell'Indiana che, nel 2005, ha scritto un articolo per reclamare a gran voce una \"mappa di navigazione\" di quel \"territorio sconosciuto che è il nostro cervello\". Ora il progetto c'è, anche perché solo da qualche anno ci sono i mezzi tecnici per attuarlo: l'imaging in vivo consente di vedere il percorso di attivazione delle aree cerebrale sulle tre dimensioni e quindi di tracciarne la mappa. Non tutti, però, sono convinti che il connettoma, così come definito dal progetto degli NIH, costituisca un vero passo avanti nella comprensione del funzionamento cerebrale, poiché traccerà solo le \"autostrade\" della trasmissione nervosa, mentre molti scenziati sono ormai convinti che è a livello del singolo neurone che si svolge il vero lavoro computazionale del cervello. Anche l'unico connettoma disponibile, quello del verme Caenorhabditis elegans, dotato di 320 neuroni, sembra confermarlo. E quindi noi umani, che di neuroni ne abbiamo circa 100 miliardi, per capire qualcosa del nostro cervello dovremmo aspettare ancora un po', senza contare che la quantità di dati che si dovranno processare per arrivare a una risoluzione talmente piccola sarà tale da far sembrare il Progetto Genoma semplice come uno di quei grandi puzzle di legno che si usano alla scuola materna. I più ottimisti sperano di arrivare a capire come funzioniamo partendo dai due estremi della questione: dall'aspetto macroscopico della connessione tra aree cerebrali, grazie al progetto dell'NIH; e dal lavoro certosino di unione e comparazione delle singole vie neuronali effettuato da chi lavora a livello microscopico, per esempio con la registrazione dell'attività di singoli neuroni mediante elettrodi intracranici. Sporn e i sostenitori del Progetto Connettoma Umano, invece, replicano che la mappa neurone per neurone non sarà necessaria: e hanno dalla loro lo studio dei fenomeni complessi che utilizza strumenti matematici avanzati per descrivere processi che coinvolgono un numero enorme di variabili, molte delle quali sconosciute. \"Per descrivere un processo economico non andiamo certo a verificare quello che ogni singolo cittadino mette nel carrello della spesa\" ha detto Sporn. Quel che è certo, è che i grandi progetti di questo tipo sono un fertlizzante naturale per lo sviluppo del ramo che coinvolgono e, inoltre, garantiscono un certo risparmio di forze, evitando, per esempio, le duplicazioni. Grazie alle banche dati del Progetto Genoma molti laboratori hanno scoperto che stavano sequenziando lo stesso gene e hanno unito i loro risultati, dando un'accelerata al processo. Comunque sia, i bandi di finanziamento per questa nuova impresa sono disponibili sul sito degli NIH e val la pena osservare con attenzione cosa ne verrà fuori."},
{"title": "I pregiudizi della carne", "text": "Oggi rubo il lavoro al blog Scienza in cucina, ma ho visto di recente due notizie curiose che riguardano la produzione di carne per consumo alimentare su cui mi piacerebbe discutere.La prima, che non è nuovissima, è uscita sull'edizione britannica di Wired e riguarda la possibilità di \"coltivare\" bistecche in laboratorio. Un ricercatore dell'Università di Eindhoven, Max Post, studia la replicazione in vitro delle cellule muscolari. Il suo obiettivo principale era in origine quello di creare vasi sanguigni artificiali, ma ha presto capito le potenzialità della \"bistecca artificiale\". Al momento deve fronteggiare alcuni problemi non indifferenti, il primo dei quali è la riproduzione della consistenza di un muscolo \"vero\". Per far ciò sta allenando le sue bistecchine in vitro con scosse elettriche: niente di molto diverso dalle apparecchiature per la ginnastica passiva! Inoltre sta cercando di indurre alcune popolazioni batteriche a produrre nella capsula di Petri quel cocktail di ormoni che contribuisce al sapore della carne animale. Ovviamente, queste bistecche mancano di grasso, ma anche questo non è un problema: basta aggiungere qualche adipocita, o forse \"pompare\" grassi (tra l'altro selezionati tra quelli più salubri) nella bistecca finita.  Quel che mi ha colpito è la dichiarazione di Post: \"Non credo che perderemo molto tempo a ricostruire il sapore della carne, perché quello si può aggiungere artificialmente alla fine. Quel che conta è proprio la consistenza\".Questo è molto indicativo di come funziona il senso del gusto, ma solleva anche altri interrogativi. L'uomo è istintivamente portato a diffidare dei cibi che non hanno aspetto e consistenza simile a quelli naturali: è un meccanismo evolutivo di difesa, che ha permesso ai nostri antenati di evitare gli avvelenamenti. Un esempio spesso riportato nei manuali di biologia è quello dei cibi blu, verso i quali gli animali hanno una naturale diffidenza poiché non è un colore che la natura produce frequentemente. Ma questo meccanismo istintivo è stato in parte annullato dalla cultura: alzi la mano chi non ha un figlio appassionato di gelato al Puffo! Inoltre, chissà se l'idea di cibarsi di carne coltivata in vitro aumenterà il numero degli adepti della fettina (per esempio convincendo i vegetariani) oppure lo ridurrà. La seconda notizia è apparsa su Neuroethics: Adam Shriver, neuroscienziato e bioeticista, propone di creare animali da allevamento privi di un gene che rende sensibili al dolore. Perché, dice Shriver, comunque la si metta, gli allevamenti procurano sofferenze agli animali e noi uomini, che abbiamo bisogno delle loro carni e dei loro prodotti, possiamo rendere la loro vita meno faticosa. Dal punto di vista tecnico non è neanche complicato: si tratta di creare embrioni knock-out per geni che conosciamo benissimo e la cui assenza è all'origine di alcune rare malattie nell'uomo. Pur condividendo l'idea di Shriver che il dolore degli animali va preso in considerazione, la sua proposta non fa altro che sostituire un problema etico con un altro: che diritto abbiamo di creare un numero estremamente grande di animali geneticamente modificati? E la sofferenza, negli animali, è davvero solo una questione fisica? E infine: se gli OGM vegetali creano tutte queste resistenze culturali, che accadrà con gli animali OGM? Forse la risposta è quella che dà Post, lo scienziato della bistecca artificiale: \"Chi ha da mangiare si porrà tutti i problemi psicologici ed etici, ma chi attualmente soffre la fame sarà solo contento di poter avere a disposizione una fonte di proteine\"."},
{"title": "Scolpito nel silicio", "text": "Ieri su Repubblica, a firma di Jaime D'Alessandro, ho letto un bell'articolo (purtroppo non disponibile on line), che racconta la storia di Gordon Bell, ricercatore della Micrososft che ha pubblicato un libro \"Total recall, how the e-memory revolution will change everything\" sul progetto MyLife-Bits. Bell ha deciso, nell'ambito di un esperimento partito nel 1998, di registrare su supporto digitale tutto ciò che gli succede: telefonate, spostamenti, immagini. Se all'inizio ha dovuto selezionare i ricordi meritevoli di archiviazione, oggi, con il progresso dei programmi di database, la maggior parte delle sue azioni viene registrata in automatico. Oramai, però, non serve essere un programmatore della Microsoft perché ciò accada: è quanto avviene a molti di noi. Io, che ho sempre avuto la mania del ricordo, ho conservato i diari dell'infanzia, poi le agende, i foglietti, le lettere degli amici. Molte cose sono ancora da qualche parte negli scatoloni su in solaio, ma molte altre sono andate perse negli innumerevoli traslochi, oppure sono stata io stessa a eliminarle, perché semplicemente non volevo più \"ricordarle\". Ma da quando utilizzo agende elettroniche, smart phones, e-mail, social networks, iPhoto, iTunes eccetera mi sorprendo sempre più spesso della precisione con cui riesco a ricostruire gli avvenimenti della mia vita. Se al contenuto di questi programmi e supporti sommiamo tutto ciò che scrivo, per diletto e per lavoro, non è poi difficile, per me (o forse anche per chi mi conosce bene), risalire addirittura alle emozioni provate in determinati momenti.  L'articolo di D'Alessandro si concentra sugli aspetti tecnici del fenomeno: la maggior capienza degli hard disk, l'aumentata compatibilità tra un programma e l'altro, necessaria al trasferimento dei dati man mano che si cambiano i computer.C'è un fatto, però: quella delle mie figlie sarà probabilmente la prima generazione che avrà a disposizione una memoria puntuale e oggettiva della propria vita, una memoria che sarà situata al di fuori del loro cervello pur essendo interrogabile, incrociabile, confrontabile. Lo psichiatra Gustavo Pietropolli Charmet, intervistato in chiusura dell'articolo, afferma :\"Essere i propri pensieri e la propria storia è un'ipotesi affascinante\". Già, perché noi siamo convinti di \"essere la nostra storia\", ma le neuroscienze ci dicono che non è così, che i ricordi vengono rimaneggiati ogni volta che li rievochiamo, che costruiamo nuove sinapsi e dotiamo il passato di nuovi significati alla luce di ciò che è il nostro presente. È sempre stato così, nella storia dell'uomo, perché anche i testi scritti (unico possibile supporto insieme alla fotografia fino a non molto tempo fa) lasciano spazio all'immaginazione, evocano solo parzialmente l'obiettività di un accadimento. Il digitale, però, ci mostra tutte le versioni del nostro ricordo in contemporanea: la foto insieme all'annotazione sull'agenda (con il luogo e la lista dei presenti) insieme alla schermata del lettore mp3 che segnala il brano che abbiamo ascoltato e scaricato mentre quella stessa foto veniva scattata. È lo stesso Pietrolli Charmet ha usare il termine \"protesi tecnologiche\" per definire tutte queste \"appendici\" della nostra memoria. Si discute tanto della \"mente espansa\" come di un'ipotesi futura, da realizzarsi quando il supporto biologico (il neurone) imparerà a dialogare col supporto digitale (il silicio), ma la mente espansa è già qui: non ho bisogno di un jack da inserire nel cranio, come in Nirvana o Johnny Mnemonic, per scaricare i miei ricordi nel computer: lo faccio quotidianamente, e con il wireless. Avere a disposizione una memoria esatta del nostro vissuto è un bene? Niente più rimozioni, dice lo psicoanalista, e quindi, teoricamente, niente conflitti. Ma anche l'impossibilità, per l'incoscio (se vogliamo restare nell'ambito dell'interpretazione psicodinamica) di agire sul nostro vissuto: niente mediazioni, filtri. Il ricordo è qui, è presente e passato nello stesso tempo. Allettante e inquietante."},
{"title": "Addio a Claude Lévi-Strauss", "text": "I coccodrilli mi vengono malissimo: o almeno credo, perché mi sa che questo è il mio primo. Stavo scrivendo tutt'altro post quando mi è arrivata l'ANSA che annuciava la morte di Claude Lévi-Strauss. Penso che non mi vengano bene i coccodrilli perché il loro scopo è quello di riassumere in poche migliaia di battute quello che un uomo ha fatto di importante nella vita: un compito impossibile, tanto più se quell'uomo, come Lévi-Strauss, ha attraversato l'intero Novecento non solo fisicamente ma anche intellettualmente.  I giornali di oggi titolano: è morto il padre dell'antropologia. In realtà l'antropologia esisteva anche prima di lui, ma era una disciplina che osservava le strutture sociali umane con l'occhio ristretto e la morale del colonialismo. Dobbiamo principalmente a Lévi-Strauss la morte definitiva del \"buon selvaggio\" e del \"mito del cannibale\", la fine dello sguardo condiscendente dell'Occidente sulle culture altre. E non mi pare poco, come eredità. Lévi-Strauss ha provato ad applicare allo studio antropologico i principi scientifici. Nei suoi scritti appare forte e chiaro il messaggio che tutti gli agglomerati sociali umani sono organizzati in base a strutture universali. I suoi studi sui legami di parentela, sui totem e i miti, sul tabù dell'incesto, danno forma universale al nostro modo di organizzarci per convivere secondo regole il più largamente condivise. L'idea che si potesse applicare all'antropologia il metodo strutturalista utilizzato nell'analisi del linguaggio è la sua grande intuizione, ma d'altronde egli è figlio del proprio ambiente, quella Francia a cavallo della seconda guerra mondiale che ha fatto propria la lezione del linguista Ferdinand De Saussure e ci ha dato il lucido sguardo storico di Michel Foucault, l'incredibile capacità di scomposizione dell'opera letteraria di Roland Barthes, il lavoro di Jacques Lacan... In tutti questi pensatori, indipendentemente dall'oggetto della loro analisi, c'è un tentativo di svelare la \"grammatica universale\" (per prendere a prestito un termine di Noam Chomsky, altro grande figlio dello strutturalismo) dell'uomo come essere sociale. Secondo me non è un caso che tutti siano stati influenzati sia da Freud sia dal marxismo, perché le strutture rigide invitano alla rottura o almeno al superamento delle stesse. Nel caso di Lévi-Strauss, però, c'era la convinzione che l'evoluzione sociale non dipende dalla volontà dell'uomo ma dalla logica insita nel sistema stesso, che è dotato si proprietà oggettive. Un concetto che mutua dalla matematica ma non solo. L'evoluzione sociale è per lui un processo \"inconscio\", altro termine che prende, questa volta, dalla psicoanalisi. Le Monde ha dedicato alla sua morte un bell'articolo. Ne traduco l'inizio per chi non legge il francese. Non volevo scrivere un coccodrillo ma ne ho scritto uno e mi spiace, perché in realtà io amo il suo lavoro principalmente perché ho molto amato i suoi scritti. Scordate tutto e riprendete (o prendete per la prima volta) \"Tristi tropici\". E viaggiate con lui tra le tribù dell'Amazzonia per scoprire che ogni società, in fondo, si somiglia. L'incipit (in francese) di Tristi tropici  Una lunga (e bellissima) intervista fatta nel 1972 per il canale Arte (in francese con sottotitoli in inglese) "},
{"title": "Aiuti immateriali per i terremotati", "text": "Scendo dalla macchina e intercetto una conversazione tra due signore in attesa sul marciapiede: \"Ora gli mandano gli psicologi, però non hanno manco le tende\". Ovviamente stanno parlando del terremoto in Abruzzo e, altrettanto ovviamente, d'istinto dò loro ragione. Ma poi mi fermo a pensare: saranno davvero tanto inutili, gli psicologi, in questo momento? E quelli che ci vanno, per lo più volontari, immagino, sono davvero in grado di far fronte a una situazione tanto grave? E ancora: c'è un modo corretto di portare avanti questo tipo di aiuto professionale? In effetti dopo eventi come questi pare a tutti normale che la popolazione soffra di disturbi psicologici come ansia e depressione, specie nel caso di coloro che hanno perso una persona cara o la casa. Ma quanti sono quelli che non si riprendono più? E come è possibile aiutarli? Sono andata a dare un'occhiata alla letteratura disponibile sulla sindrome post-traumatica da stress (PTSD): le caratteristiche del disturbo sono simili, indipendentemente dalla causa (violenza, guerra o, come in questo caso, calamità naturali), però c'è chi ha studiato nello specifico cosa accade nei terremotati, quando oltre al proprio mondo interiore anche in mondo reale viene distrutto. Gli adulti sviluppano forme intense di ansia e depressione, i bambini perdono il sonno, gli adolescenti tendono a rifugiarsi nella droga o adottano comportamenti antisociali. Nel 2005 la rivista Epidemiologic Reviews ha pubblicato una revisione secondo la quale la prevalenza di PTSD, nell'anno successivo a un grave evento, varia dal 5 al 60 per cento a seconda delle valutazioni effettuate (ma in media si attesta intorno al 30 per cento). Visitare e valutare in acuto le popolazioni colpite è ovviamente molto complesso, perché le priorità sono davvero altre. Ma sapere che a un anno di distanza tre persone su dieci hanno ancora un consistente disturbo psichico non è cosa da poco, perché si tratta di malattie invalidanti, che impediscono di tornare alla vita normale. Altre ricerche hanno tentato di capire che cosa determini la cronicizzazione del PTSD e i parametri più importanti sembrano essere la durata temporale del trauma (nel caso specifico, il numero delle scosse e il loro ripetersi), la perdita di persone care, la vicinanza all'epicentro dell'evento, il senso di controllo sulla propria vita (la possibilità di ottenere rapidamente informazioni attendibili, di aiutare i propri cari, di raggiungere i punti di soccorso) e, ovviamente, l'efficacia dell'aiuto e del supporto ottenuto dopo la fase acuta. Ma ci sono anche fattori individuali: uno studio recentissimo , condotto dall UCLA sui sopravvissuti al terribile terremoto che nel 1988 causò quasi 20.000 morti in Armenia, con epicentro nella città di Gumri, ha individuato una predisposizione genetica alla cronicizzazione di ansia e depressione nel PTSD, analizzando i nuclei familiari esposti al disastro. Ciò significa che gli interventi di sostegno andrebbero mirati, specie nella fase postacuta. La Turchia, che condivide con l'Italia la triste sorte di essere un territorio ad alto rischio sismico, fornisce altri dati importanti per programmare gli interventi: a sei anni dal terremoto che colpi la città di Marmara, un questionario di qualità della vita somministrato a 200 studenti universitari sopravvissuti al disastro e a 200 provenienti da altre aree del Paese dimostra che i risultati scolastici dei primi sono nettamente inferiori rispetto ai secondi. Le cause di questa differenza, però, sono molto spesso materiali: difficoltà economiche, percorso scolastico interrotto per via dell'assenza di strutture agibili eccetera. Ansia e depressione si sommano a questa realtà, rendendo ancora più complesso l'adattamento. Cosa fare, quindi? Una risposta la dà un articolo uscito già nel 2000 sull'American Journal of Psychiatry: intervenire rapidamente per migliorare le condizioni di vita dei sopravvissuti (perché povertà, disoccupazione e lunghe permanenze fuori casa in abitazioni di fortuna sono importanti fattori di rischio) ma provvedere anche a una valutazione accurata dei sintomi nella fase acuta, specie per quanto riguarda bambini e adolescenti. I farmaci ansiolitici, per esempio, il cui consumo aumenta per molti anni nelle aree colpite da eventi come terremoti, uragani e violenze, possono aiutare ma alungo andare diventano addirittura un ostacolo alla corretta presa in carico della malattia. Gli psicologi, quindi, potrebbero davvero essere utili, purché non si limitino a una generica opera di sostegno ma siano dotati di strumenti efficaci. E qui sta il punto dolente: la psicologia delle emergenze non è una disciplina che si improvvisa, ma che dovrebbe essere inserita nel quadro più generale degli interventi in aree disastrate. Ho cercato di capire in che modo, in questo momento, si tenta di dare anche questo tipo di assistenza agli abruzzesi, ma non sono riuscita a trovare un vero e proprio coordinamento, se si eccettua l'attività di reclutamento dei volontari effettuata dall'Ordine degli psicologi della Regione. D'altronde anche in passato l'esperienza è stata di qualità variabile, come dimostra questo articolo che ho trovato sul sito della Società italiana di psicologia dell'emergenza. Prendere in carico il benessere psicologico di migliaia di persone che hanno perso tutto non è come far fronte a un disastro aereo con 200 vittime: le ferite da sanare sono ben più diffuse e consistenti. Non vuol dire, però, che non ci si possa provare."},
{"title": "La disinformazione sulle staminali viaggia (anche) su Facebook", "text": "Stamattina mio marito, che come me è un giornalista scientifico, ha aperto il suo account su Facebook e ha scoperto che uno dei suoi \"amici\" (una persona che lo ha contattato dicendo di essere un collega) ha postato un link a un gruppo che sostiene l'efficacia delle terapie con cellule staminali di derivazione cordonale effettuate in Cina e in altri paesi asiatici per curare diverse malattie neurodegenerative (dall'Alzheimer alle lesioni traumatiche, fino alla degenerazione maculare retinica). Nel commentare il link, ha ricordato che non vi sono prove di efficacia di tali cure, anzi: gli studi seri effettuati con cellule staminali sia cordonali sia embrionali hanno avuto esiti negativi e, in alcuni casi, come nella terapia del morbo di Parkinson, addirittura nefasti, poiché utilizzando staminali (specie se embrionali) si aumenta il rischio di sviluppare tumori nella sede di inoculo. Sono in corso diversi studi promettenti con l'obiettivo di bypassare questo effetto, uno dei quali uscito anche recentemente, ma al momento si tratta solo di promesse. L' \"amico di Facebook\" si scopre essere il rappresentante europeo di una società che effettua terapie con staminali cordonali in Cina e in altri paesi asiatici, su pazienti stranieri e ovviamente a pagamento. L'inefficacia di queste cure è stata ampiamente discussa in ambito scientifico ed è stata anche oggetto di molte trasmissioni televisive, tra cui le Iene e Mi Manda Rai3 (nello specifico, il 10 aprile scorso). Anche riviste importanti come Nature si sono ovviamente più volte interessate alla questione (trovate qui il link a uno degli ultimi interventi), dato che in Cina le diverse strutture che offrono queste terapie stanno accumulando una casistica non indifferente, ma nulla è mai stato pubblicato su riviste peer reviewed sugli esiti a breve e a lungo termine. Purtroppo molti giornalisti, anche in buona fede, cadono nella trappola dell'aneddotica. Vengono invitati in Cina o in altre sedi esotiche, visitano gli ospedali, incontrano pazienti disperati che affermano di sentirsi meglio e tornano convinti di aver \"visto\" l'efficacia della cura. Anche chi è dubbioso a volte utilizza toni a mio avviso non sufficientemente critici nei confronti di un fenomeno potenzialmente pericoloso: i controlli sulle cellule utilizzate rispettano sulla carta gli standard USA, ma si tratta di Paesi dove l'interesse a fare una vera supervisione è ovviamente molto basso, dal momento che, come racconta anche questa testimonianza tratta da un blog specializzato, le tariffe non sono propriamente popolari. E se anche l'intervento non fosse pericoloso, è comunque truffaldino, perché promette risultati che non possono essere garantiti da alcuno studio serio. Un esempio di informazione confondente è, a mio avviso, questo articolo uscito un paio di mesi fa sul Corriere della Sera. Il titolo riporta il termine \"curare\" tra virgolette, ma il testo lascia intatto il dubbio che forse, chissà, potrebbe esserci qualcosa di vero, che la scienza ufficiale (sempre conservatrice nelle visioni più complottarde) ancora non riconosce. L'opinione di chi è \"contro\" è messa sullo stesso piano di quella di chi è \"a favore\", come se non esistesse una base obiettiva di prove tale da rendere i due approcci non equivalenti. Insomma, è la versione \"staminali\" della vicenda \"radon\" nella previsione dei terremoti... Eppure dovrebbe bastare il lungo elenco di patologie che, secondo le aziende che forniscono queste terapie, sarebbero curabili (tutte causate dalla degenerazione di una parte del sistema nervoso, ma per cause diversissime tra loro) a suscitare almeno qualche dubbio. Ricordo che due anni fa ho partecipato a un convegno promosso dal Collegio Ghislieri di Pavia sul tema delle applicazioni cliniche delle cellule staminali. Dopo molte relazioni di esperti dedicate alle difficoltà ottenute dalla medicina rigenerativa con staminali, il mio compito era quello di dimostrare, titoli di giornale alla mano, come i media avessero invece calcato troppo la mano sulle sorti magnifiche e progressive. Non avevo finito di parlare che una collega si alzava per dire che lei in Cina ci era appena stata, e aveva visto con i propri occhi i \"miracoli\" delle cure con staminali e che, per quel che la riguardava, credeva più ai propri occhi che a tutti quella teoria che le avevamo propinato fino a quel momento. La morale: sulle disgrazie altrui fioriscono mercati particolarmente aggressivi e pervasivi, perché è difficile togliere speranze a chi non ne ha proprio. I media sono spesso vittime, come il lettore comune, di un'illusione percettiva, per cui vale più la testimonianza del singolo delle prove faticosamente accumulate con gli anni. Ahimé, mi sento solo di concludere questa vicenda (che su FB sta andando avanti con diversi interventi) con le parole dell'editoriale di Nature che citavo più sopra: \"Questo approccio da mercato libero applicato alla medicina sperimentale, nel quale medici e imprenditori sfruttano sia la fiducia del paziente che l'assenza, in molti paesi, di regole per le terapie con cellule staminali sembra destinato a finire male\"."},
{"title": "Dilemmi educativi", "text": "Ho sempre pensato che, quando si insegna qualcosa, è giusto parlare di tutti i punti di vista sulla questione. Questo studio condotto da un docente dell'Università del Minnesota forse mi dà torto. Il professor Randy Moore ha intervistato 1.000 studenti universitari iscritti al corso introduttivo di biologia per capire se l'accettazione della teoria dell'evoluzione può dipendere dal tipo di approccio a cui i ragazzi sono esposti durante gli anni del liceo. Ha scoperto che basta parlare del creazionismo per indurre il dubbio circa la scientificità dell'evoluzionismo. Gli stessi autori della ricerca sono rimasti stupefatti dall'impatto che l'introduzione del creazionismo nelle scuole (sia pure come \"visione alternativa\" a quella evoluzionistica) ha sulla percezione di scientificità della teoria dell'evoluzione. Morale: questo è quanto si ottiene quando si mescolano capre e cavoli. Chi sostiene che in nome di una non meglio precisata \"democraticità\" bisogna dare spazio a tutte le visioni del mondo (comprese quelle antiscientifiche) non fa i conti con la gran confusione che ne ricavano i più giovani. Come giustamente fanno notare gli autori dello studio, il problema è il contesto. Il creazionismo è una teoria religiosa o filosofica, se si preferisce, e come tale va insegnata. A lezione di biologia, invece, si deve insegnare ciò che è dimostrato. E se pensate he si tratti di problemi che riguardano solo quegli invasati degli americani, date un'occhiata al grafico che vi riporto più sotto. È tratto da un lavoro apparso su Science nel 2006, che valutava l'adesione generale alla teoria dell'evoluzione in diversi Paesi, tra cui l'Italia. Non siamo mica messi così bene... Miller, J.D., Scott, E.C., &amp; Okamoto, S. (2006). Science communication. Public acceptance of evolution. Science, 313, 765-766 "},
{"title": "Specchio, specchio delle mie brame...", "text": "La polemica divampa ormai furiosa da quasi venti giorni su riviste specializzate, sul web e persino sui giornali: i neuroni specchio - scoperta che ormai connota la città di Parma nel mondo quasi quanto il parmigiano e il prosciutto crudo - esistono o non esistono nell'uomo?  A gettare scompiglio, come i più attenti già sapranno, è un articolo che il neuroscienziato italo-americano Alfonso Caramazza, ora direttore, oltre che del Laboratorio di scienze cognitive di Harvard, anche del Centro mente e cervello di Rovereto, ha pubblicato sulla rivista PNAS. Un po' di storia Alla fine degli anni '80 il gruppo di neurofisiologi dell'Università di Parma guidato da Giacomo Rizzolatti identifica casualmente nella scimmia una popolazione di neuroni che si attiva sia quando l'animale compie un atto motorio finalizzato sia quando lo osserva in un'altra scimmia e persino nell'uomo. Attraverso una serie di studi successivi, viene elaborata la teoria dei neuroni specchio, secondo la quale esiste nel nostro cervello una popolazione neuronale in grado di attivarsi in presenza di atti motori indipendentemente dalla modalità con cui questi sono compiuti (quindi anche attraverso l'osservazione e persino attraverso l'ascolto di un suono, come avviene, per esempio, quando si sente il rumore di un guscio di noce che si rompe e si immagina l'azione sottostante). Per farla breve, i neuroni specchio sarebbero alla base della capacità di alcune specie superiori di capire le intenzioni altrui. In pratica sarebbe il substrato fisiologico dell'empatia e, di concerto, anche della cosiddetta teoria della mente, la capacità di pensare all'altro come essere pensante. In sostanza, la radice della coesione sociale. Alcuni neuroscienziati del gruppo di Parma si sono spinti addirittura a interpretare, alla luce dei neuroni specchio, fenomeni come la produzione artistica (Vittorio Gallese) o, per l'appunto, la coesione sociale e la politica (Marco Iacoboni). I neuroni specchio interessano anche i filosofi: per esempio, Laura Boella, che insegna filosofia morale all'Università Statale di Milano, li ritiene alla base del meccanismo di riconoscimento intersoggettivo ipotizzata dal fenomenologo francese Merlau-Ponty, nonché da gran parte della filosofia contemporanea. la presenza dei neuroni specchio è stata dimostrata con registrazione diretta tramite elettrodi intracranici solo nelle scimmie. Le prove a sostegno della loro esistenza nell'uomo sono numerose ma utilizzano sistemi di imaging come la PET o la risonanza magnetica funzionale, perché le tecniche invasive, pur possibili, sono ovviamente complesse ed eticamente controverse. Inoltre, per quanto finora si è capito, il sistema specchio nell'uomo avrebbe una struttura diversa che nell'animale e forse non sostenuta esclusivamente da popolazioni neuronali \"specchio\" ma anche da altre \"stazioni\", come il solco temporale superiore, nel quale i neuroni specchio non sono presenti a detta di tutti gli esperti. Lo studio di PNAS Usando una tecnica di risonanza magnetica che permette di misurare l'adattamento (cioè la diminuzione fisiologica dell'attività dei neuroni che vengono utilizzati ripetutamente per lo stesso scopo) di certe aree cerebrali (fMRI adaptation), Caramazza ha elaborato un esperimento, condotto su 12 soggetti, che ha misurato l'adattamento di determinate zone del cervello di fronte ad atti motori ripetuti sia attivamente sia con la sola osservazione. In teoria, sostiene il neuroscienziato di origini siciliane, se i neuroni specchio esistono davvero dovrebbero adattarsi alla ripetizione dello stimolo transmodale (cioè agito e osservato), il che, nel suo esperimento, non accade. Sempre secondo Caramazza, che sostiene con ciò di aver dimostrato l'inesistenza dei neuroni specchio nell'uomo, questi dati confermano quelli pubblicati da Ilan Dinstein della New York University nel 2007. I termini della polemica A favore di Rizzolatti e del gruppo di Parma si sono schierati immediatamente personaggi del calibro di Piergiorgio Strata, direttore dell'EBRI (l'istituto di neuroscienze voluto da Rita Levi Montalcini) che sottolinea la scarsa affidabilità della tecnica utilizzata e il numero ridotto (12) di soggetti esaminati. Lo stesso Vittorio Gallese ha ricordato che l'anno scorso, con la stessa tecnica, Nancy Kanwisher, che lavora all'MIT di Boston, era giunta a risultati diametralmente opposti. Anche dal punto di vista dell'evoluzione, dicono molti esperti, sarebbe assurdo pensare a un sistema così utile e \"parsimonioso\" che si mantiene fino ai primati ma viene perso proprio dall'uomo. A confronto ci sono due visioni del funzionamento dei processi cognitivi superiori. Per semplificare un po' con l'accetta (altrimenti qui si fa notte!), secondo il modello sostenuto dalla scoperta dei neuroni specchio, il sistema motorio non sarebbe solo deputato alla semplice esecuzione meccanica di gesti, ma sarebbe in grado di attribuire a questi stessi gesti significati ed emozioni. Se i neuroni specchio non fossero presenti nell'uomo, invece, si tornerebbe a una visione definita \"riduzionista\": un sistema motorio deputato all'azione e un sistema cognitivo superiore che \"associa\" all'azione significati sulla base di un proprio funzionamento autonomo. La teoria della mente tornerebbe ad essere quello che era in origine (un modello cognitivo separato dalla funzione motoria) mentre con i neuroni specchio è \"embedded\" all'azione (o alla sua osservazione negli altri). La polemica ha assunto anche toni decisamente sgradevoli: Caramazza difende il proprio operato e sostiene che il mondo scientifico italiano attua verso la teoria dei neuroni specchio una sorta di \"protezionismo\", mentre il gruppo di Parma lo ha accusato nell'ordine di aver pubblicato su PNAS un lavoro mediocre perché aiutato da referees amici (Iacoboni) o di aver montato un caso per far pubblicità al neonato Cimec (Gallese). Insomma, tira una brutta aria nelle neuroscienze che trascende anche l'importanza della posta in gioco: Copernico contro Tolomeo... PS: Per chi volesse saperne di più, vi segnalo che nel numero di luglio di Le Scienze ci sarà un articolo di quattro pagine, a firma di Andrea Lavazza, con le opinioni di tutti i ricercatori in causa e una sintesi della questione."},
{"title": "Architetture per la mente", "text": "Ci sono luoghi creati dall'uomo che amo particolarmente: la Sainte Chapelle a Parigi, per esempio, la biblioteca barocca dell'osservatorio astronomico di Praga o la nuova Tate Gallery di Londra, tanto per fare tre esempi realizzati in epoche totalmente diverse. Ora c'è chi sta studiando il modo con cui il nostro cervello percepisce gli elementi architettonici per stabilire come creare a tavolino la magia dei luoghi, ma anche per migliorare la qualità architettonica di ambienti particolari, come gli ospedali o le stanze dei malati di Alzheimer. Ne parla sull'ultimo numero di Neuron John P. Eberhard, presidente emerito dell'Academy of neuroscience for architecture, nata negli Stati Uniti nel 2003. Non che prima di allora gli architetti non considerassero i limiti (e i pregi) del corpo umano nell'interazione con l'ambiente, ma ora, dice Eberhard nell'articolo, bisogna che facciano propri anche alcuni concetti frutto della ricerca neuroscientifica, per esempio le modalità di apprendimento quando costruiscono scuole, i sistemi di memoria quando fanno le case di riposo per anziani e le fasi dello sviluppo sensomotorio dei prematuri nel costruire le unità di cure intensive neonatali, troppo pensate per la comodità del personale curante e poco per i piccoli pazienti. Si tratta quindi di un approccio che supera la semplice ergonomia o lo studio dell'illuminazione per dare spazio a una visione \"olistica\" della nostra interazione con l'ambiente alla luce delle modalità di funzionamento del cervello. I \"neuroarchitetti\" (oddio, un altro \"neuroqualcosa\", perdonatemi la licenza poetica   ) studiano, insieme ai neuroscienziati, come usiamo i sensi, come archiviamo e riutilizziamo le esperienze sensitive, come pianifichiamo i movimenti (anche quelli oculari: il timore reverenziale che suscitano gli spazi molto alti, come le cattedrali gotiche, dipende, dicono, dal modo con cui muoviamo gli occhi e scopriamo, quasi improvvisamente, il vuoto sopra la nostra testa). Hanno in corso, per esempio, uno studio per capire per quale ragione (e attraverso quali meccanismi) il nostro cervello è attratto dalla sezione aurea, già presente tra le regole cardine dell'architettura palladiana. Tutto ciò mi piace moltissimo e credo che comincerò a riesaminare la mia lista dei luoghi preferiti alla luce delle cose che so sul funzionamento del cervello, nel tentativo di carpirne il segreto. Anzi, mi pare proprio un bel gioco per le vacanze estive!"},
{"title": "Oggi sciopero dei blog contro il decreto Alfano", "text": ""},
{"title": "Lo psicologo sull'iPhone", "text": "Mentre nel mio precedente post ancora si discute di intelligenza artificiale e di relativi programmi, c'è chi ha fatto di più. È di ieri la notizia (arivata tramite comunicati stampa e prontamente divulgata dai siti specializzati) che dal 3 marzo prossimo il programma di intelligenza artificiale ELIZA sarà disponibile per l'iPhone con l'intento di fornire \"supporto psicologico\". Dovete sapere che da qualche settimana sono anch'io in possesso di questa nuova meraviglia tecnologica e devo dire che mi diverto molto con le diverse applicazioni, ma questa proprio non me l'aspettavo. Non so a quanto verrà venduto questo nuovo widget, ma in genere si tratta di pochi euro: un affare, rispetto agli 80-100 euro a seduta dell'analista   . Per tornare seri, credo che lo proverò per la curiosità di verificare l'efficienza dell'AI portatile (considerando la relativamente modesta capacità computazionale di un telefonino). E poi mi farò due riflessioni sulla banalizzazione della psicoterapia, ridotta a un insieme di massime (si spera almeno sintatticamenti pertinenti), un po' come il bigliettino nel biscotto della fortuna..."},
{"title": "Un cervello molto antico", "text": "Il cervello fossilizzato di un pesce vecchio di quasi 300 milioni di anni (chiamato iniopterigio) è stato recuperato in una cava del Kansas da un gruppo di scienziati del Museo di storia naturale di Parigi e dell'American Museum of Natural History di New York. Si tratta del più antico reperto fossile di cervello (un caso unico, dal momento che i tessuti cerebrali vanno rapidamente in decomposizione), la cui analisi è uscita ieri in anteprima sulla rivista PNAS (da cui è tratta anche la foto qui sotto). Nello studiare la struttura di quello che sembrava solo un \"banale\" cranio di pesce (prezioso solo perché era rimasto intero, mentre nella maggior parte dei casi si ritrovano schiacciati), i ricercatori hanno usato una tecnica di imaging molto particolare, l'olotomografia, che utilizza il raggio prodotto da un sincrotrone (nel caso specifico l'European Synchrotron Radiation Facility) e ricostruisce con l'ausilio del computer la stratificazione delle strutture. Così si sono accorti, quasi casualmente, che al centro vi era una sostanza più densa della matrice circostante, e il computer ha svelato che si tratta proprio del cervello. Sono conservati anche il cervelletto, l'inizio del midollo spinale, i lobi ottici (molto sviluppati) e alcuni nervi. L'unica parte che non si è conservata è quella frontale, probabilmente perché era troppo sottile per mantenersi sufficientemente a lungo da andare incontro al processo di fosforilazione e calcificazione. Probabilmente non ve ne importa nulla del cervello di un pesce così vecchio e poco commestibile   , ma si tratta di una scoperta davvero importante per la neuropaleontologia, che studia l'evoluzione del cervello (di quello animale ma anche di quello umano). Finora queste analisi sono state fatte basandosi principalmente sulla dimensione del cranio (cranio piccolo, cervello piccolo e viceversa). Ora, questo fossile ha una cavità endocranica molto più grande del relativo cervello, e quindi gli esperti si stanno chiedendo in che modo questa novità mette in crisi alcune certezze acquisite. L'uso dell'olotomografia con il sincrotrone, tra l'altro, potrebbe aiutare a trovare anche altri cervelli fossili che magari finora non sono stati notati. Insomma, si apre un mondo! Qui sotto vi linko il video 3D della ricostruzione tomografica: Proiezione laterale (in verde, il cranio; in rosso e blu la cavità endocranica; in giallo il cervellino).  "},
{"title": "Cervelli in dialogo", "text": "Questa settimana sono stata a Cambridge per un incontro davvero interessante, organizzato dalla Scuola internazionale superiore di studi avanzati di Trieste (SISSA). Scopo del meeting era quello di discutere i pregi e i limiti delle nuove tecniche di visualizzazione del cervello, il loro impatto sulla vita dei pazienti ma anche sulla società. Si è parlato, per esempio, di come le neuroimmagini possano modificare la percezione che la gente ha della malattia mentale (e quindi ridurre lo stigma che accompagna il malato psichiatrico); di come modificano le aspettative dei pazienti circa la propria diagnosi; di come possono essere usate in ambito legale (per dimostrare, per esempio, la non responsabilità individuale); di come (e se) è necessario regolarne l'utilizzo. Insomma, una bellissima occasione per riflettere, in modo interdisciplinare, sulle conseguenze di una nuova tecnologia: non a caso l'incontro fa parte del progetto Brain in dialogue, coordinato dalla SISSA ma promosso dalla Commissione europea nell'ambito del VII Programma quadro, che si propone di favorire la riflessione etica nell'ambito della ricerca scientifica. Dopo tre giorni di relazioni e dibattiti, a cui hanno partecipato neuroscienziati, pazienti, giuristi ed esperti di bioetica è apparso molto chiaro che il neuroimaging non è uno strumento diagnostico come un altro. In primo luogo ha contribuito all'identificazione tra mente e cervello (quello che gli anglosassoni chiamano mind embodiment), diffondendo, anche tra i medici, una visione quasi \"lombrosiana\" del funzionamento cerebrale: a tale lesione o alterazione corrisponde tale danno funzionale e, di riflesso, tale comportamento. In effetti, non è affatto così, perché le variabili individuali sono molte e perché il cervello è un organo altamente plastico, le cui funzioni vengono modulate dall'interazione con l'ambiente. Tutte queste belle immagini colorate alle quali gli studi recenti di imaging funzionale ci hanno abituati sono poi una sorta di inganno percettivo: sono la rappresentazione grafica di una sommatoria di cervelli che, tra loro, possono essere molto diversi. Si tratta quindi di una stima \"grossolana\" delle zone che si attivano in risposta a un determinato stimolo o nel compiere un certo atto. Senza contare che nel cervello molte azioni vengono compiute \"per inibizione\" (cioè bloccando l'attività di determinate aree, e quindi il loro controllo su altre), mentre il neuroimaging mostra solo le zone che accrescono il proprio metabolismo e non quelle che lo diminuiscono. Mi dicevano, tra l'altro, gli organizzatori che dovrebbero essere disponibili on line quanto prima alcune sintesi video degli interventi di Cambridge. Il progetto non finisce qui: un primo incontro italiano, al quale partecipo anch'io, è già previsto per martedì prossimo (24 marzo) a Trieste, per parlare di neuroimaging e malattia di Alzheimer."},
{"title": "Un dio che ama e uno che punisce (nella nostra testa)", "text": "È uscito ormai da quasi un mese su PNAS lo studio di Dimitrios Kapogiannis e colleghi del National Institute of Neurological Disorders di Bethesda, ma vedo girando per i blog di neuroscienze che tra gli esperti se ne discute ancora animatamente. Sarà che il tema è di quelli controversi: le basi cognitive e neurologiche delle credenze religiose. Le radici biologiche dell'esperienza religiosa sono argomento di speculazione da parte di diverse discipline, dall'antropologia alla psicologia evoluzionistica e alla genetica (chi non ricorda la questione del cosiddetto \"gene di Dio\" e altri esperimenti di cui trovate una sintesi esauriente in questo bel post su Psicocafé). Attualmente la teoria più accreditata, dal punto di vista delle neuroscienze, è che il comportamento religioso e le relative credenze siano un \"sottoprodotto evoluzionistico\" della teoria della mente, cioè della nostra capacità di pensare agli altri come soggetti pensanti. In sostanza, se riusciamo a concepire un essere superiore dotato di volontà è perché percepiamo come tali i nostri simili È probabile che anche altri meccanismi cognitivi siano coinvolti nel processo, dal momento che esistono delle \"varianti patologiche\" dello stesso (per esempio, alcune forme di epilessia temporale inducono esperienze religiose molto intense). Altri aspetti della religiosità, come il ruolo socializzante della stessa, hanno spostato l'attenzione verso il lobi frontali. Alcuni pazienti con un deficit di attività del lobo parietale hanno dichiarato di aver vissuto intense esperienze mistiche (per esempio, sensazioni di perdita di contatto con il corpo, fluttuazione eccetera). Quel che manca, dice però Kapogiannis, è un modello unitario della complessa esperienza che è la religiosità. Basandosi su una serie di analisi condotte dal Baylor Institute for Studies of Religion di Houston, Kapogiannis afferma che le due componenti chiave di tale processo cognitivo sono la percezione del livello di coinvolgimento di Dio nella realtà e la sua funzione punitiva: questo, perlomeno, in Occidente, dove il divino assume assolve principalmente a tali ruoli. Per farla breve, dopo aver infilato nella risonanza magnetica funzionale alcuni volontari, i ricercatori hanno ottenuto un'idea del network che si attiva, dlle nostre parti, nel corso di un'esperienza religiosa : le aree prefrontali e frontali posteriori, tipiche della teoria della mente, quando si pensa a un dio che interagisce con l'uomo. In particolare, lavorano i neuroni specchio, esattamente come quando si analizzano le intenzioni e le azioni di un altro essere umano. Le aree della percezione emotiva si accendono quando i soggetti pensano alle emozioni divine (amore, rabbia eccetera).  È interessante notare, invece, che quando i volontari pensano alla dottrina religiosa invece che a una divinità \"umanizzata\" si attivano le aree del pensiero astratto e dell'interpretazione del linguaggio metaforico, come per qualsiasi altra conoscenza slegata dalla realtà percepita. Ovviamente i credenti attivano più intensamente le aree emotive dei non credenti. A seconda del tipo di religiosità, la statistica mostra differenze significative nel reclutamento delle diverse aree cerebrali tra coloro che vivono la divinità come un essere principalmente punitivo (sotto, nell'immagine di destra) e chi, invece, come fonte di amore e concordia (qui sotto, a sinistra).  Ora, questo è un bello studio, dal punto di vista del disegno e dell'analisi statistica. Quello che ci dice è che senza lo sviluppo di un cervello sociale l'uomo non sarebbe neppure in grado di concepire il divino. E dice anche che il nostro cervello è fatto per rendere il divino antropomorfo nelle sue manifestazioni, almeno nella cultura giudaico-cristiana (il gruppo sta proseguendo lo studio in altre culture, per vedere se i network sono sovrapponibili) Poi, contrariamente a quanto hanno voluto far credere i giornali con titoloni sulla \"naturalità\" del sentimento religioso, non ci dice proprio nulla sulla necessità di credere, né, tanto meno, ovviamente, sull'esistenza o inesistenza di Dio."},
{"title": "Predisposti a seguire la massa", "text": "La ricerca che sarà pubblicata sul numero di domani di Neuron non è di quelle che mi mettono di buon umore. In genere mi piacciono le persone originali, quelle che esprimono opinioni non banali, che \"osano\" uscire dai percorsi tracciati... Se è così difficile trovarle, ci spiega ora il neurologo Vassily Klucharev del F.C. Donders Center for Cognitive Neuroimaging (in Olanda) è perché il nostro cervello è fatto per seguire la massa, per adeguarsi all'opinione e al comportamento della maggioranza. Niente di nuovissimo, dal momento che diversi studi avevano abbondantemente dimostrato l'effetto del giudizio del gruppo nella formazione delle opinioni individuali. Ora però, grazie alla ricerca olandese, possiamo guardare nel cervello delle persone nel momento in cui esprimono un giudizio di gradimento su una serie di volti. La risonanza magnetica ne ha registrato l'attivazione cerebrale e i neurologi hanno puntato il dito su due aree in particolare: la porzione rostrale del cingolo e il nucleo accumbens. La prima controlla la messa in atto dei comportamenti, il secondo è implicato nell'anticipazione e nel processamento delle ricompense oltre che nell'apprendimento sociale. In pratica, chi esprime un'opinione, e poi si accorge che si discosta troppo dalla media, attiva queste due aree cerebrali, che inviano un segnale di \"probabile errore\". Dal momento che, secondo la teoria del rinforzo nell'apprendimento, noi tendiamo a comportarci nel modo che con maggiore probabilità ci porterà un beneficio (per esempio il riconoscimento sociale, l'approvazione altrui), il segnale di errore e le aree cerebrali che se ne occupano costituirebbero il substrato neurologico dell'omologazione. Klucharev ha dimostrato anche che coloro che hanno un'attivazione meno intensa del nucleo accumbens (quindi che vivono una minore sensazione di conflitto) sono anche quelli che riescono a discostarsi meno dal giudizio iniziale, anche se non è conforme alla media. Ovviamente si tratta di una netta minoranza, altrimenti l'intero sistema non sarebbe più efficace. È chiaro che l'evoluzione ha preferito la conformità sociale all'originalità, con tutto quanto ne consegue di positivo (ma anche di negativo). PS: giusto per bilanciare la notizia, ho pubblicato un'originalissima opera di Karen Norberg. È un cervello lavorato a maglia, ma assolutamente corretto dal punto di vista anatomico. Di questa curiosa artista ha parlato recentemente anche New Scientist. Potete vedere altre opere analoghe cliccando qui."},
{"title": "Quando c'era Baffone", "text": "La libertà non ha prezzo, ma le liberalizzazioni sì. Questo potrebbe essere, in sintesi, la morale di uno studio apparso questa settimana su The Lancet, che ha fatto discutere non poco, anche sui giornali italiani. Un gruppo di sociologi ed economisti dell'Università di Cambridge ha infatti messo in relazione gli effetti della privatizzazione accelerata delle industrie nell'ex Unione Sovietica dal 1989 al 2002 con il tasso di mortalità, e lo ha confrontato con quanto è accaduto in altri Paesi dell'ex blocco societico che hanno però scelto una via più graduale di accesso al capitalismo. I dati sono impressionanti e mostrano un incremento della mortalità maschile a breve termine tra i 15 e i 59 anni pari al 12,8 per cento nell'insieme dei Paesi esaminati, con però variazioni che vanno da +45 per cento nell'ex Unione Sovietica al -10 per cento della Slovenia. Sotto accusa la cosiddetta shock therapy, preconizzata da economisti cone Jeffrey Sachs, secondo i quali i Paesi economicamente arretrati non possono far altro che affidarsi il più rapidamente possibile alle leggi del mercato.  Secondo gli autori dello studio, favorire gradualmente le privatizzazioni delle industrie e delle aziende, così come suggerisce la scuola detta appunto \"gradualista\" o istituzionalista, non inficia il risultato finale (ovvero il grado di sviluppo raggiunto dal Paese) ma risparmia vite umane (nel caso specifico, circa un milione). In Russia, la nazione che ha privatizzato più in fretta, tra il 1991 e il 1994 si sono persi 5 anni di aspettativa di vita. Un calo praticamente mai verificatosi prima in un Paese sviluppato. La causa sarebbe principalmente la perdita dei posti di lavoro, ai quali fa seguito la mancanza di senso sociale e di sostegno reciproco. È vero infatti che in Unione Sovietica erano le fabbriche e i luoghi di lavoro a erogare gran parte degli interventi di welfare e di prevenzione in ambito di salute ma, secondo gli esperti britannici, questo non spiega tutto. Dall'analisi, infatti, si evince un dato importante: per ogni incremento di 1 punto percentuale di popolazione appartenente a una qualsivoglia organizzazione sociale (partito, chiesa, sindacato o altro) si registra una diminuzione dello 0,27 per cento della relazione tra privatizzazione e aumento dei decessi. Quando almeno il 45 per cento della popolazione è membro di una qualche \"comunità\", l'associazione tra privatizzazione, disoccupazione e aumento della mortalità scompare, anche se i servizi statali rimangono carenti. È quello che gli esperti di Cambridge chiamano \"effetto capitale sociale\": una rete di supporto i cui effetti sulla salute sono consistenti, ancorché tutti da chiarire. Non è la prima volta che l'appartenenza a un \"gruppo\" si rivela positiva per la salute (vi sono, per esempio, diversi studi che riguardano gli anziani e la maggiore longevità associata alla frequentazione di una chiesa o di un centro di aggregazione), ma è la prima volta che funge da vero e proprio paracadute sociale in un momento di scomparsa dello Stato come erogatore di welfare. Ora, non è mia intenzione entrare in considerazioni politiche che esulano dal tema di questo blog, ma poiché i fattori di rischio maggiormente associati all'aumentata mortalità in Russia e Bielorussia sembrano essere l'alcolismo, la tossicodipendenza e il suicidio, lo studio dovrebbe almeno far discutere coloro che pensano che il mercato, lasciato a se stesso senza alcuna forma di regolamentazione o di paracadute sociale, possa essere la panacea di tutti i mali. Perché lo sviluppo economico \"selvaggio\" si paga in termini di vite umane. Mi pare una lezione tanto più attuale in questo momento di crisi economica globale: la salute, dicono chiaramente gli epidemiologi di Cambridge, passa attraverso il mantenimento di una rete di relazioni e di sostegno tra gli individui, senza la quale la disgregazione del tessuto sociale si trasforma in disadattamento individuale e generazionale e, alla fine, in anni di vita persi. --------------------------------------------------------------------------------------------------- Stuckler D, King L, McKee M. Mass privatisation and the post-comunist mortality crisis: a cross-national analysis. Lancet 2009; DOI:10.1016/S0140-6736(09)60005-2"},
{"title": "La banalità del male", "text": "L'esperimento sull'autorità di Stanley Milgram è uno dei grandi classici della psicologia sociale. Negli anni '60 Milgram elaborò un protocollo per lo studio dell'obbedienza. Un volontario veniva fatto sedere di fronte a un apparecchio con diverse leve in grado di fornire una scossa elettrica a un secondo soggetto, posto in una stanza attigua. Apparentemente si trattava di un esperimento di apprendimento (il soggetto chiuso nella stanza doveva ripetere una sequenza di parole presentate dal volontario). A ogni errore, il volontario veniva invitato, dallo sperimentatore, a somministrare una scossa \"di rinforzo\" all' \"allievo\". Le scosse erano di intensità crescente da 15 a 450 Volt. Le levette erano contrassegnate con diciture quali \"scossa lieve\", \"scossa intensa\", \"scossa molto intensa\" e così via. In realtà l'allievo era un complice dello sperimentatore e non subiva nessuna scossa: i suoi lamenti erano fittizzi. Il risultato è noto: benché l'allievo mimasse un disagio crescente e, ad un certo punto, chiedesse chiaramente di essere liberato, la maggior parte dei volontari (il 65 per cento) proseguva fino alla scossa più intensa, persino quando dalla stanza attigua non proveniva più alcun suono. Il 79 per cento dei soggetti superò la fase dei 150 Volt, in corrispondenza della quale il falso allievo supplicava di smettere e chiedeva di rinunciare. L'esperimento Milgram ha dimostrato che un'autorità ritenuta legittima può spingere una persona a superare i propri limiti morali, in nome di un \"bene\" superiore. I risultati ottenuti con questo primo studio - ispirato allo psicologo americano dal processo al gerarca nazista Eichmann - sono stati più volte confermati da ulteriori sperimentazioni che non hanno però potuto riprodurne il disegno , in quanto il livello di stress emotivo al quale erano sottoposti i volontari è stato ritenuto non etico. Ora Jerry Burger, uno psicologo dell'Università di Santa Clara ha potuto utilizzare lo stesso protocollo con lievi modifiche che lo hanno reso compatibile con i moderni standard etici e i risultati del suo studio sono usciti di recente su American Psychologist, una rivista peer reviewed. Perché ripetere uno studio già fatto? Perché, dice Burger, i suoi studenti erano convinti che oggi, 40 anni dopo Milgram, le persone sarebbero state meno passive e meno pronte a un'obbedienza acritica. Purtroppo si sbagliavano, perché i dati sono più o meno sovrapponibili. E dire che ne è passata di acqua sotto i ponti: ci sono stati i processi ai criminali nazisti, l'istituzione di Tribunali internazionali per i crimini contro l'umanità, le discussioni sui limiti tra metodi leciti di interrogatorio e tortura, le immagini dal carcere di Abu Ghraib eccetera. Eppure, tutto ciò non è bastato a inficiare il nostro innato bisogno di seguire l'autorità. E dico nostro perché una delle affermazioni più impressionanti che fanno gli psicologi che hanno condotto lo studio è che è impossibile sapere a priori chi andrà avanti e chi si fermerà: non c'è alcun segnale preventivo e nemmeno le valutazioni psicometriche si correlano in modo significativo con la capacità di esercitare il senso critico in situazioni in cui un'autorità ritenuta legittima ordina di fare qualcosa in contrasto con le proprie convinzioni. Nel 2006 il programma Primetime di ABC News girò un video sull'esperimento che trovate linkato più sotto. Vale la pena guardarlo se non altro per sentire uno dei volontari dire, 46 anni dopo Eichmann, esattamente la stessa frase: \"I was doing my job, I was supposed to do this\". ------------------------------------ Jerry M. Burger. Replicating Milgram: would people still obey today? American Psychologist; january 2009: 1-11. "},
{"title": "Se il blogger non ha parole", "text": "Da un paio di giorni cincischio con la tastiera. Non che non abbia molte cose da raccontare: come vi dicevo in un post precedente, ho passato 48 ore chiusa in un'aula dell'università di Padova per seguire un interessante convegno di neuroetica. Potrei dirvi di esperimenti curiosi, speculazioni sul libero arbitrio e la velocità di conduzione dei segnali elettrici nel cervello, di intelligenza artificiale, di mente espansa e di tante altre cose. Mi scuserete, però, ma proprio non ce la faccio. Quello che sta accadendo in queste ore in questo Paese rende qualsiasi curiosità scientifica banale e quasi fuori luogo. Alcuni amici mi hanno sollecitata, attraverso il mio account su Facebook, a scrivere ancora di stato vegetativo, di coscienza e del caso Englaro. Mi dicono che bisogna informare la gente, per evitare che passino sotto silenzio alcune aberrazioni pronunciate in questi giorni (una tra tante, l'affermazione fatta da Berlusconi secondo la quale \"il 50 per cento dei casi come Eluana si riprendono\"). Quello che avevo da dire, però, l'ho già detto in precedenza e, sinceramente, non mi pare che la battaglia in corso sul corpo di quella povera donna abbia più nulla a che vedere con le diagnosi neurologiche o la neuroetica. Una cosa, però, ho imparato in questi due giorni: che se non si apre veramente la discussione sui temi bioetici alla società civile (cosa alla quale alcuni esperti sono decisamente refrattari), si consente alla peggiore politica di farli propri impedendo qualsiasi comprensione reale dei termini della questione (e anche il loro uso strumentale). E questo vale in particolare per tutto quanto attiene agli studi sul cervello e sulla mente, in grado di scardinare convinzioni comuni su cosa è la vita, la morte, la libertà individuale, la stessa soggettività dell'uomo. Ora, durante la cena con alcuni dei relatori del convegno ho sentito dire nell'ordine: che non è ancora giunto il momento di aprire il dibattito sull'etica delle neuroscienze perché scienziati e filosofi non hanno ancora risposte sicure (come se la società civile dovesse entrare in gioco solo dopo che le Grandi Menti sono giunte a una conclusione); che non è etico chiedere a un medico di rispettare la volontà di un paziente perché il potere decisionale deve essere del medico, che ha le competenze tecniche per valutare le situazioni (detto da uno psichiatra, e pazienza se la medicina paternalistica è già stata messa in discussione da un bel pezzo); che in nessuna teoria filosofica la libertà dell'individuo è assoluta, ma è sempre subordinata al bene della società intera (e ci mancherebbe, però bisogna vedere chi lo determina, il bene sociale)... Per fortuna non tutti coloro che si occupano di questi temi sono così (non lo erano neanche a Padova), e meno male. Però credo che mai come oggi fare informazione (e informarsi) su questi temi complessi sia un dovere civile."},
{"title": "Darwin cyberpunk", "text": "Questo post volevo scriverlo ieri per festeggiare degnamente il compleanno di Darwin, ma non ho avuto tempo. E poi, diciamolo, perché essere così scontati   ? Insomma, mentre leggevo le decine di comunicati che mi sono arrivati nella mailbox per l'occasione facevo il punto mentalmente su quanto la teoria dell'evoluzione sia stata preziosa per la ricerca sul cervello e ovviamente, nella massa di idee e sperimentazioni che mi si parava davanti, ha avuto la meglio il darwinismo neuronale di Gerald Edelman. Quel che mi ha sempre affascinato della sua speculazione è l'idea che solo l'evoluzione (cioè l'influsso dell'ambiente, e quindi dell'esperienza, sul nostro cervello) possa spiegare l'individualità. Ogni cervello umano è un unicum, in cui le connessioni tra i singoli neuroni e tra le reti funzionali (i network) sono modellati sulla base della pressione selettiva. Questa intuizione di Edelman (che partiva dall'analisi della formazione dei cloni anticorpali, per i quali ha peraltro vinto il Nobel) ha preso il nome di \"darwinismo neuronale\" e, benché studi successivi ne abbiano in parte modificato i contorni, rimane comunque valida per spiegare come le connessioni tra le singole cellule possano organizzarsi funzionalmente, come questa organizzazione corrisponda alla rappresentazione mentale della realtà, come si rafforzino (elettricamente o chimicamente) le vie di comunicazione tra neuroni e quali sono gli elementi di base che ci consentono di categorizzare e generalizzare le esperienze che abbiamo grazie alla nostra interazione con l'ambiente. Ora, la stessa teoria è stata preziosissima per lo sviluppo dell'intelligenza artificiale (AI): quando si costruisce una rete neurale, anche la più semplice, il suo schema di apprendimento - la \"strada\" che gli imput elettrici faranno e la posizione degli output nella rete - è in parte imprevedibile, anche se si sottopongono due reti identiche agli stessi compiti. Questo perché la sequenza di presentazione degli stimoli piuttosto che la qualità del materiale utilizzato per creare la rete stessa (la sua stessa struttura atomica e subatomica) influenzano la configurazione finale alla luce dell'interazione con l'ambiente. Già nei primi anni '90 sono stati messi a punto progetti di ricerca sull'AI basati sul concetto di selezione evolutiva: per esempio il Guelph Darwin Project ha studiato gli algoritmi di apprendimento di alcune reti neurali sulla base degli algoritmi evolutivi del genoma umano. Esistono reti neurali costruite per lavorare grazie all'evoluzione (sono le EANN, evolutionary artificial neural networks). Le EANN sono state utilizzate anche nel campo dei videogiochi per elaborare il comportamento dei personaggi \"non giocanti\": per esempio, nel calcio vi sono alcuni giocatori della squadra che non sono comandati dal giocatore umano, ma che devono interagire con le sue decisioni, nel contesto di una competizione tra squadre. Al Virginia Tech hanno lavorato alla creazione di robot in grado di apprendere da soli dai propri errori, che non a caso si chiamano Darwin. Qui sotto potrette trovare il link a due filmati molto carini che vi spiegano come  il sistema è stato sviluppato nel tempo.   Un amico fisico che lavora in Germania sui sistemi non lineari - mentre cercavo di capire il ruolo delle neuroscienze e della teoria dell'evoluzione nello sviluppo dell'AI scansando dalle pubblicazioni originali le astruse formule a me incomprensibili - ebbe pietà di me e semplificò la faccenda mettendola così: \"Per creare una vera intelligenza artificiale, cioè un sistema in grado di apprendere, all'inizio gli esperti hanno pensato: basta capire come funziona logicamente e scrivere il programma giusto, poi lo si 'accende' e il gioco è fatto.  Secondo questa prima ipotesi, i  limiti  al progresso stavano tutti nella capacità computazionale a disposizione e nella qualità dell'hardware. Poi si è capito che così si creavano sistemi rigidi, proprio l'opposto di quello che volevamo. In qualche modo doveva entrare in gioco l'interazione con l'ambiente e la sua capacità di modificare il programma. Ma come lo modificava, se non sulla base dei risultati ottenuti dall'interazione? È il 'rientro' dell'informazione nel sistema a determinarne, in sostanza, il funzionamento. E qui è arrivato il darwinismo neuronale a dare un modello biologico attendibile alla teoria\". Detta così pare semplice ma non lo è, perché altre teorie (per esempio quella del caos) interferiscono con la linearità del sistema, ma, in buona sostanza, questo è il succo della faccenda. Poiché si è capito che anche l'intelligenza artificiale si evolve, c'è chi si è chiesto se, a questo punto, anche gli eventuali robot possano un giorno progredire autonomamente. La settimana scorsa ho ascoltato con grande curiosità la presentazione di Riccardo Manzotti, della IULM di Milano, sul tema \"Robotica e riproducibilità tecnica delle funzioni superiori\".  In mezzo a tante questioni, ha affrontato anche quella della definzione di una macchina pensante: il robot che apprende dalle proprie azioni e che è capace, per esempio, di fare scelte con contenuto etico e morale, è un oggetto o un soggetto? Non a caso ha esordito con una slide che citava Morpheus, il grande saggio di Matrix, quando racconta a Neo l'evoluzione autonoma delle macchine intelligenti e gli effetti di questo evento sul destino del genere umano (e chi non ha visto Matrix mi perdoni, ma può sempre ricorrere a Blade Runner. E se anche qui l'esempio non fa suonare alcuna campanella, vada a leggere un po' di buona fantascienza, che serve...   . Vi sono altri punti affascinanti in questo progresso rapidissimo dello studio dell'AI, per esempio quello della possibilità di espandere la mente umana o connetterla a un supporto artificiale esterno. In questo caso, che cosa potrebbe accadere? Uomini e macchine si evolveranno in parallelo, in interdipendenza o autonomamente gli uni dalle altre? Ovviamente siamo ancora lontani dalla capacità di costruire macchine pensanti in grado di acquisire completa autonomia evolutiva, ma in nuce le premesse ci sono già. Come vedete, nel mio omaggio a Darwin abbiamo fatto un giro piuttosto lungo. È il mio modo per raccontare quanto incredibilmente geniale sia stata la sua intuizione e quali ricadute abbia ancora oggi in tutti gli ambiti della scienza. Da Edelman al cyberpunk passando per le Galapagos, e che qualcuno osi ancora dire che Darwin è inutile o, peggio, obsoleto! PS: qui il link a un'intervista in podcast in cui Edelman racconta in parte quello che ho scritto in questo post (oltre a tante altre cose)."},
{"title": "Il coma non è morbillo", "text": "Ho esitato a lungo prima di scrivere questo post sul caso Englaro, perché mi pare ora di far calare un dignitoso silenzio sul dramma umano di suo padre, ma purtroppo temo di non potermi esimere dal fare qualche puntualizzazione. In questi giorni leggo sui giornali e vedo nelle trasmissioni televisive (una tra tutte il Porta a Porta di giovedì sera) una gran confusione di termini, non sempre dovuta a ignoranza ma, temo, al tentativo deliberato di fare un po' di fumo per confondere le idee della gente. Allora mi scuseranno quelli che queste cose la già le sanno, ma mi sento in dovere di ricordare che il disturbi della coscienza, di cui il coma è solo una delle manifestazioni, sono tra loro diversi e diverse sono le prognosi. Il paziente in coma in genere non è in grado di rispondere agli stimoli esterni e per lo più ha gli occhi chiusi e non mantiene il ritmo sonno-veglia. Anche nel coma ci sono diverse gradazioni che vengono misurate sulla base di una scala usata in tutto il mondo, la scala di Glasgow. Il paziente in coma può \"scivolare\" nel cosiddetto stato vegetativo persistente: quello che lo differenzia dal coma stesso è il fatto che il malato comincia ad aprire gli occhi in base alla luce e al buio e si muove, ma mai in risposta a stimoli ambientali (può anche guardarsi in giro, ma non è possibile trovare un nesso causale ripetuto tra movimento e stimolo). Dal punto di vista anatomico, questa situazione è provocata da una parziale riattivazione delle funzioni del midollo allungato e, talvolta, anche di piccole aree della corteccia che sono però disconnesse tra loro (e la connessione tra le aree corticali è uno dei presupposti necessari per esprimere una volontarietà, ovvero per reagire all'ambiente). Se tale stato perdura oltre un anno, allora si fa diagnosi di stato vegetativo permanente (che è la situazione di Eluana Englaro). Se la diagnosi è stata fatta correttamente (cioè se tutti gli esami e i test sulla funzionalità cerebrale concordano con la diagnosi), lo stato vegetativo permanente è da considerarsi irreversibile. Le autopsie effettuate sui pazienti deceduti con questa diagnosi (tra cui la ben nota e sfortunata Terry Schiavo) mostrano una pressoché completa atrofia della corteccia cerebrale. In alcuni rari casi vi possono essere errori diagnostici, per esempio perché il paziente ha anche un problema motorio (cioè è parzialmente o totalmente paralizzato): può darsi che sia invece in quello che viene chiamato stato di minima coscienza, che in parte si sovrappone alle forme più lievi di coma. Ciò significa che il malato è in realtà in grado di comprendere almeno in parte ciò che gli sta accadendo attorno ma non sa come rispondere volontariamente. Se la diagnosi è però corredata da tutti gli esami necessari e, soprattutto, se la situazione è rimasta immutata per molti anni, le probabilità di errore sono bassissime. Infine c'è la sindrome locked-in, in cui non ci sono problemi di coscienza, perché il cervello è sano e continua a funzionare all'interno di un corpo totalmente paralizzato, per esempio a causa di un ictus pontino o dell'evoluzione di malattie come la sclerosi laterale amiotrofica (vedi Piergiorgio Welby). Ora, che questi pazienti continuino a pensare dentro il loro corpo è ovvio e connaturato alle cause della loro malattia. L'altra sera, da Vespa, si è fatto, a mio avviso volontariamente, una gran confusione chiamando a testimoniare contro le decisioni di Beppe Englaro il padre di una ragazza che però NON È in stato vegetativo ma è una paziente locked-in: la differenza, come capirete, non è accademica. Ugualmente la fiction che andrà in onda in queste sere sul bimbo che si è svegliato ascoltando Venditti induce ancora più confusione, perché in quel caso si tratta di COMA. Insomma, la varicella non è morbillo e nemmeno rosolia. Ma quando si vuole tirare acqua al mulino di chi grida all'omicidio, tutto fa brodo e impedire alla gente di capire esattamente di cosa si sta parlando, indipendentemente dalle convinzioni personali, è una strategia vincente."},
{"title": "L'anno che verrà", "text": "In queste ultime settimane ho viaggiato un po’ per l’Italia per questioni di lavoro (e questa è anche la ragione della mia scarsa presenza sul blog) e ho chiacchierato con tassisti del Nord, del Centro e del Sud. Si sa, i tassisti sono la voce del popolo per eccellenza, e così mi ha colpito il fatto che tutti hanno attaccato bottone parlando della crisi finanziaria, del nuovo anno che verrà e dei soldi che mancano. Uno di loro, particolarmente attrezzato, mi ha detto che registra i suoi introiti su un foglio Excel e che rispetto al mese di novembre del 2007 ha incassato il 19,5 per cento in meno. “Il taxi è un lusso, signora mia!”, e io ho annuito convinta con un’occhiata al tassametro. La conclusione era sempre la stessa: “Quest’anno, niente regali di Natale, e anche i bambini dovranno accontentarsi”. Essere ottimisti è davvero dura, quando ci si mette anche il segretario generale della CISL Raffaele Bonanni (“il 2009 sarà un anno orribile”) e persino Cesare Romiti, che dalla scomoda poltrona di Ballarò ha annunciato cataclismi per l’economia mondiale e italiana in particolare.Il colpo di grazia me l'ha dato mia madre, raccontando del bisnonno rovinato dalla crisi del ’29 e costretto contare sull’aiuto pubblico per sfamare i 13 figli.Anch’io ammetto di essere diventata più parca: il lavoro diminuisce, il futuro è incerto, si sta attenti a quel esce dal portafoglio. Ogni tanto il Solone di turno minaccia: se non ripartono i consumi, qui finisce male. Ma per far ripartire i consumi serve l’ottimismo della gente, un bene che scarseggia sugli scaffali.Questa crisi finanziaria, dicono gli esperti di economia comportamentale (una specialità a cavallo tra l’economia e la psicologia) ha messo in difficoltà alcuni modelli interpretativi. Lo stesso Alan Greenspan, ex presidente della Federal Reserve statunitense, ha ammesso di essere stato colto di sorpresa: “Ho fatto un errore presumendo che gli interessi personali delle istituzioni, in particolare di banche e simili, fossero tali da costituire la migliore garanzia a protezione dei loro stessi beni e della solidità delle loro imprese”.Il New York Times ha dedicato un articolo al coming out degli studiosi di psicologia dell’economia: in sostanza questi esperti un po’ bistrattati dai \"veri\" economisti sono usciti alla luce del sole per ricordare che loro avevano previsto tutto il baillamme che stiamo vivendo. Da un bel po' di anni dicono che le decisioni economiche che ogni individuo prende non sono razionali e non sono governate dall’obiettivo di ottenere sempre il massimo, ma sono guidate dalle percezioni, cioè da un complesso sistema cognitivo che influenza la valutazione delle prospettive future. Solo così si spiega la cecità degli investitori sui rischi reali legati, per esempio, ad alcuni tipi di investimento come quello sui mutui subprime.Nassim Nicholas Taleb, esperto di matematica finanziaria e filosofo, ha creato alcuni anni fa la “teoria del cigno nero” per spiegare il modo con cui gli individui percepiscono eventi casuali potenzialmente benefici o malefici, ma comunque al di fuori dalla norma: semplicemente, non li vedono, perché la nostra mente ha bisogno di ricondurre gli accadimenti del mondo a un progetto ordinato.Questo vale sia nei periodi di ottimismo sia in quelli di pessismismo: poiché la nostra mente si è evoluta per prendere decisioni in un mondo decisamente meno complesso di quello attuale, secondo Taleb la nostra capacità di governare i rischi è inficiata dalla tendenza a vedere negli accadimenti una conferma dei nostri pregiudizi. Inoltre, nel farci un'idea del futuro, sovrastimiamo gli eventi recenti, tendiamo ad attribuire fatti diversi a una causa comune e infine, siamo convinti che ciò che ci accade di buono sia frutto della nostra genialità e non di una semplice botta di fortuna. In economia tutto ciò si traduce in un comportamento individuale perfettamente idiota. Se tutto va bene non vediamo i segnali di allarme, se va male ci chiudiamo a riccio per proteggere quel poco che abbiamo e, di conseguenza, non agiamo razionalmente per aprirci prospettive per il futuro.Le idee di Taleb, che pure qualcuno guarda con sospetto, non sono poi molto dissimili da quelle di Daniel Kahneman, psicologo, vincitore, nel 2002, del premio Nobel per l’economia per aver integrato allo studio di quest’ultima le tecniche della psicologia cognitiva. Uno dei punti chiave della sua \"teoria delle prospettive\" afferma che in economia le decisioni non sono razionali e che sono molto influenzate dall’inquadramento, cioè da come la situazione generale viene presentata a chi deve decidere.Tornando ai tassisti, al bisnonno e alle mie spese natalizie, è evidente che più sento dire da persone che dovrebbero conoscere l’economia che ci aspettano tempi grami, più mi convinco che è ora di chiudere il portafoglio. Il problema è che ci sono anche alcuni esperti, specie politici nostrani   , che invece mi dicono che non c’è ragione di aver paura e che mettere i soldi sotto il materasso non aiuterà a migliorare la situazione. Sarà che della seconda categoria mi fido poco, ma al momento sto facendo la formica."},
{"title": "Contagiatemi di felicità", "text": "Dallo studio Framingham, uno dei caposaldi della moderna epidemiologia, abbiamo imparato quasi tutto quello che oggi sappiamo sulla relazione tra stili di vita e malattie cardiovascolari. Ora dobbiamo agli abitanti di questa famosa cittadina statunitense anche il segreto della felicità: circondarsi di persone felici. Oggi è uscito,  sul British Medical Journal, uno studio longitudinale su quasi 5.000 soggetti seguiti per oltre 20 anni in modo indiretto (ovvero attraverso le loro cartelle cliniche che contengono anche una valutazione psicologica periodica con scale per la depressione). Obiettivo: scoprire se la felicità può diffondersi da persona a persona e, soprattutto, come si formano i cluster di persone felici. Una sintesi dello studio la trovate su questo stesso sito. Gli studi sulla felicità, dicono gli autori, si sono finora concentrati sugli aspetti socioeconomici e genetici, ma non sugli aspetti relazionali. Inoltre diverse ricerche hanno dimostrato che le emozioni sono contagiose, ma nessuno si è chiesto se esiste un \"network della felicità\", ovvero se questa emozione (e il suo inverso, ovvero l'infelicità) si può trasmettere anche in modo indiretto (per esempio se la mia felicità può contagiare l'amico di un mio amico).     Le conclusioni sono curiose e, soprattutto, importanti anche dal punto di vista sociale, perché affermano che la felicità è un'emozione che si distribuisce secondo la logica del network, e che si presenta in cluster di individui legati tra loro fino a tre gradi di separazione. Se guardate l'immagine che ho caricato dal BMJ, l'idea risulterà più chiara. Il grafico in alto rappresenta la situazione del gruppo esaminato nel 1996 e quello in basso l'evoluzione nel 2000. Ogni nodo rappresenta un individuo (tondo per le donne, quadrato per gli uomini). Le linee tra i nodi indicano la relazione (nere per i familiari, rosse per amici e partner). Il colore del nodo indica il grado di felicità (blu i più tristi, verde tendente al giallo i più felici). Quindi un determinante chiave della nostra felicità è la felicità di chi ci sta vicino. Gli stati emotivi si trasferirebbero da un individuo all'altro attraverso la mimica, specie quella facciale. In gioco ci sono i soliti neuroni specchio, non a caso coinvolti nelle percezioni delle emozioni degli altri veicolate attraverso il movimento . Quel che colpisce, del modello messo a punto sugli abitanti di Framingham, è che in base a un algoritmo e conoscendo il mondo relazionale di un individuo è possibile predire chi sarà felice negli anni a venire e chi no. Inoltre le persone felici tendono a essere naturalmente il centro del loro cluster relazionale, e questo, dicono gli autori, confermerebbe il ruolo dell'evoluzione in tutta la faccenda (dato che siamo animali sociali, mettiamo al centro della nostra rete chi ci può apportare benessere). Inquietante, invece, il fatto che la felicità non è solo funzione dell'esperienza individuale o delle nostre scelte, ma è una proprietà del gruppo. Se a questo fatto sommiamo i risultati di tutti gli studi sui determinanti genetici del benessere psicologico, non ci rimane molto nelle mani...  Poiché la felicità è un elemento che influenza in modo sostanziale lo stato di salute (anche secondo l'OMS), il British Medical Journal si chiede se non debba essere un dovere sociale quello di favorire questo contagio (che farebbe risparmiare un sacco di soldi ai sistemi sanitari nazionali   ) . A mio avviso, il punto critico dell'analisi è proprio la definizione di felicità: se fosse così semplice da misurare (ovvero solo come assenza di depressione o comunque di umore sottotono), sarebbe falice farci tutti felici (magari con un pillolone!). Inoltre la significatività statistica del risultato è un tantino bassa (vedi più sotto gli intervalli di confidenza). Rimane il fatto che, qualsiasi cosa abbiano valutato, chi ci sta intorno influenza la nostra vita,: un amico che abita a 1,5 Km da noi ed è felice aumenta la nostra probabilità di felicità del 25 per cento (con un intervallo di confidenza al 95% un tantino elevato, da 1 a 57 per cento). I partner felici la incrementano dell'8 per cento (ma con un IC 95 da 0,2 a 16 per cento), i nostri allegrissimi vicini di casa del 34 per cento (IC 95 tra 7 e 70 per cento!). Vabbé, accontentiamoci, anche perché un dato scientificamente sicuro c'è: i nostri colleghi non ci infleunzano per niente, anche se passiamo otto ore al giorno con loro. Magia del posto di lavoro, che funziona da schermo antifelicità meglio dello scudo spaziale."},
{"title": "Doping cerebrale", "text": "Sono giorni che mi gira per la testa quanto ho letto lunedì su Nature: un lungo articolo, firmato da alcuni neuroscienziati \"di grido\", tra cui Michael Gazzaniga e Martha Farah, che in sostanza chiede di liberalizzare l'uso dei farmaci che potenziano le capacità cognitive. Ne ha scritto anche Elena Dusi su Repubblica. Tutto nasce da un sondaggio online effettuato sempre da Nature qualche mese fa tra i suoi lettori, dal quale si evince che dal 7 al 25 per cento degli studenti USA assume o ha assunto sostanze per migliorare le proprie performance intellettuali (e con loro anche diversi docenti e ricercatori). I farmaci più gettonati sono il metilfenidato e i sali di amfetamina, ambedue nati per la terapia della sindrome da iperattività e deficit di attenzione nell'infanzia. Gira di straforo anche il modafinil, un farmaco registrato per la cura della narcolessia e degli effetti negativi del lavoro notturno. Oltre a questi, alcuni studi (tra cui quello di Lynch e Gall uscito su Trend in Neuroscience nel 2006) dimostrerebbero che i farmaci per l'Alzheimer, che agiscono sul sistema dell'aceticolina, migliorano la memoria nei soggetti sani. Questa sorta di doping cerebrale è illegale negli Stati Uniti come in Italia: i firmatari dell'appello su Nature chiedono ora di depenalizzarne l'uso, dal momento che, a loro avviso, l'umanità già pratica da lungo tempo il \"potenziamento cerebrale\", con l'educazione ma anche con l'ausilio delle tecnologie (computer, Internet eccetera). Quindi perché non i farmaci? Ovviamente pongono alcune condizioni, per esempio che si facciano studi anche sui soggetti sani per valutare il profilo rischio-beneficio. Dal punto di vista etico, i neuroscienziati vedono un solo problema: quello di aumentare le disparità interindividuali, se questi prodotti dovessero essere accessibili solo ad alcuni. Per il resto, dicono, qualsiasi azione volta a migliorare le proprie prestazioni (compreso l'uso di interfacce uomo-computer, come microchip impiantabili o apparecchi per la stimolazione intracerebrale profonda) è eticamente accettabile. C'è un altro punto su cui chiedono attenzione: quello della libertà individuale. Nessuno dovrebbe essere obbligato ad accrescere il proprio potenziale cerebrale, anche se nell'articolo si equipara l'uso di queste sostanze all'educazione scolastica obbligatoria. Inoltre bisogna avere la certezza, soprattutto se si pensa di utilizzarle sui bambini, di non alterare lo sviluppo cerebrale. Anche la libertà, però, potrebbe avere dei limti: sarebbe giusto, si chiedono, accettare che un chirurgo non assuma un farmaco che lo rende molto più lucido e preciso, dal momento che il suo datore di lavoro lo obbliga pur sempre a seguire un determinato corso di studi? Non si può non discutere dell'argomento, dicono gli autori, perché se non lo fanno i neuroscienziati, gli educatori e il legislatore, lo farà il mercato clandestino e non controllato, perché la posta in gioco è alta, visto l'allungamento della durata di vita e l'invecchiamento della popolazione (e vista la corsa alla performance, dico io). Quindi è necessario mettere in piedi al più presto un programma di ricerca sui meccanismi di potenziamento cerebrale, che dia una base teorica sicura alle discussioni successive. Questo, in soldoni, il contenuto dell'articolo di Nature che, lo ammetto, mi ha lasciato perplessa. Non riesco proprio a considerare l'uso di apparecchiature e tecnologie alla pari con qualcosa che entra nel mio cervello o che con esso si interfaccia (come un biochip): mi pare che ci sia un salto di qualità. È vero che l'apprendimento scolastico modifica i miei circuiti neuronali, ma è pur vero che utilizza le mie potenzialità naturali. E non trovo che potenziare il cervello sia come potenziare qualsiasi altro organo, dal momento che il cervello determina la mia identità, mentre gli altri organi sono, di fatto, \"intercambiabili\" (se mi trapiantano un cuore, io resto sempre io, ma senza il mio cervello non sono più la stessa). E, per finire, questa storia del potenziamento cerebrale mi ricorda alcuni inquietanti scenari alla Philip Dick, oppure il \"Mondo nuovo\" di Huxley, con una parte dell'umanità tenuta a bada con le pillole di \"soma\". Insomma, non so bene cosa pensare: è una prospettiva di cui capisco il fascino e percepisco il lato oscuro."},
{"title": "Esercizi di felicità", "text": "Cercavo qualcosa di leggero e beneaugurante per l'anno nuovo, e sono incappata in un esercizio \"evidence-based\" per aumentare il proprio buonumore. Io ci credo poco, a questi trucchi di psicologia comportamentale spicciola che tanto piacciono agli americani, ma sta per iniziare un anno nuovo e tentare non nuoce! Bastano due minuti al giorno per esercitare l'arte della gratitudine: elencate tre cose per le quali siete grati alla vita, e senza le quali la vostra giornata sarebbe peggiore. A quanto pare non servono pensieri profondi , bastano osservazioni come \"Oggi non ho mal di testa\", \"Che bel pranzetto\", \"Che divertenti le mie bambine\". Pensatelo e poi archiviate il tutto!  Col tempo pare che il vostro livello medio di benessere si alzerà. Emmons e McCoullogh lo hanno dimostrato nel 2003, con uno studio randomizzato controllato: un gruppo ha scritto, una volta la settimana, per 10 settimane, una lista di 5 cose per le quali essere grato. Al termine dell'esperimento chi aveva fatto l'esercizio registrava un incremento del 25 per cento di felicità (oltre a un sonno migliore) rispetto al gruppo di controllo. Una revisione del 2005, invece, sostiene che è possibile incrementare l'ottimismo con questo tipo di esercizi e di conseguenza aumentare il proprio benessere generale. Infine, sempre lo stesso anno, alcuni psicologi USA hanno nuovamente misurato i benefici degli \"esercizi di gratitudine\". Insomma, la faccenda mi ricorda un libro per il quale andavo pazza da bambina: Pollyanna, di Eleanor Porter. Lo scrisse nel 1913: era piuttosto avanti con i tempi   . E con ciò, vi auguro un felice 2009."},
{"title": "Tre segnalazioni", "text": "Mi è arrivato ieri, e stamattina non sono riuscita a lavorare, tanto sono rimasta affascinata dal diario-racconto che Mario Riccio, insieme alla giornalista Gianna Milano, ha appena pubblicato per l'editore Sironi (Gianna Milano, Mario Riccio. Storia di una morte opportuna - Il diario del medico che ha fatto la volontà di Welby - Sironi editore, in libreria dal 10 ottobre). La vicenda umana e professionale di Riccio, l'unico medico che si è offerto di aiutare Piergiorgio Welby a rifiutare le cure (lui non accetta che il suo atto sia chiamato eutanasia, e leggendo capirete perché) è, a mio avviso, una bella iniezione di fiducia nella classe medica, che non è fatta solo di individui che badano al proprio tornaconto. Preziosissima la parte di note curata da Gianna Milano, grande esperta di bioetica, che permette al lettore di inquadrare la vicenda nel contesto dei media, delle leggi, della politica e della discusssione sugli atti di fine vita. Ne abbiamo parlato anche su questo blog, ed è bene che non cali il silenzio sulla questione. Segnalo anche, a chi non lo avesse letto sui giornali, che è uscito anche un libro scritto da Beppino Englaro con Elena Nave, intitolato \"Eluana. La libertà e la vita\", editore Rizzoli. Non l'ho ancora letto, ma sono contenta che sia stato scritto. Anche di questo caso abbiamo già parlato. Di tutt'altro genere la terza segnalazione: il 24 ottobre ci sarà a Ferrara un interassante convegno interdisciplinare sui neuroni specchio - \"Neuroni specchio - La relazione empatica tra scienza, filosofia, arte e cura\". Ci sarà Vittorio Gallese, ma anche molti altri relatori interessanti. Vi allego il programma in pdf. dep-web-a3-conv-neuroni-specchio-fe-24-10-08.pdf"},
{"title": "Un giorno perfetto sotto le grinfie di un persecutore", "text": "Di stalking, ovvero della tendenza di alcune persone a mettere in atto vere e proprie campagne persecutorie nei confronti di ex mogli o mariti, amanti o persino personaggi famosi, si parla molto in questi giorni a causa dell'uscita nelle sale del film di Ferzan Ozpetek \"Un giorno perfetto\". Non sono ancora riuscita ad andare a vedere il film, che è stato proiettato persino alla Camera dei deputati da un gruppo di parlamentari del PD che vuole promuovere una legge antistalking, ma la storia è simile a quella di molti casi di cronaca che leggiamo sui giornali: un marito apparentemente normale, lasciato dalla moglie che vuole divorziare, non riesce ad accettare l'abbandono ed entra in una spirale di violenza psicologica e fisica nei confronti dell'ex moglie. A volte, come in un paio di recenti casi di cronaca, la vittima ci rimette la vita, così come, purtroppo, anche a coloro che alla vittima sono cari, come figli o genitori. Ora due dei massimi esperti mondiali di stalking, gli australiani Rosemary Purcell e Paul Mullen, hanno studiato circa 20.000 episodi di stalking contro la famiglia reale britannica in un periodo di 15 anni, per cercare di comprendere quanti dei persecutori di VIP sono affetti da disturbi psichiatrici e quanti possono diventare effettivamente pericolosi. Dopo aver scremato gli episodi dovuti, per esempio, a ubriachezza o ad altre cause contingenti (diligentemente registrati dalla polizia londinese), gli esperti hanno selezionato circa 250 episodi che rispondono alle definizioni più comunemente accettate per lo stalking (un'intrusione importante nella vita della vittima, un'ossessione amorosa non giustificata dalla realtà dei fatti o, nel caso dei VIP, la convinzione di essere parenti rigettati o viceversa, di doversi rivolgere a loro per chiedere riparazione di quanlche ingiustizia). Circa l'80 per cento dei responsabili di questi atti soffrivano di schizofrenia, allucinazioni o altre psicosi gravi. Secondo Mullen, i persecutori dei VIP sono più frequentemente affetti da malattie psichiatriche dei persecutori della gente comune, benché anche in quel caso spesso si riscontrino chiari segni di depressione e di comportamento antisociale. Nella maggior parte dei casi, questi ultimi sono amanti rifiutati, affetti da vera e propria \"erotomania\". La depressione, però, è un fattore di rischio per comportamenti violenti e autolesionisti al contempo, come accade appunto in chi uccide l'ex moglie e poi commette suicidio. Viceversa, chi spara al VIP (come il famoso assassino di John Lennon) raramente poi si fa violenza. Un altro punto importante riguarda l'identità delle vittime: tra i VIP, sono perseguitati in ugual modo maschi e femmine, ma nello stalking \"familiare\", le donne sono molto più spesso vittime che persecutrici, perché alla base del comportamento violento c'è un forte sentimento di inadeguatezza maschile, di virilità minacciata dall'abbandono. Personalmente conosco almeno un paio di persone che sono state pesantemente perseguitate da ex fidanzati: una addirittura è andata talmente in crisi da vivere per anni senza il nome sul citofono e con il numero di telefono segreto, nel terrore delle chiamate notturne, di cui inizialmente non aveva nemmeno individuato l'autore (era una persona con cui aveva avuto una relazione di pochi giorni svariati anni prima). In nessuno dei due casi c'è stata molta comprensione da parte della polizia, se non dopo qualche anno e dopo episodi veramente gravi, come minacce di morte e insulti a sfondo sessuale scritti con lo spray sul muro del palazzo delle vittime. Per arrivare in tribunale, poi, ci sono voluti altri anni, e la condanna del persecutore non comprendeva alcuna forma di supporto psichiatrico, il che significa che se c'è sotto una patologia, nessuno l'ha diagnsticata né tantomeno curata."},
{"title": "Quel politico è pazzo!", "text": "Man mano che si avvicinano le elezioni presidenziali americane, i giornali e i blog d'oltreoceano non parlano d'altro. E quelli che si occupano di psicologia o neuroscienze non fanno eccezione. Si va dall'analisi del \"body language\" dei due candidati Obama e McCain (per capire se sono ansiosi, se dicono la verità, se sono sufficientemente sicuri di se stessi) alle valutazioni sulla loro stabilità mentale (e su quella dei loro vice, vedi Sarah Palin). Così mi sono ricordata che un mesetto fa la rivista JAMA, edita dall'American Medical Association, ha pubblicato un bell'editoriale sul ruolo dei medici e degli psichiatri nel discutere di personaggi pubblici. Si può valutare la stabilità mentale di un politico sulla base di quello che è di dominio pubblico circa la sua vita? È lecito che uno psicologo o uno psichiatra si sbilancino a esaminare con gli strumenti propri della loro professione la performance di un politico in un dibattito televisivo (o la propensione di un altro a circondarsi di \"nani e ballerine\")?. JAMA ricorda che nel 1964, in occasione della sfida Goldwater-Johnson, la rivista Fact promosse un'indagine presso i membri dell'American Psychiatric Association (APA) per conoscere la loro opinione sulla sanità mentale del candidato Barry Goldwater: la maggior parte degli psichiatri affermò che non era adatto a diventare presidente degli Stati Uniti, in quanto affetto da \"megalomania\" \"paranoia\" e \"grezzamente psicotico\".  In sostanza diedero un parere professionale (usando termini clinici) su una persona che non avevano visitato personalmente: il candidato presidente vinse una causa contro Fact, che gli versò un milione di dollari di risarcimento.   Nel 1973 l'APA promosse una carta etica della professione psichiatrica che contiene la cosiddetta \"regola Goldwater\" secondo la quale uno psichiatra interpellato dai media può dire la sua su una patologia in generale, ma non applicare quell'etichetta al personaggio famoso (questo vale, ovviamente, anche per i casi di cronaca nera in cui sono chiamati a dire la loro vari criminologi, vedi in Italia il caso di Cogne).  In sostanza, dice l'editoriale di JAMA - ricordando il caso del governatore Spitzer costretto a dimettersi per aver passato una notte brava con una prostituta dopo aver tuonato per anni contro il dilagare del vizio - uno psichiatra è libero di spiegare al pubblico che determinati individui possono essere affetti da una personalità narcisista e quali sono le manifestazioni di questo disturbo, ma non possono dire che Spitzer è un narcisista (quindi non possiamo dirlo neanche noi qui in Italia del nostro presidente del Consiglio   ).  Una distinzione sottile, ma importante, perché comunque il pubblico ha diritto a sapere in che stato mentale è chi lo governa. Sempre JAMA cita l'esempio di Ronald Reagan, che pochi anni dopo aver lasciato la Casa Bianca risultò affetto da Alzheimer. Pare che diversi giornalisti e collaboratori del presidente avessero notato, quando era ancora in carica, che era piuttosto distratto (per non dire smemorato) e che talvolta risultava assente. Nessuno ne parlò, e tutt'ora non si sa quando Reagan si sottopose per la prima volta a una valutazione neuropsicologica formale.  Come giornalista, mi sono trovata qualche volta (poche, per fortuna) nella scomoda situazione di dover scrivere di un certo argomento con un caporedattore che vuole a tutti i costi \"l'opinione dell'esperto\". I miei referenti più seri in genere si rifiutano di parlare di persone che non hanno mai visitato, quindi, se proprio sono obbligata, attingo a una lista di \"tuttologi\" di cui ovviamente non rivelerò mai i nomi   ."},
{"title": "Una, cento, mille coscienze", "text": "La settimana scorsa, a Pavia, è stato organizzato un convegno sul tema della coscienza, affrontato sia dal punto di vista della filosofia sia da quello delle neuroscienze. L'occasione è stata la consegna di un premio a Patricia Smith-Churchland, neurofilosofa di origine canadese ma che vive e lavora a San Diego.  La Churchland è poco amata da molti suoi colleghi filosofi perché da sempre sostiene che temi come il libero arbitrio o la coscienza debbano essere periodicamente rivisti, anche dal punto di vista filosofico, alla luce delle scoperte delle neuroscienze. Insomma, la fine del \"penso dunque sono\" cartesiano. Questo ruolo subordinato non piace ad alcuni filosofi, tra i quali anche Elio Franzini, professore di estetica e fenomenologo, che ha sostenuto con grande fervore la \"supremazia\" della filosofia sulle neuroscienze, intendendo con ciò la necessità di comprendere se l'oggetto di discussione (nel caso, la coscienza umana) possa essere davvero analizzato alla luce della biologia. Secondo lui non è possibile, perché ciò che chiamiamo coscienza è in realtà il frutto dell'interazione tra la biologia e la cultura. In sostanza, ha detto, non tutto è riconducibile al livello mentale e corporeo ma vi sono aspetti cognitivi per così dire \"estrinseci\", legati ad aspetti storici. Non a caso Franzini si rifà a Husserl. Non era d'accordo con lui Salvatore Veca, che ha detto chiaramente che la fenomenologia è l'estremo tentativo (\"eroico\", l'ha definito) di mantenere il primato della filosofia sulle scienze. Ma è la Churchland che ha dato, a mio avviso, il contributo più intrigante, ricordando come, storicamente, la filosofia si sia sempre occupata dei problemi che la scienza fino a quel momento non era in grado di analizzare con strumenti empirici (dall'astronomia alla natura del fuoco, tanto per fare degli esempi), ma che poi si è \"ritirata\" quando le scoperte scientifiche hanno gettato una luce oggettiva sui fenomeni della natura. Così deve essere, secondo lei, anche per gli studi sulla coscienza, anche se bisogna intendersi bene su cosa significa \"coscienza\": quella che oggi chiamiamo con un termine unico è probabilmente un insieme di manifestazioni che hanno cause diverse. Per rendere chiaro il concetto, ha fatto l'esempio del fuoco: un tempo si pensava che il fuoco del camino, il luccichio delle lucciole e  il sole fossero manifestazioni dello stesso fenomeno, oggi sappiamo che in un caso la causa è l'ossidazione, in un altro la biofluorescenza e in un altro ancora la fusione nucleare. Con la coscienza, dice Churchland, accadrà così. Le hanno indirettamente dato ragione i neuropsicologi e gli esperti di neuroimaging. In particolare Gabriella Bottini ed Eraldo Paulesu hanno chiarito benissimo i progressi (e i limiti) delle neuroscienze nello spiegare fenomeni complessi come la coscienza umana: attraverso lo studio di alterazioni parziali dei fenomeni di coscienza, come la percezione del Sé corporeo e dello spazio oppure della memoria si possono costruire, come un puzzle, modelli del funzionamento del nostro cervello, ma ancora non una risposta definitiva e globale di ciò che è la coscienza e di come funziona. Tornando alla Churchland, secondo lei il compito delle neuroscienze è \"riduzionistico\", nel senso migliore del termine, ovvero di ricondurre a un modello unitario fenomeni che avvengono a diversi livelli (a livello psicofisico, dei sistemi neurali, dei network funzionali o addirittura a livello del singolo neurone o delle sinapsi). Che questo serva anche alla filosofia è indubbio, per esempio è servito a chiarire (o meglio, a confermare, perché i filosofi già lo dicevano) che non c'è coscienza se non c'è linguaggio. E i filosofi che devono fare, cambiare mestiere? Io non credo, perché il loro contributo alla comprensione di ciò che siamo rimane fondamentale (tanto più se non si è d'accordo con Francis Crick, che sostiene che la biologia sarà un giorno in grado di spiegare tutto della mente umana) ma, come ha detto Veca in conclusione del convegno, non sono più liberi come un tempo di spaziare ovunque con le loro teorie , perché devono tenere conto dei paletti che le scoperte delle neuroscienze hanno inevitabilmente posto sulla loro strada."},
{"title": "Una nuova lingua si impara per osmosi", "text": "L’altra sera, a una festa, ho intercettato la conversazione di due “sciure” milanesi, preoccupate per le conseguenze della riforma Gelmini sul futuro scolastico dei loro bambini. Su un punto soltanto davano ragione alla “ministressa”: quello delle famose classi ponte per insegnare la lingua italiana ai figli di immigrati. Motivo altruistico: “Poveri bambini, in classe arrancano e sono isolati perché non capiscono niente. Meglio farli arrivare nella classe normale quando già parlano l’italiano”. Così ho pensato che di questo argomento si è detto molto in termini sociali, etici e politici (non che non serva, ovviamente), ma che per una volta la scienza qualcosina da dire ce l’avrebbe. E allora diciamolo. E cominciamo con il ricordare che l’apprendimento di una seconda lingua è un argomento studiatissimo, anzi, uno dei primi argomenti studiati dalle scienze cognitive insieme alla rappresentazione cerebrale delle aree del linguaggio di Broca e Wernicke! Non ci vorrebbe molto, quindi, a progettare un sistema didattico che tenga conto di quanto sappiamo sui meccanismi di apprendimento invece che su altre questioni, ma vabbé… La fonte più diretta di apprendimento di una lingua è la lingua stessa, come hanno dimostrato vari studi: e per imparare in fretta, il contatto con la nuova lingua dovrebbe avvenire a un livello solo un tantino più complesso del proprio, in particolare per quanto riguarda il vocabolario (almeno secondo la monitor theory del linguista Stephen Krashen). Ciò significa che se si vuole insegnare rapidamente una nuova lingua ai bambini, bisogna certamente prevedere dei corsi ad hoc per fornire loro qualche elemento di partenza. Attenzione, però, perché già negli anni ‘90 Mike Long, linguista dell’Università del Maryland, ha dimostrato sperimentalmente la cosiddetta teoria dell’interazione, ovvero che l’acquisizione di una nuova lingua è accelerata e migliorata dall’uso della stessa nelle interazioni quotidiane. Scoperta dell’acqua calda, in un certo senso  , giacché fin dall’800 si spediscono i giovani a imparare una nuova lingua all’estero e si cerca di tenereli separati dai loro concittadini affinché la full immersion abbia effetto. E che già basterebbe a smentire l’idea che le classi separate siano un servizio a favore del bambino straniero, che non verrebbe esposto all’italiano o che si troverebbe esposto a un italiano quantomeno traballante, visto che condividerebbe la giornata con altri “non parlanti”. Aggiungiamoci che il cosiddetto insegnamento esplicito (cioè quello formale, basato su fonologia, semantica e grammatica) ottiene risultati inferiori al cosiddetto insegnamento sociopragmatico (banalmente, imparo perché mi serve comunicare). Certo, poi bisogna correggere gli errori dell’apprendimento spontaneo con quello formale, e a questo dovrebbero servire i corsi di lingua per stranieri  erogati, però, da personale appositamente preparato (la didattica non è mica cosa che si improvvisa!) e che la scuola non può esimersi di offrire, se il suo scopo è l’integrazione (linguistica, ma non solo). Anche sugli errori, peraltro, vi sono due teorie: a grandi linee, secondo i seguaci di Chomsky e della sua teoria della grammatica universale, gli errori che fanno i bambini che imparano una seconda lingua sono solo dovuti alla scarsa performance, specie se l’esposizione alla nuova lingua avviene dopo la cosiddetta “età critica” (quella entro la quale si ha accesso facilmente ai meccanismi della grammatica universale e quindi si impara meglio e più in fretta), concetto peraltro ancora molto controverso. Per gli psicolinguisti, invece, gli errori sono affettivamente connottati, e dipendono anche dalle situazioni emotive che hanno caratterizzato la fase di apprendimento: la nuova lingua è servita a crearsi nuove amicizie o è stata fattore di esclusione? I genitori danno valore ai progressi compiuti e vedono positivamente l’integrazione scolastica e linguistica oppure no? Diversi studi dimostrano anche che lo stress abbassa la performance linguistica, e infatti tutti noi parliamo una lingua straniera molto peggio se stiamo facendo un esame piuttosto che conversando amabilmente intorno a un tavolo (e anche questo dovrebbe far riflettere sulla proposta di istituire esami di italiano per l’ammissione a scuola, valutazione attualmente fatta informalmente dagli insegnanti che osservano il bambino nella vita di tutti i giorni).  In tutti i casi, la separazione dei nuovi arrivati dai bambini italiani ne ritarderebbe l’integrazione linguistica."},
{"title": "Il fascino della simmetria e dell'incompletezza (ESOF 2008 - 4)", "text": "Ci sono scienziati che hanno davvero il dono dello showman: è il caso del matematico Marcus du Sautoy, dell'Università di Oxford, che a Barcellona si è prodotto davanti a una platea di oltre un migliaio di partecipanti a ESOF, nell'aula magna della Fiera. Tema dell'incontro: la simmetria nell'arte e nella natura e le sue basi matematiche. Persino io, che come ho detto più volte, per la matematica non ho proprio il bernoccolo, sono rimasta affascinata nello scoprire la perizia dei ceramisti arabi che, nel creare le piastrelle dell'Alhambra di Granada, hanno applicato alcune delle più complesse leggi della matematica per dare vita a decorazioni sempre diverse (ci sono piastrelle che rispondono a ben 17 diverse leggi della simmetria, e pare che i matematici in visita al monumento si facciano un punto d'onore di identificarle tutte!). Per non parlare della ben nota simmetria nelle variazioni Goldberg di Bach... E poi ho ammirato la saggezza dei giapponesi che, nel tempio di Nikke, hanno eretto otto colonne decorate con motivi simmetrici, solo che l'ottava è a testa in giù. Un errore grossolano degli architetti? Tutt'altro. Du Sautoy cita un testo giapponese del XIV secolo: \"In ogni cosa l'uniformità non è desiderabile. Lasciare una cosa incompleta la rende interessante, e dà la sensazione che ci sia posto per crescere\".  Questa frase mi ha fatto pensare a una delle mie manie scaramantiche: quando vado in un posto nuovo, lascio sempre qualcosa da vedere per la volta successiva, che sia un museo o una piazza o un monumento. Mi serve per non soffrire di nostalgia e per avere una buona scusa per dirmi: \"Vabbé, ci tornerò\". Quello che non è compiuto è ancora aperto, ovvero psicologia spicciola dell'incompletezza Non vi racconto altro, tanto non sono capace. Vi allego (in inglese) il testo della conferenza di du Sautoy, messa a disposizione del pubblico e dei media. Chi è curioso può cliccarci su e scaricherà un file pdf. Con questo  chiudo la mia trasferta barcellonese e anche i post off topic! symmetrynew.pdf"},
{"title": "Per l'Alzheimer speranze precoci", "text": "Sono da quaalche giorno in vacanza in Francia, dove i giornali hanno dato molto risalto alla notizia del successo ottenuto da un nuovo farmaco contro la malattia di Alzheimer, il Rember. L'annuncio di questa nuova sostanza è stato dato da un gruppo di ricercatori dell'Università di Aberdeen al Congresso internazionale per la malattia di Alzheimer ed è stato ripreso immediatamente dalla stampa britannica, quindi dalla maggior parte della stampa laica anche italiana (qui trovate l'articolo originale uscito sul Times di Londra). Come sempre in questi casi, i malati si sono scatenati e chiedono di poter entrare a far parte del gruppo di fortunati che parteciperà alla fase 3 della messa a punto della nuova cura. In effetti, i dati annunciati sono esaltanti: secondo uno dei ricercatori rallenterebbe \"fino all'81 per cento\" la progressione della malattia e sarebbe \"il doppio più efficace\" del miglior farmaco oggi in commercio. Anche il meccanismo d'azione è innovativo: è in grado infatti di \"sciogliere\" letteralmente i depositi di proteina tau che sono ritenuti una delle cause della malattia. \"È la più grande scoperta su questa malattia dopo quella di Alois Alzheimer\" ha dichiarato sempre lo scopritore con innegabile modestia. Spero davvero che questo farmaco mantenga le promesse e che sia davvero la svolta per questa terribile malattia, ma come sempre resto perplessa quando si fanno annunci tanto eclatanti dopo uno studio su 321 soggetti. Sì, avete letto bene, solo 321. In effetti sono un numero congruo per un farmaco fermo alla fase 2 della ricerca clinica, cioè a una fase relativamente precoce, la cui funzione dovrebbe essere quella di studiarne la farmacocinetica e il dosaggio, non di valutarne l'efficacia se non come analisi secondaria. Cosa significhi poi un miglioramento \"fino all'81 per cento\" delle prestazioni ccognitive non è chiarissimo: tanta precisione ha spesso un secondo fine, purtroppo. In questo caso come in altri, l'entrata in commercio di un nuovo farmaco (tanto più se utile per curare una malattia grave o senza speranza) è anticipato in modo acritico dalla stampa laica, senza attendere che i dati siano pubblicati su una rivista peer reviewed. La ragione è semplice: sviluppare una nuova molecola costa tanto, e così i ricercatori attirano gli investitori. E non solo: un farmaco già pubblicizzato come \"miracoloso\" venderà tantissimo non appena sarà sul mercato (non prima del 2012, a quanto pare). E se nelle prossime fasi di ricerca si manifestassero effetti collaterali importanti? C'è qualche articolo che ha denunciato questa possibilità? Oppure se, più banalmente, il tasso di efficacia scendesse? Cosa ricorderanno i pazienti, secondo voi? Gli annunci pieni di speranza di questi giorni o i più modesti risultati pubblicati su una rivista scientifica?"},
{"title": "La paura della Fine del mondo", "text": "No, non sto parlando di una strana setta millenarista, né di un nuovo studio di archeopsicologia del Medioevo, bensì dell'articolo uscito oggi in prima pagina su Repubblica, e di cui ha parlato anche qualcuno nei commenti al blog di Marco Cattaneo, in cui si raccontano i timori di un gruppo di fisici americani sull'ormai prossimo esperimento del Large Hadron Collider di Ginevra. Secondo questi fisici, e secondo il gruppo LHC Defense che lo sostiene, l'energia sviluppata dall'esperimento potrebbe provocare la creazione di un grande buco nero che potrebbe inghiottirci tutti. Non solo: dall'autoscontro tra particelle di grande massa potrebbero venir fuori, sempre secondo loro, altre particelle dal comportamento ignoto, chiamate \"strangelets\", simili a quelle che si trovano al centro delle stelle. Questa della fine del mondo provocata dalla mano dell'uomo non è una novità nella storia della psicologia sociale: il timore dello \"scienziato pazzo\" che gioca con la materia ha radici antiche (gli alchimisti furono bruciati sui roghi), per non parlare del timore che suscita un'esperimento che si propone di \"simulare l'origine del mondo\", così come è stato descritto al volgo il progetto dell'LHC. E poiché siamo comunque tutti figli della cultura giudaico-cristiana, sappiamo cosa accade quando l'uomo pretende di farsi dio! Non esiste setta che non abbia fatto leva sulla paura dell'annientamento del genere umano o del suo habitat per spingere gli individui più labili psicologicamente a fare ciò che il guru di turno voleva facessero, compresi gesti estremi come i suicidi di massa. Senza andare così lontano, comunque, non si può scindere la paura irrazionale verso i progressi della scienza che abita una parte importante della popolazione degli Stati Uniti d'America dalla sempre maggiore diffusione del protestantesimo creazionista. Un sondaggio effettuato da America On Line rivela che il 49 per cento degli americani ritiene gli esperimenti dell'LHC potenzialmente pericolosi per l'esistenza stessa del mondo (su che informazioni scientifiche si fondi questo giudizio non è dato sapere). Ma c'è un altro aspetto su cui sarebbe interessante discutere, ovvero quello della percezione del rischio, argomento assai dibattuto, specie tra i medici. In un articolo sul New Yorker pubblicato nel maggio del 2007, una giornalista intervistava Jos Engelen, il responsabile scientifico del CERN, che così si esprimeva sui timori di chi vuole bloccare l'esperimento dell'LHC: \"In meccanica quantistica esiste la probabilità che questa penna cada attraverso il tavolo. Improvvisamente si troverebbe per terra. Poiché la penna può comportarsi come un'onda, può attraversare il tavolo: chiamiamo questo 'effetto tunnel'. Se si calcola la probabilità che ciò accada, non è uguale a zero. È una probabilità molto piccola. Ma non succede mai. Non l'ho mai visto succedere. Lei non l'ha mai visto succedere. Ma per il pubblico generale, se fai un'osservazione casuale e dici 'non è identica a zero, è molto piccola'...\". Engelen prosegue dicendo che i fisici del CERN sono stati istruiti a dire che il rischio è pari a zero, anche se in realtà il numero è molto piccolo ma NON È zero. Il problema è che per noi tutto ciò che è possibile è reale, non importa quanto grande sia questa possibilità. Quindi sbrigatevi a dire la vostra sulle paure che vi animano, perché abbiamo solo 10 giorni e non sono sicura che il mio collegamento Internet continui a funzionare anche in un grande buco di antimateria (già funziona così così in questo mondo molto materiale   )."},
{"title": "Un testimone inaffidabile", "text": "Una ragazza di 19 anni, che chiameremo Lucia, viene derubata del suo portafoglio, nel quale tiene poche lire e una tessera del cinema con relativa foto, peraltro piuttosto datata. Non ritenendo il fatto importante, decide di non sporgere denuncia Sei mesi dopo viene convocata in commissariato, dove viene accusata di aver rubato in un grande magazzino: una sorvegliante ha sentito armeggiare dentro un camerino, ha fermato una ragazza che però è scappata lasciando a terra un portafoglio, con dentro una tessera del cinema. La sorvegliante ha riconosciuto con sicurezza la persona sulla fototessera: è proprio la ragazza che stava rubando. La denuncia fa il suo corso, ed è così che Lucia si ritrova in tribunale. Assistita da un avvocato, porta in aula diversi testimoni a suo favore, che dichiarano che lei si trovava altrove al momento del fattaccio. Alla prima udienza la sorvegliante del negozio non si presenta, il pubblico ministero chiede l'archiviazione per un palese errore di persona, ma il giudice dichiara che, poiché c'è stato un formale riconoscimento, seppure in fotografia, la sorvegliante deve venire in tribunale a disconoscere Lucia. Passa un anno (dal momento del furto ne sono passati più di due, sono i tempi della giustizia italiana): nuova udienza e colpo di scena. La sorvegliante, in aula, conferma il riconoscimento. Secondo lei, la ragazza che stava rubando nel grande magazzino era proprio Lucia. Le testimonianze a favore di Lucia, a questo punto, valgono carta straccia: arriva una condanna per furto con taccheggio, per fortuna \"assorbita\" dalla condizionale. Ora, posso assicurarvi che Lucia non era colpevole e che il riconoscimento è fasullo. Peraltro, a distanza di due anni da un evento tutto sommato banale per una sorvegliante di grande magazzino, e dopo che quest'ultima aveva tenuto in mano per diverso tempo la fototessera di Lucia, imprimendosi ben bene il suo volto nella memoria (mentre aveva appena intravvisto la ladra che scappava), la testimonianza oculare, da un punto di vista strettamente scientifico, non vale un fico secco. E se posso assicurarvi che Lucia era davvero estranea alla faccenda, è perché Lucia sono io. Da allora (sono passati 20 anni) ogni volta che leggo di condanne basate sulla parola di un testimone oculare, mi tornano in mente brutti ricordi. Già, perché gli psicologi che studiano i meccanismi di percezione visiva e di memoria sanno benissimo che il testimone oculare è spesso inattendibile. Lo conferma un numero monografico della rivista  Applied Cognitive Psychology appena uscito, che tenta di fare il punto sull'annosa questione, nella speranza che anche chi amministra la giustizia abbia una formazione sufficiente a discriminare la testimonianza credibile da quella che non lo è. Il che non è semplice, perché il testimone è in perfetta buona fede anche quando sbaglia (e io l'ho provato sulla mia pelle!). Esattamente cento anni fa lo psicologo tedesco-americano Hugo Münsterberg pubblicava \"On the witness stand\", il primo tentativo di inquadrare dal punto di vista scientifico il problema della testimonianza in tribunale. Potete leggere qui per intero il suo testo, peraltro controverso. I punti critici della testimonianza, però, ci sono già tutti, e ben chiari: i limiti della nostra capacità percettiva, la tendenza del nostro cervello a \"uniformare\" i ricordi (il che consente una sorta di risparmio del magazzino mnesico), la creazione di false memorie per non parlare della peculiarità dei meccanismi di riconoscimento dei volti umani. Nel mio caso, per esempio, sarebbe bastato un semplice test per dimostrare al giudice che chiunque tenga in mano a lungo una fotografia finisce per sovrapporre quel volto a quello reale, tanto più se quest'ultimo è stato visto per pochissimi secondi e in una situazione emotivamente intensa. All'epoca queste cose non le sapevo, e peraltro il giudice non intendeva certo perdere tempo con un caso così banale, tant'è che rifiutò persino di sentire altri testimoni a mio favore ed emise subito la sentenza. Anche questo è il frutto di un pregiudizio duro a morire: quello per cui ciò che vediamo è anche ciò che ricorderemo. Credo che la ragione per cui le giurie danno tanto peso alle testimonianze oculari, anche a fronte di altri elementi probanti, è la difficoltà di ammettere che ciò che ricordiamo della nostra vita è una sorta di ricostruzione a posteriori, distorta dalle emozioni e dalle esperienze precedenti e successive. PS: Morale della storia: precipitatevi a fare denuncia di furto, se in ciò che vi è stato rubato c'è un documento con i vostri dati o una vostra fotografia, anche se non è ufficiale!"},
{"title": "L'analisi vista dal buco della serratura", "text": "Non abbiamo mai chiacchierato di psicoanalisi in questo blog, se non di sfuggita. La messa in onda sul canale Cult di Sky della serie \"In treatment\" mi dà l'occasione di parlarne. Per chi non ha l'abbonamento a Sky o non ha visto le prime due serate, vale la pena di descrivere questa produzione televisiva veramente fuori dagli schemi.  Ogni giorno della settimana è dedicato a un paziente del dottor Paul Weston, interpretato da un bravissimo Gabriel Byrne: l'ambientazione è lo studio di Weston, arredato come un salotto di una casa borghese americana. Al termine della settimana, il venerdì, è Weston a diventare paziente della propria analista e supervisore, interpretata da Dianne West. Dimenticate i pazienti nevrotici alla Woody Allen e gli analisti afasici della miglior tradizione freudiana cinematografica, eliminate dall'immaginario anche il vetusto lettino: l'analista oggi è il confessore della società moderna, è colui che aiuta a trovare le ragioni profonde del proprio agire. \"In treatment\" è il remake di Be'tipul, una serie israeliana ideata da Hagai Levi, che fa parte anche del team autorale della versione made in USA. Be'tipul ha suscitato molto dibattito in Israele, perché i personaggi che arrivavano sul divano dell'analista per riversare le loro storie erano lo specchio di una società tormentata e in difficoltà quale è quella israeliana attuale, divisa tra memorie difficili da sepellire e un futuro tutto da ripensare, su cui pesano anche i sensi di colpa individuali (di chi ha tradito le grandi aspettative ideologiche della generazione dei fondatori) e collettivi. Della versione USA ho visto solo i primi due episodi, ma già la figura di Alex, top gun che cerca di dare un senso alla sua vita dopo aver sganciato sull'Irak una bomba che ha fatto 16 vittime tra i bambini di una scuola mi pare ricalcare lo spirito della serie originale. Per chi, come me, ha fatto l'esperienza dell'analisi e pensa che costituisca un formidabile strumento di autocoscienza, la serie è persino commovente, perché vi si ritrovano i meccanismi tipici di tutte le analisi, la vicinanza-lontananza con l'analista, il transfert, le barriere, la voglia di scappare che talvolta coglie chi si avvicina pericolosamente a qualcosa di doloroso che si preferirebbe rimuovere. Agli altri, la serie può apparire claustrofobica, basata com'è sull'unità di luogo e di tempo, come nella migliore tradizione teatrale. Ed è la prima volta, per quanto io ricordo, che la televisione o il cinema restituiscono alla psicoanalisi la sua natura di \"percorso dell'anima\" e non di macchietta. Niente a che vedere con le manie della New York anni '80, con le fisime intellettualoidi, ma solo un aiuto per superare la solitudine, la depressione e il dolore. E l'analista non è un superuomo, solo un essere dotato di alcuni strumenti di comprensione della psiche umana. Attendo con ansia le prossime puntate."},
{"title": "Nuove esperienze, nuovi neuroni", "text": "Da oltre dieci giorni sono in Germania, al Max Planck Institute for Biophysical Chemistry di Goettingen: sono state giornate intense di sperimentazioni e di incontri con scienziati che si occupano, a vario titolo, di neuroscienze. Spero di aver modo di condividere quanto prima l'incredibile esperienza che sto vivendo: vi basti sapere che in questa cittadina minuscola, famosa per la sua università e per la presenza di diverse sedi dell'Istituto Max Planck, vivono attualmente ben 16 premi Nobel, soprattutto nell'ambito della fisica e della chimica, ma anche un paio di Nobel per la medicina. È facile incontrarli il sabato al brunch della pasticceria Cron und Lanz. Uno di loro, il chimico Manfred Eigen, un vecchio signore dall'aria austera, si è semplicemente seduto al nostro tavolo, incuriosito dal fatto che quattro giovani discutessero animatamente dell'apporto reciproco di genetica e ambiente alla formazione dell'intelligenza di un individuo. Il racconto della mia esperienza qui sarebbe off topic, e dovrebbe stare più appropriatamente nel blog di Marco Cattaneo: contrariamente a quanto mi capita di percepire in Italia, la ricerca è presa sul serio ed è finanziata adeguatamente (loro si lamentano, ovviamente, ma basta uno sguardo ai loro laboratori - per non parlare delle loro fantastiche guest house   - per capire che siamo anni luce dall'Italia). Gli studenti che sono qui per il PhD o per un postdoc sono al 60 per cento stranieri (sì, avete letto bene, sei su dieci non sono tedeschi e la scelta avviene solo sulla base del curriculum). Ovviamente abbondano gli italiani, ma anche i greci e i russi. La discussione alla quale ha voluto partecipare anche il nostro eminente vicino è partita da un'interessante seminario tenuto presso il Dipartimento di dinamiche non lineari da una ricercatrice recentemente passata da Princeton ad Harvard, Yevgenia Kozorovitskiy. Obiettivo del suo lavoro: dimostrare, a livello cellulare e molecolare, i cambiamenti che l'esperienza provoca sulla struttura cerebrale. Si sa, infatti, già dagli anni Sessanta, che l'esperienza, intesa come contatto con l'ambiente e successivo apprendimento di nuovi comportamenti, induce un aumento della lunghezza e della complessità dei dendriti, quei piccoli filamenti che nelle cellule nervose consentono di creare una fitta rete di comunicazione. Si sa anche che la stimolazione ambientale aumenta le sinapsi, ovvero le parti attive dei dendriti stessi, in pratica le \"centraline\" della trasmissione nervosa tra le cellule. Da qualche anno, però, contrariamente a quanto molti credono, si è scoperto che l'esperienza aumenta anche il numero dei neuroni, soprattutto nelle aree coinvolte dai fenomeni di apprendimento, come l'ippocampo. La Kozorovitskiy è riuscita a dimostrarlo nei primati, sfruttando le proprietà della microscopia a fluorescenza: in pratica, ha verificato che nel cervello dei uistitì maschi (una piccola specie di scimmie), la paternità provoca non solo un aumento dei dendriti e delle sinapsi, ma anche del numero degli assoni (ovvero della parte terminale della cellula nervosa) e dei neuroni nell'ippocampo e nella corteccia prefrontale, che ha un ruolo importante nella programmazione di nuovi movimenti. Tra queste scimmie sono i padri a curare la prole, a causa del peso dei neonati: la paternità è quindi una fase di intenso apprendimento di nuovi comportamenti, un po' come accade, peraltro, anche a noi umani. Lo stesso aumento del numero di neuroni si verifica in animali che vivono in gabbie in cui l'ambiente è ricco e stimolante, mentre non si verifica negli esemplari che vivono in gabbie molto semplici. Il fatto che l'ambiente, e le esperienze che viviamo, modulino la plasticità cerebrale, ovvero siano in grado di creare nuovo tessuto cerebrale, è una scoperta importante, soprattutto perché dimostra non solo l'utilità dell'educazione e della stimolazione (tra l'altro il fenomeno è particolarmente accentuato negli esemplari giovani, mentre tende a decrescere con l'età), ma dà anche una base fisiologica alla neuroriabilitazione: se possiamo indurre la crescita di nuovi neuroni, allora ha un senso  impiegare tempo e risorse a riabilitare chi è stato vittima di un ictus e forse anche chi è affetto da forme di demenza. L'importanza di un ambiente complesso e stimolante per mantenere attivi i meccanismi di apprendimento e di attenzione potrebbe mettere in crisi il modello delle residenze per Alzheimer, dove le stanze sono tutte uguali e gli stimoli ridotti. Certo, i dati sono sperimentali e per ora limitati a scimmie e roditori, ma mi pare un filone da seguire con attenzione."},
{"title": "Un pi greco nel cervello", "text": "Quando mi hanno proposto di passare qualche giorno al Max Planck Institute for Dynamics and Self-organization per lavorare insieme ai neurofisici, ho pensato che chi stava organizzando la mia permanenza qui a Goettingen fosse pazzo: non solo ho sempre fatto fatica ad avere voti decenti in matematica (ebbene, sì...) ma le mie conoscenze di fisica si fermano all'esame previsto nel corso di medicina e dalle nubi generali emergono solo alcuni isole (tipo il teorema di Bernoulli, chissà perché). E invece è stato illuminante: anche se non sono in grado di leggere le equazioni che i fisici applicano allo studio dei meccanismi neuronali, i concetti che stanno alla base di questo nuovo filone di indagine sono \"metaforicamente\" comprensibili. Mi hanno spiegato, per esempio, che tutto quanto conosciamo sulla trasmissione nervosa è dedotto da modelli sperimentali che non mantengono la complessità del sistema: un conto è un neurone che comunica con un altro neurone in una provetta, un altro è capire come fa uno dei miliardi di neuroni che abbiamo nel nostro cervello a \"parlare\" con un altro neurone, tenendo conto che, se si mettono in fila tutti i nostri assoni (ovvero la parte che funge da \"filo elettrico\" nella conduzione del segnale nervoso) si ottiene una lunghezza simile a quella che separa la Terra dalla Luna. In pratica quello che fanno i neurofisici è dare i numeri (in senso letterale, ovviamente!): misurano, grazie a simulazioni basate su modelli matematici che tengono conto della teoria del caos e delle dinamiche non lineari che governano la trasmissione nervosa, i diversi parametri e cercano di definire un modello di network compatibile con i dati sperimentali ottenuti dalle neuroscienze cognitive e dalla neurofisiologia. Tanto per raccontarvi un aneddoto un po' mistico, oggi ho fatto una lunga chiacchierata con Fred Wolf, il responsabile di un gruppo di ricerca che si dedica allo studio della corteccia visiva. Come è noto, la corteccia visiva ha al suo interno una sorta di mappa che risponde agli stimoli esterni: ciò è facilmente visibile usando una tecnica di neuroimaging che visualizza il momento in cui il neurone \"scarica\" il suo potenziale d'azione misurando il calcio intracellulare con la microscopia a due fotoni. La tecnica si può applicare in vivo sulle scimmie: le aree attivate diventano scure e si può quindi seguire, come in un film, il reclutamento successivo dei neuroni. Se davanti agli occhi del macaco si visualizza una girandola luminosa, la corteccia mostrerà una girandola analoga, ma scura. Ciò significa, in pratica, che il nostro cervello sta \"mappando\" lo stimolo visivo. Questo meccanismo si riproduce a vari livelli: si vede sull'insieme della corteccia visiva, ma si riconosce anche se si selezionano le singole colonne di neuroni che ne costituiscono la struttura di base. L'attivazione \"rotatoria\" dei neuroni avviene intorno a un punto fisso, come il centro della ruota di un carretto: questo punto viene chiamato, con un termine inglese, pinwheel (vedete una mappa a colori della corteccia visiva qui di fianco, la freccia indica il pinwheel). I neuroni si attivano in cluster intorno a un certo numero di pinwheels per rispondere ai diversi stimoli. Per studiare la formazione dei cluster i neurofisici hanno usato la teoria degli attrattori instabili (quelli di Lorenz e del battito d'ali della farfalla) ma magari di questo parliamo un'altra volta. Wolf e colleghi si sono però chiesti se era possibile misurare la densità di questi pinwheels nella corteccia, e lo hanno fatto in diverse specie di carnivori, primati e subprimati. La densità dei pinwheels per colonna è diversa lungo la scala evolutiva ma ha comunque una distribuzione lineare. Quindi, mi spiegava Wolf, hanno deciso di calcolare la densità media per colonna attraverso tutte le specie e il numero che venuto fuori è una costante ben nota: 3,14, ovvero pi greco. E io sono rimasta a bocca aperta davanti allo schermo del computer."},
{"title": "Uomini e topi", "text": "Non sono una sostenitrice a spada tratta delle sperimentazioni su animali, ma sono convinta che talvolta non vi siano alternative. Ne ho avuto conferma qui in Germania visitando l'Istituto di medicina sperimentale, dove un gruppo di neurologi sta tentando di risolvere alcune gravi patologie neurologiche, come la sclerosi multipla, e di comprendere meglio quelle psichiatriche, come l'autismo e la schizofrenia. Michael Sereda, con cui ho lavorato ieri, è un neurologo specializzato in malattie demielinizzanti (quelle in cui l'assone perde il suo rivestimento protettivo, ovvero la mielina). È riuscito a isolare una forma monogenica (cioè provocata da un solo gene) di una rara malattia demielinizzante, la sindrome di Charcot-Marie-Tooth, che può dare informazioni importanti anche su patologie molto più frequenti come la sclerosi multipla. Nello stesso istituto c'è una \"animal house\", ovvero una sorta di \"fabbrica\" di animali transgenici. Sono andata a vederla: vi sono circa 27.000 topi mutati che fanno da modello per oltre 500 malattie umane. Ogni volta che uno scienziato ha bisogno di un modello animale della malattia che deve studiare, ovviamente a patto che abbia trovato il gene responsabile, un gruppo di genetisti manipola il DNA del topo e lo fa riprodurre (con tecniche di inseminazione in vitro). Vi sono malattie provocate dalla mancata attività di un gene, e in quel caso lo si elimina dal genoma del topo, oppure dall'eccessiva attività, allora si possono inserire più copie dello stesso gene in modo che lavori di più. Grazie ai modelli animali, questi ricercatori hanno identificato alcuni meccanismi biochimici alla base della perdita di mielina, ma hanno soprattutto scoperto che anche se non si può indurre la mielina a riformarsi, si possono proteggere le cellule nervose con alcuni farmaci (per esempio hanno provato, sempre sui topi, un ormone femminile, il progesterone, e ha funzionato, consentendo un parziale recupero della funzione motoria nei topi che l'avevano persa). Ora partirà finalmente anche una sperimentazione sull'uomo, su quelle stesse famiglie che hanno contribuito fin dall'inizio, accettando di far studiare il proprio genoma, alla messa a punto di una possibile cura. Questo per quanto riguarda la neurologia: è però ancora più interessante scoprire che è possibile creare un modello genetico di malattie psichiatriche. Per ora esiste un modello attendibile di topo autistico (ho visto con i miei occhi questi topi aggirarsi nelle gabbiette tenendosi a distanza dai propri simili e avvicinarsi, invece, agli oggetti inanimati, proprio come fanno i bambini autistici). L'autismo sembrerebbe, secondo i risultati di questi studi diretti da Nils Brose, appartenere a una nuova categoria di malattie, le \"sinaptopatie\", ovvero alterazioni delle sinapsi, i bottoni che consentono la comunicazione tra le cellule nervose. Certo, in questo caso è coinvolto un gene piuttosto raro, responsabile solo di una piccola parte dei casi di autismo nell'uomo, ma si tratta di una scoperta importante, perché spiega come mai le tecniche di neuroimaging non notano alcuna alterazione organica negli autistici (mentre si notano alterazioni agli esami funzionali, come la PET o la risonanza magnetica funzionale). Il fatto che alla base della malattia vi sia un difetto biochimico spiegherebbe anche come mai l'autismo si presenta con diversi stadi di gravità anche in presenza di una stessa alterazione genetica: si sa, infatti, che l'attività di un gene è modulabile e che quindi è possibile avere proteine diverse da uno stesso gene, con effetti diversi sui sintomi. In sostanza alcune malattie psichiatriche (compresa la schizofrenia, di cui qui stanno cercando le origini genetiche e che pare essere anch'essa una sinaptopatia) potrebbero avere alla base un'alterazione biochimica che solo oggi si riesce a studiare, anche grazie al modello murino. Tra l'altro, è anche possibile fare una risonanza magnetica a un topo: l'ho fatta io stessa, qui a Goettingen, e devo dire che il topino si è ripreso in fretta malgrado la mia inesperienza. È stato anestetizzato, intubato e ventilato (non avete idea di come sia difficile intubare un topino di pochi centimetri, la cui laringe appare non più grossa di una capocchia di spillo!): abbiamo potuto studiare la morfologia del suo cervello e quando l'anestesia ha smesso di fare effetto abbiamo verificato che potesse respirare da solo e l'abbiamo rimesso nella sua gabbia. Da quando sono qui non ho mai visto nessuno comportarsi in modo crudele verso gli animali, anzi. Tutto è strettamente regolato da norme rigide e in tutti i laboratori che usano topi ho notato una sorta di \"affetto\" nei confronti degli animali coinvolti negli studi. Un aneddotto curioso me lo ha raccontato il veterinario responsabile dell'animal house. Quando hanno iniziato a produrre topi con la fecondazione in vitro, i tassi di gravidanze erano molto bassi, benché gli embrioni fossero a posto e gli ormoni delle topine artificialmente portati ai livelli utili per la gestazione. Poi hanno capito: perché si instauri una gravidanza, le topine devono \"fare sesso\" con i topi, altrimenti non riescono a rimanere incinte. Ora fanno così: mettono insieme in una gabbia la futura mamma e un topo vasectomizzato (perché a loro servono topi dal \"curriculum genetico\" controllato). Solo dopo che i due si sono divertiti un po', allora impiantano l'embrione e la gravidanza prosegue felicemente. Suvvia, anche i topi hanno una psiche..."},
{"title": "Un premio alla mente materiale", "text": "La Norvegia è sempre stata un tantino gelosa della Svezia a causa del premio Nobel. Benché, per volontà stessa di Alfred Nobel, il premio per la Pace venga deciso da un comitato norvegese e consegnato a Oslo, nel campo delle scienze il primato di Stoccolma restava ovviamente imbattuto. Da quest'anno, però, un ricco fisico e uomo d'affari di origine norvegese emigrato negli Stati Uniti, Fred Kavli, ci ha messo una pezza indicendo il Premio Kavli per l'astrofisca, le nanoscienze e le neuroscienze.  Il monte premi è allettante (un milione di euro, più o meno l'equivalente del Nobel) e il comitato di saggi che decide il vincitore è di tutto rispetto (per le neuroscienze sono in cinque, e tra questi c'è anche il grande Eric Kandel). Nell'ambito delle neuroscienze, hanno dovuto dividersi il malloppo (e l'onore di essere i primi) tre grandi ricercatori: Sten Grillner, dell'Istituto Karolinska di Stoccolma, Thomas Jessel, della Columbia University e Pasko Rakic, neurobiologo a Yale. Tutti e tre hanno contribuito a svelare i meccanismi con cui si instaura il circuito senso-motorio che ci consente di muoverci. Non solo: hanno dimostrato che il network che controlla i movimenti è presente, con una struttura pressoché analoga, a tutti i livelli della scala evolutiva. Con altri lavori, Rakic ha chiarito in che modo i neuroni si differenziano a partire dalla stessa cellula staminale e come si orientano per costituire le colonne della corteccia cerebrale, considerate \"i mattoni della coscienza\" (così è scritto nella motivazione del premio). Tutto ciò mi pare molto significativo: il focus delle neuroscienze, fino a non molti anni fa incentrato sui macrosistemi che governano il pensiero (mappe cerebrali, aree di attivazione, neuroimaging) sta spostandosi sempre più sui meccanismi di base. È a livello biochimico che, ci dicono questi scienziati, troveremo la chiave per capire perché noi esseri umani siamo quel che siamo e perché e come pensiamo. In sostanza, è una sorta di rivoluzione copernicana: la neuropsicologia classica cede le armi alla chimica, alla biologia e alla fisica, nel tentativo di disegnare una filosofia della mente \"basata sulla materia\". PS: ultimamente da queste parti si parla molto di neuroscienze e pochissimo di psiche, complice il mio viaggio nelle terre germaniche. Alcuni amici mi hanno fatto notare che sono questioni interessanti da leggere, ma che è più semplice intervenire su argomenti più vicini al quotidiano e quindi mi hanno suggerito di ritornare alla \"vita vissuta\". Ora che ho (purtroppo!) ritrovato la mia solita scrivania e ho abbandonato i laboratori del Max Planck conto di recuperare a breve il mio solito umore \"fluttuante\" e il relativo interesse per i moti dell'animo   ."},
{"title": "Quando torna la difesa della razza", "text": "Venerdì scorso, in occasione della partita Italia-Romania, Radio Popolare di Milano ha organizzato la sua trasmissione di commento dal campo nomadi di Triboniano, uno dei più grandi della città. All'evento hanno partecipato anche cento famiglie di ascoltatori, ospitate da altrettante famiglie rom. Con mio marito abbiamo discusso a lungo dell'opportunità di andare anche noi, con le nostre due figlie di 5 e 3 anni. Mi sembrava un gesto doveroso, visti i tempi che corrono e il crescente razzismo (non so come altro chiamare la schedatura di cittadini italiani di origine rom a cui abbiamo assistito a Milano, uno dei quali persino reduce dai campi di sterminio nazisti e medaglio d'oro al Valor Civile). Dopo averci pensato un po' abbiamo concluso, con dispiacere, che per le nostre figlie, poco abituate all'impatto con la povertà ma anche, molto banalmente, con un ambiente non protetto (nel senso proprio materiale del termine) forse sarebbe stato un po' troppo, e anche per noi, che ci saremmo sentiti in dovere di controllare che non si ficcassero a giocare tra le lamiere arrugginite invece di seguire la partita con i nostri ospiti. Una vicenda molto privata, che però, unità alla lettura dei giornali, ai progetti di legge per inviare l'esercito nelle città a garantire \"l'ordine pubblico\", alle notizie riportate ieri da Metropoli - il supplemento sull'immigrazione che esce ogni domenica con Repubblica (retate negli autobus milanesi per controllare i documenti, ma solo a coloro che hanno la faccia da \"straniero\", permessi per ricongiungimenti famigliari bloccati improvvisamente alla vigilia dell'arrivo di una moglie o di un marito lasciati in patria da anni, revisione del diritto di asilo politico) - mi fanno tremare le vene dei polsi. Mi chiedo se la nostra società è abbastanza immunizzata dal virus dell'intolleranza, se chi ci governa ha qualche nozione di psicologia sociale (in particolare del rischio che corre la democrazia quando si solleticano certi istinti). Sempre su Metropoli, una signora francese racconta di aver tentato di separare alcuni bulletti quindicenni che tormentavano un compagno e di essersi sentita dire da uno di loro: \"vattene via, che io sono italiano e le leggi le faccio io\". Ecco, quando leggo queste cose, mi pento di non aver superato i miei timori e di aver voluto proteggere le mie figlie da un impatto troppo brutale con \"il diverso\", perché non so bene come si può tenere i nostri figli nella bambagia e nello stesso tempo farne non solo degli esseri tolleranti, ma anche sensibili ai campanelli d'allarme dell'intolleranza altrui. Forse perché sono un'italiana di prima generazione (la mia famiglia è arrivata a Milano dal Medio Oriente negli anni Sessanta, mio nonno è stato apolide fino al 1975), forse perché appartengo a una minoranza religiosa che ne ha viste delle belle anche in questo Paese, non riesco a non avere paura della paura altrui, e ancor più di chi la cavalca per i propri interessi. C'è, nel clima di caccia allo straniero che stiamo vivendo, qualcosa di profondamente primitivo: secondo alcune teorie della psicologia sociale, l'odio verso il diverso sarebbe stato un meccanismo di protezione evolutivamente utile per la conservazione del patrimonio genetico all'interno dei primi gruppi sociali umani, estremamente aggressivi gli uni contro gli altri. Ma l'uomo, per fortuna, è il prodotto dell'interazione tra genetica e cultura (e la seconda dovrebbe avere il ruolo di mitigare la prima) altrimenti l'istinto di conservazione ci porterebbe a speronare chiunque ci sorpassi pericolosamente sull'autostrada. Questo vale anche per gli effetti di eventuali geni \"patologici\" come quello dell'iperattività di cui dà notizia nel suo blog Giovanni Spataro e che sarebbe stato identificato in alcune popolazioni nomadi del Kenya. Come sanno le insegnanti che lavorano con i bambini rom, tenerli fermi seduti al banco non è facile, ma basta trovare una modalità didattica più vicina alla loro tradizione orale e nomade (e stiamo parlando di cultura, non di genetica) per ottenere ottimi risultati. Cosa c'entra tutto ciò con la scienza? C'entra, e se qualcuno non se lo ricorda dovrebbe leggere il bel libro di Valentina Pisanty \"Educare all'odio: La Difesa della razza (1938-1943)\", Federico Motta editore. Vi si ricorda come la rivista omonima, nata sotto gli auspici del ministero della Cultura popolare di Mussolini, fosse finanziata da grandi industrie, banche e assicurazioni con lo scopo di dare una base pseudoscientifica al \"problema della razza\". Vi si faceva riferimento alla tassonomia della \"genetica\" e dell'antropologia, alla psicologia sociale delle \"razze\" (che sarebbero state portatrici di tratti caratteriali geneticamente determinati, e magari a qualcuno questo può ricordare le recenti affermazioni del premio Nobel James Watson circa le basi genetiche dell'intelligenza nella popolazione afro-americana). Concludo con le parole di Umberto Eco, nella prefazione del libro di cui sopra, che mi paiono quanto mai attuali: \"Come è possibile che queste cose siano state scritte, che molti le abbiano lette, che tantissimi le abbiano credute, che la maggioranza degli italiani le abbia ignorate, o tollerate, o lasciate passare come innocente esercizio filosofico e parascientifico?\"."},
{"title": "Perché piace tanto l'uomo infido", "text": "Permettetemi una digressione in stile “Sex and the city”, in questi tempi grami. Mi sono arrivati gli abstract della convention annuale della Human Behavior and Evolution Society che si è tenuta a Kyoto il mese scorso. Si tratta di un gruppo di biologi che studia gli aspetti emotivi, cognitivi e sessuali del comportamento umano alla luce della teoria dell’evoluzione. Ora, due gruppi diversi si sono chiesti come mai gli uomini più stronzi (scusate il termine poco elegante, ma non mi viene niente di più chiaro e il termine “cattivo”, traduzione dell’inglese “bad guy” usato dai ricercatori, in italiano ha tutt’altro significato) sono quelli che statisticamente hanno più donne. La personalità antisociale, impulsiva e narcisista (quella che gli psicologi chiamano “la triade oscura”) di alcuni uomini dovrebbe essere uno svantaggio evolutivo, e non un vantaggio! Invece un gruppo di psicologi della New Mexico State University ha stabilito che su 200 studenti universitari, quelli che hanno più rapporti sessuali e più partner sono quelli che hanno il profilo psicologico peggiore. Questo perché la triade oscura si accompagna a comportamenti compulsivi, al bisogno di cambiare continuamente compagna e all’incapacità di mantenere relazioni stabili.  Il colmo è che questa pare essere una costante transculturale, come dimostra un altro studio condotto dalla Bradley University, nell’Illinois, su 35.000 soggetti in 57 diversi Paesi. Conclusione: la triade oscura si accompagna a un discreto successo ripoduttivo e quindi questi tratti di personalità apparentemente negativi dal punto di vista evoluzionistico sono riusciti a rimanere presenti nel genere umano. Questo ovviamente perché il materiale genetico viene “depositato” in più partner femminili, aumentando le probabilità di una gravidanza. Ora, se i geni narcisisti ed egoisti fossero davvero tanto utili, tutti gli uomini apparterrebbero alla categoria dei “bad guys”. Se non è così, si sono detti gli evoluzionisti, è perché hanno anche degli effetti negativi: e la possibilità di avere molte donne funziona solo se gli infidi sono pochi, altrimenti il genere umano (specie quello femminile!) si farebbe più furbo e li isolerebbe. In questo ameno quadretto, però, mancano appunto le donne: nessuno spiega perché sono attratte così tanto da questi maschi tronfi e pieni di sé, e purtuttavia affascinanti (alzi la mano la fanciulla che non si è fatta incastrare almeno una volta nella vita!). Sarà, come dicono molte sociologhe femministe, la sindrome dell’ \"io ti salverò”: quale vittoria maggiore per una donna che trasformare lo scapolo impenitente in un fedele compagno di vita. Nessuno ha fatto uno studio, ma temo che i tassi di riuscita siano piuttosto bassi."},
{"title": "Una corte davvero Civile", "text": "La notizia è di un'oretta fa: la Corte d'Appello civile di Milano ha autorizzato i familiari di Eluana Englaro, la ragazza in stato vegetativo permenente da quando, nel 1992, è stata vittima di un incidente automobilistico, a sospendere l'idratazione e la nutrizione artificiale che la mantengono in vita. Di questa vicenda si è parlato molto su tutti i giornali italiani, spesso anche a sproposito. A me preme in questo momento ricordare la tenacia di un padre, Beppino Englaro, che non ha mai voluto lasciare la figlia in uno stato che egli riteneva non compatibile con quanto Eluana avrebbe voluto per se stessa. L'ho intervistato più volte e mi ha sempre colpito un'affermazione che fece anni fa: \"I medici mi hanno detto tante volte all'inizio di questa vicenda: ma se la riporti a casa e faccia quello che vuole, nessuno le negherà un certificato di morte per una qualche complicanza. Ma così non sarebbe dignitoso ed è un modo indegno di un Paese civile\". Oggi ha ribadito la sua tempra quando ha riposto ai giornalisti: \"Dolore? Mia figlia è morta 16 anni fa\". Nelle rianimazioni italiane (e non solo) si fa così, come hanno dimostrato diversi studi sull'eutanasia attiva e passiva: siamo il Paese dove la medicina, in mancanza di leggi chiare sul valore della volontà del singolo, si arrangia come può. Ora la sentenza di Milano risponde a due quesiti posti dalla precedente sentenza della Corte di Cassazione che, contrariamente a quanto alcuni giornali avevano riferito all'epoca, non aveva affatto rigettato la richiesta di interruzione dell'alimentazione, ma aveva invitato ad accertare l'irreversibilità dello stato vegetativo e la volontà espressa dalla paziente in vita. Persino il curatore esterno nominato dal Tribunale per accertare l'assenza di interessi personali nella decisione del padre si è pronunciato a favore della fine di questo incredibile calvario. È bastato confermare che a 16 anni dall'evento lo stato vegetativo permanente è effettivamente irreversibile e che le amiche ripetessero le precedenti testimonianze sulla volontà di Eluana di non essere mantenuta in vita in casi simili per ottenere una sentenza che è immediatamente applicabile, anche se appellabile (ci sono 60 giorni per farlo, ma i parenti di Eluana potrebbero interrompere anche subito le cure per prevenire gli effetti di un eventuale ricorso). La Corte d'Appello civile ha anche stabilito le norme attuative: l'interruzione delle cure deve avvenire in un luogo adatto (hospice od ospedale) e con l'utilizzo del supporto farmacologico necessario (sedativi, antiepilettici). Quale progresso rispetto ad altri casi di cronaca anche recenti... Mi pare un momento importante per la bioetica in Italia e anche per ognuno di noi: è un precedente legale che riconosce il valore della volontà anticipata in materia di cure (e questo malgrado il progetto di legge sul testamento biologico giaccia da svariate legislature in Parlamento). Dall'altro riconosce quanto gran parte dei neurologi afferma da tempo, ovvero che la morte di un individuo non corrisponde alla cessazione del battito cardiaco, ma alla morte cerebrale e che, più in particolare, non è necessario che sia l'intero encefalo a essere danneggiato, ma basta che lo sia la corteccia, la parte senza la quale noi non siamo ciò che siamo, né siamo dotati di alcuna volontà, capacità di percepire il mondo esterno e gli stimoli provenienti dal nostro stesso organismo. Lo stato vegetativo permanente spesso coincide con la morte corticale e forse, tra qualche anno, con la morte tout court."},
{"title": "ESOF 2008, il Forum della scienza europea (1)", "text": "Questo e altri post che seguiranno nei prossimi giorni potrebbero essere off topic, ma mi pare interessante raccontarvi il vario mondo che si incontra qui a Barcellona in occasione di ESOF 2008, un evento promosso da Euroscience Open Forum, una piattaforma europea nata per promuovere la comunicazione tra scienziati, giornalisti, politici, industria e societa' civile sui temi scottanti della scienza e della pianificazione della ricerca. Tutto e' molto imponente, qui: migliaia di partecipanti (circa 3500) da tutto il mondo (spiccano i giapponesi, benche' si tratti di un'iniziativa europea), un programma scientifico di tutto rispetto incentrato su alcuni temi di punta: mente e coscienza, lo studio della materia, il cibo e la scienza (altro che Expo di Milano!), la comunicazione scientifica e il suo ruolo sociale, il ruolo dell'arte tra scienza e societa'. Seguire tutto e' impossibile e invidio sempre i miei colleghi giornalisti \"veri\" che si accontentano di passare 10 minuti in ogni sessione e approfittano poi delle conferenze stampa per strappare al relatore quattro dichiarazioni e scrivere un pezzo. Io, a dire il vero, proprio non sono capace. Ci ho provato: ma quando un argomento mi interessa, resto fino alla fine, e mi perdo tutto quanto si svolge nelle sale collaterali. Ho avuto la conferma definitiva di essere una pessima giornalista, in termini di produttivita'... Mi sa che ne faro' un bel tema di dibnattito alla sessione dedicata alla comunicazione della scienza! Parallelamente agli incontri scientifici, a cui assistono soprattutto giovani ricercatori, c'e' un'area espositiva aperta alla cittadinanza e soprattutto ai bambini, una sorta di museo della scienza temporaneo, con alcune installazioni veramente carine. Nell'area dedicata al cervello c'e' un padiglione italiano, preparato dall'Universita' di Torino sotto la supervisione della professoressa Anna Berti: in varie piccole stanze si tenta di far capire al pubblico cosa sono le alterazioni della coscienza da lesione cerebrale, come il neglect, l'anosognosia, la prosopagnosia. C'e' una bella parte teorica su pannelli, alcuni video di pazienti molto ben girati e, in alcune cabine buie, quattro o cinque elaborazioni artristiche sulla patologia. Interessante e d'impatto... Spero che qualcuno lo porti tal quale anche in Italia. Sara' che Torino e' la sede del prossimo meeting ESOF nel 2010, per cui, questa volta, la rappresentanza italiana e' folta e quasi tutta piemontese. Parlando con una delle responsabili del programma scientifico di ESOF 2010 mi ha detto che nell'autunno prossimo verra' emesso una sorta di \"call for papers\" sui temi che il comitato scientifico avra' stabilito. Della possibilita' di partecipare attivamente sapro' dirvi di piu' domani dopo l'incontro previsto con il team organizzatore."},
{"title": "Piacere e felicità (ESOF 2008 - 2)", "text": "Quanta parte ha il piacere nella nostra vita e quale ruolo gioca nel nostro modo di essere? Ne hanno discusso qui  a Barcellona diversi neuroscienziati in una sessione diretta da Pierre Magistretti, direttore del Brain-Mind Institute di Losanna, in Svizzera. In realta' eravamo li' per parlare delle implicazioni etiche e pratiche dello sviluppo del neuroimaging, ma siamo tutti stati catturati dal video di un paziente affetto da dolori da arto fantasma guarito grazie all'impianto di un elettrodo intracranico. Nell'intervista filmata egli raccontava che nel momento esatto in cui il dolore ha cessato di tormentarlo, si e' sentito veramente felice. Alla domanda: \"Se fosse possibile aumentare il livello delle scariche che l'apparecchio le da' per farla felice, lei sarebbe d'accordo?\" rispondeva \"Certo che si'!\". Cosi' ci siamo trovati a parlare della differenza tra piacere, assenza di dolore e felicita' e, soprattutto, sulle implicazioni etiche di questa felicita' \"sintetica\". Magistretti ha detto chiaramente che al momento attuale siamo in grado di attivare i centri del piacere, ma che la felicita' e' altra cosa e dipende anche da fattori esterni. Cio' non toglie che possiamo gia' indurre una sorta di insensibilita' nei confronti della sofferenza e dell'ansia o addirittura riattivare i centri cerebrali che ci rendono sensibili al piacere: una scoperta molto utile per curare, per esempio, i pazienti con depressione grave affetti da anedonia refrattaria alla terapia farmacologica. Se pero' privassimo l'umanita' della ricerca del piacere, ha detto ancora Magistretti, probabilmente ne indurremmo la perdita, per la stessa ragione per cui il tossicodipendente e' un essere inadatto alla vita sociale: perche' molto di cio' che facciamo ogni giorno e' un pezzo della nostra personale ricerca della felicita', senza la quale probabilmente ce ne staremmo passivamente chiusi nel nostro piacere sintetico, evitando l'interazione con gli altri."},
{"title": "Ma che bel cervello! (ESOF 2008 - 3)", "text": "Scusate il narcisismo, ma non resisto. Qui a Barcellona, allo stand del Ministero per la ricerca svedese, c'è un bellissimo giochino per vedere il proprio cervello riflesso nello specchio. Ed ecco qui la mia materia grigia (anche, la bianca, veramente!).   E poi, per chi volesse saperne di più su quanto si è discusso in tema di cervello e neuroetica, ecco un video riassuntivo, purtroppo solo in inglese (mi scuserete, ma non ho proprio il tempo di sottotitolarlo). "},
{"title": "Coscienza globale", "text": "Oggi è Natale, e per festeggiare seguirò la tradizione del British Medical Journal, che dedica sempre il suo numero natalizio alle ricerche più assurde. Vi pare che nel mondo oggi vi sia più pace e felicità di un paio di giorni fa? Non è merito dello Spirito del Natale, ma di un orgasmo collettivo avvenuto sabato mattina scorsa esattamente alle 7.08 ora italiana. L'iniziativa, giunta al suo secondo anno di vita, si chiama Globalorgasm (ne hanno parlato anche alcuni giornali italiani come il Corriere):  un orgasmo planetario, da raggiungere tutti nello stesso momento, per favorire la pace nel mondo (e infatti al momento clou non bisogna pensare egoisticamente al proprio piacere, ma ai mali del pianeta, e se questo vi mette in difficoltà e manda all'aria la performance, affari vostri). Global Orgasm si basa, a detta degli organizzatori, sulle teorie scientifiche di un gruppo di ricercatori dell'Università di Princeton, secondo i quali avvenimenti di grande portata emotiva e di ampia partecipazione collettiva sono in grado di influenzare gli eventi, deviandoli dalla casualità. La ragione per cui vi informo solo oggi di questa ineguagliabile iniziativa, rendendo di fatto impossibile la vostra personale partecipazione al progetto   , è che ho voluto andare a vedere esattamente di cosa si occupano le grandi menti di Princeton, il che, in questi giorni di regali e pranzi da preparare è stato piuttosto complicato. La loro ricerca si chiama Global Consciousness Project (GCP) ed è diretta da Roger Nelson, professore in pensione di psicologia cognitiva e titolare, fino al 2002, di un progetto di analisi delle anomalie presso il dipartimento di Meccanica aerospaziale dell'Università di Princeton.  Dopo il pensionamento, lo psicologo si è dedicato anima e corpo al GCP, che riceve anche finanziamenti statali. In sostanza si tratta di una rete di computer che funge da generatore di eventi casuali (di fatto genera stringhe binarie). I membri del GCP affermano che, in concomitanza con eventi di portata planetaria (come il funerale di Lady Diana o di madre Teresa, sic!), i computer generano per brevi periodi stringhe significative e non randomizzate. Cosa influenza le macchine tanto da cambiare la produzione casuale dei numeri? Nelson non sa dirlo: afferma solo di avere prove \"statisticamente molto significative\" dell'esistenza di questo effetto. La \"coscienza globale\" non sarebbe una forma di energia fisicamente misurabile, dicono i membri del GCP (che chiamano in causa persino il concetto di noosfera del filosofo francese Pierre Teilhard de Chardin), nondimeno interagisce con la materia (secondo loro a livello subatomico), visto che influenza i chip dei computer. Il sito del GCP è ricco di spiegazioni, comprese le equazioni utilizzate per l'analisi statistica dei risultati, e che io sono incapace di comprendere (forse qualche matematico vuole dare un'occhiata e farmi sapere cosa ne pensa?). Tutto ciò mi ricorda altre bufale sulle \"energie\" non misurabili, come la famosa memoria dell'acqua che avrebbe dovuto spiegare il funzionamento dell'omeopatia. Inoltre, anche ammesso che la coscienza globale possa influenzare la produzione di una sfilza di 1 e di 0, non vedo il nesso diretto con l'orgasmo collettivo e la pace nel mondo (iniziativa che peraltro è indipendente dal progetto GCP). Però oggi è Natale, voglio essere buona e quindi ve la butto lì e che ognuno ne faccia l'uso che crede. Peace and love, fratelli! "},
{"title": "Marketing mentale", "text": "La notizia è ghiotta per la stampa generalista, e infatti qualche furbo ha rotto anzitempo l'embargo che la rivista PNAS aveva posto sullo studio di Antonio Rangel (un economista di Standford con il pallino della \"neuroeconomia\") e colleghi: gonfiare il prezzo di un oggetto o di un cibo ne aumenta il godimento.  In sostanza la ricerca illumina di una nuova luce (anche un tantino inquietante) le iniziative di marketing, suggerendo che il prezzo percepito agisca direttamente sul cervello della persona attivando le aree del piacere. Già si sapeva, per esempio, che le aspettative individuali influenzano la percezione del piacere o del dolore: se da una degustazione ci attendiamo un sapore piacevole, l'attesa ne potenzia l'intensità (sia in senso positivo, se il cibo è davvero buono, sia in senso negativo se ci delude). Il gruppo di neurologi che ha affiancato Rangel ha testato 20 volontari che assaggiavano vini (e visualizzavano il relativo costo) mentre si trovavano sotto la risonanza magnetica funzionale. Due dei vini, però, sono stati presentati due volte, con prezzi molto differenti, e i volontari hanno apprezzato decisamente di più il vino più caro. La fRM, nello stesso momento, registrava un aumento dell'attività nell'area della corteccia orbitofrontale che codifica le esperienze piacevoli nell'ambito degli odori, dei sapori e dell'ascolto musicale. Le aree cerebrali deputate invece alla codifica di altre sensazioni, come la dolcezza, non appaiono diversamente attive quando assaggiano lo stesso vino con relativi prezzi: in sostanza il vino \"appare\" migliore alla coscienza, ma le aree gustative non si fanno ingannare. \"Il risultato dimostra che l'aumento di piacere che sperimenta il soggetto quando ha a che fare con prodotti costosi è un effetto reale della mente ed è più complesso di quanto ipotizzato dalle teorie economiche\" ha dichiarato Rangel. Ora sarei curiosa di sapere se il fatto di acquistare un prodotto apparentemente costoso a basso prezzo, per esempio durante i saldi, attiva le stesse aree del piacere o altre (la parte \"formica\" del nostro cervello!). A guardare le file che si sono formate in questi giorni a Milano fuori dai negozi, direi che l'effetto cerebrale del saldo è molto intenso!"},
{"title": "Leggere nella mente", "text": "Segnalo a chi non lo avesse letto un bell'articolo di John-Dylan Haynes, ricercatore dell'Istituto Max Planck di Lipsia, uscito di recente su Tuttoscienze de La Stampa. Il tema è la possibilità offerta dal neuroimaging di leggere nella mente delle persone. Il gruppo di Haynes ha fatto un esperimento: ha infilato alcuni volontari  nella solita risonanza magnetica funzionale e ha chiesto loro di fare una scelta tra due decisioni (sommare o sottrarre due numeri dati) e di non comunicarla agli esaminatori, i quali l'hanno dedotta basandosi sull'attività neuronale nell'intervallo temporale tra l'assegnazione del compito e la proiezione sullo schermo interno alla risonanza dei numeri sui quali operare. Il tasso di accuratezza nell'indovinare la decisione sfiora il 70 per cento: in pratica i ricercatori sono stati capaci di \"leggere\", nella corteccia prefrontale, il pensiero del soggetto con una buona percentuale di riuscita. Lo stesso Haynes ammette che il meccanismo funziona perché le intenzioni non risiedono nei singoli neuroni, ma vengono codificate in schemi di attività distribuiti nello spazio (chiamati in termine tecnico multivariate pattern recognition, o schemi di riconoscimento multivariati). Questo complica la faccenda, perché ovviamente bisogna conoscere a priori il pattern sviluppato da ogni singolo pensiero per poter \"leggere\" la mente e, tra l'altro, ogni persona ha il suo. Infine, al momento si possono leggere solo pensieri \"in alternativa\", perché non si sa abbastanza su come vengono conservati e recuperati i pensieri complessi nell'archivio mnesico. Nonostante ciò, la possibilità di sapere cosa pensa una persona è già un fatto concreto, che non manca di sucitare perplessità di tipo etico tra gli addetti ai lavoro. Le ricadute, a mio avviso, non sono solo positive, come sembra concludere Haynes che cita, per esempio, la possibilità di governare col pensiero le protesi meccaniche, ma anche estremamente pericolose, basti pensare ai numerosi tentativi di creare delle \"macchine della verità\" per uso legale, o alle varianti moderne (e meno cruente) di \"lobotomia\"  o di controllo del pensiero.  Credo che lo sviluppo delle neuroscienze debba per forza essere accompagnato da un'attenta riflessione sugli usi di determinate scoperte (e non sull'opportunità di fare ricerca, dal momento che penso che porre limiti all'attività della scienza è sbagliato, semmai si lavora a posteriori sui risultati della stessa). Un argomento su cui discuteremo a lungo, credo."},
{"title": "Elettroshock, tra scienza ed etica", "text": " Il 21 febbraio scorso, nell'ambito della Società italiana di psicopatologia, un gruppo di psichiatri italiani, tra cui Athanasios Koukopoulos e Giovan Battista Cassano, ha fatto girare tra i colleghi una petizione, indirizzata al ministro della salute Livia Turco, per sdoganare l'elettroshock (oggi chiamato con il termine più elegante di terapia elettroconvulsivante o TEC) nei casi di depressione grave refrattaria alla terapia farmacologica. Koukopoulos, che è presidente dell'AITEC (Associazione italiana per la terapia elettroconvulsivante), ha dichiarato quanto segue all'ADN Kronos: Secondo gli psichiatri che sostengono l'utilità dell'elettroshock, la ragione per cui in Italia questa tecnica è poco usata è puramente politica: frutto di una visione \"distorta\" della malattia mentale, originata dal movimento dell'antipsichiatria che \"ha provato a sottrarre i disturbi psichici alla medicina\", dice ancora il presidente dell'AITEC. All'incontro romano era presente anche  Tom Bolwig, psichiatra all'università di Copenhaghen, per raccontare che \"in Danimarca vivono 6 milioni di persone eppure vi sono 35 centri specializzati nella TEC, addirittura uno in Groenlandia\". A parte che non vedo perché il fatto che i groenlandesi possano utilizzare in tutta comodità questa terapia (che definirei almeno controversa) debba convincere noi italiani, che abbiamo dato vita, con la legge Basaglia, a uno degli esperiementi terapeutici della malattia mentale più ammirati al mondo, a diffonderne la disponibilità, è la conclusione dell'intervista a Koukopoulos che mi ha particolarmente colpita: \"Come sempre in Italia - conclude - ci si ferma di fronte a problemi etici. Speriamo però che lo scenario cambi presto\". Ora, nel caso specifico il problemi etici non mi sembrano irrilevanti: benché la tecnica sia ovviamente cambiata nel corso degli anni e venga oggi somministrata in modo umano, sotto anestesia, con l'aiuto di miorilassanti che impediscono le fratture da contrazione muscolare involontaria e quant'altro, rimane il fatto che il suo scopo è quello di provocare una crisi epilettica e un conseguente rilascio quanto più massiccio e immediato di neurostrasmettitori nel cervello, in particolare GABA e serotonina, i cui effetti a lungo termine sono ancora molto controversi. Ma fermiamoci alla letteratura scientifica: su Clinical Evidence, il sistema di revisioni della letteratura medica a cura della British Medical Association (e una delle fonti più autorevoli in materia di indicazioni ai trattamenti), la revisione sulla TEC (pubblicata nel giugno 2007 e basata su studi disponibili nell'aprile 2006) dimostra effettivamente un miglioramento dei sintomi depressivi gravi nel breve lasso di tempo (1-6 settimane) sia rispetto al placebo sia rispetto alla terapia farmacologica, ma segnala anche la scarsa qualità degli studi disponibili, in particolare nella popolazione più anziana. Alla voce \"danni\", la revisione getta la spugna: non ci sono studi di qualità sufficiente ad escludere danni cognitivi in chi viene sottoposto all'elettroshock, anche se un'analisi condotta da Evidence Based Mental Health nel 2003 dimostra che un terzo dei pazienti ha perdite pemanenti di memoria. Conclusioni: la TEC, dice Clinical Evidence, non è una cura che viene accettata facilmente dai pazienti e dalle famiglie e non sembra avere effetti a lungo termine. Viene quindi suggerita solo in casi di depressione veramente grave, in cui tutti i possibili trattamenti abbiano fallito e in cui serva un effetto rapido, quasi salvavita. Come questo si concili con la richiesta dell'AITEC di istituire, in Italia, almeno un centro per l'elettroshock ogni milione di abitanti, continua a lasciarmi perplessa. Andiamo avanti: già nel 2003 il British Medical Journal, in un editoriale che dava conto delle prime linee guida in materia, segnalava gli stessi limiti, invitava i servizi psichiatrici britannici (tra i più attivi nel praticare la TEC in Europa) ad adeguarsi alle indicazioni e denunciava il rischio di un uso improprio. Arriviamo all'ultima revisione, comparsa nel novembre scorso sul New England Journal of Medicine (purtroppo accessibile solo su abbonamento): in sostanza si conferma l'ultilità della TEC in alcuni casi molto selezionati, e si segnala però quanto l'efficacia dipenda dalla tecnica utilizzata, con tassi di remissione dei sintomi assai variabili (da 20 all'80 per cento, in base ai diversi studi). Negli USA,  dice la rivista, il protocollo standard prevede tre sedute a settimana per 6-12 settimane. Gli effetti collaterali comprendono amnesia anterograda (di breve durata) e retrograda, talvolta con buchi di memoria del passato che si estendono a mesi o anni prima del trattamento.  L'analisi dice testualmente: \"Benché l'amnesia retrograda spesso migliori nei mesi dopo l'elettroshock, in molti pazienti il recupero è incompleto, con amnesie prolungate che riguardano eventi occorsi nel periodo del trattamento\". Tra le aree di incertezza, anche l'utilità di questa cura nel prevenire le ricadute di gravi forme depressive e, soprattutto, l'accettabilità individuale e sociale e il conseguente stigma permanente. Per questo, sottolineano gli americani, la scelta di procedere con l'elettroshock deve essere ampiamente discussa col paziente e con i familiari. Se mi sono addentrata nelle questioni più prettamente scientifiche, è perché penso che non bisogna mai avere opinioni preconcette su nulla: questo è quanto la letteratura medica può oggi fornire a sostegno di una pratica medica che ha lasciato, in passato,  segni indelebili nella vita di molte persone e che ha rappresentato incontestabilmente, insieme alla lobotomia, uno dei punti più bassi della pratica psichiatrica intesa come strumento di controllo della società sulla morale e sulla \"normalità\". Che oggi non la si esegua più nello stesso modo, e forse nemmeno con gli stessi intenti, è evidente: ma che una società scientifica si proponga di diffonderla ulteriormente, malgrado le perplessità ancora vive dal punto di vista scientifico, da quello etico e sociale - e che tutto ciò accada, guarda caso, nel trentennale della riforma Basaglia che ha ridato (laddove applicata pienamente) ai malati psichiatrici una dignità di pazienti come tutti gli altri - non mi pare un caso."},
{"title": "Neuroni specchio in 9 minuti", "text": "In un rapido giro su PsicoCafé, interessante blog italiano di psicologia e neuroscienze, ho trovato la segnalazione di questo video di 9 minuti, peraltro non nuovissimo, sui neuroni specchio. Il non lo conoscevo, ma lo trovo bellissimo e veramente chiaro. L'ideale per una pausa caffé..."},
{"title": "Dì qualcosa di sinistra (o di destra)", "text": "Chissà se Veltroni e Berlusconi parteciperanno, la prossima settimana, dall'11 al 13 marzo, presso l'Aula Magna della Facoltà di psicologia della Sapienza  di Roma a un curioso convegno che ha come tema la neuropolitica. L'iniziativa è parte del programma promosso ogni anno dalla Dana Foundation for Brain Initiative, una delle istituzioni più vitali nel finanziamento delle neuroscienze, noto come Brain Awarness Week. I relatori, come potete vedere dal sito, sono di primordine nel loro campo, ma è l'argomento a suscitare, in questi tempi elettorali, la mia curiosità. Esiste davvero un \"cervello di sinistra\" e un \"cervello di destra\"? Ovviamente non sto parlando di lateralizzazione anatomica, ma di orientamento elettorale. Sembrerebbe proprio di sì, a leggere quando scrivevano David Amodio e colleghi su Nature Neuroscience nel settembre scorso:  \"Politologi e psicologi hanno notato che in media i conservatori mostrano stili cognitivi più strutturati e persistenti, mentre i liberali rispondono di più alla complessità, all'ambiguità e alla novità dell'informazione\". Per liberali si intende ovviamente la \"sinistra\" americana (i \"liberals\"), dal momento che nella maggior parte dei casi queste ricerche vengono dal mondo anglosassone. Lo studio ha utilizzato la tecnica dei potenziali evocati per dimostrare che, in effetti, le persone più liberali mostrano una maggiore sensibilità cognitiva agli stimoli che chiedono di alterare lo schema abituale di risposta. Come a dire, hanno il cervello più \"flessibile\"...  Un altro studio divertente (che forse spiega perché da noi alla fine andiamo a votare con liste dalle preferenze blindate   ), segnalato anch'esso tra i materiali di background nel sito del convegno romano, riguarda il giudizio sugli uomini politici. Usando la risonanza magnetica funzionale e mostrando ai soggetti foto e nomi di politici americani molto noti, sia liberali sia conservatori, i ricercatori di Bethesda hanno scoperto che si attivano prima le aree emozionali, ma poco dopo anche quelle della corteccia prefrontale. Ciò suggerisce, quindi, che dopo una iniziale \"simpatia di pelle\" subentra un giudizio più ponderato, basato su valutazioni razionali. Un giusto equilibrio, quindi, tra sentimenti \"a priori\", per esempio dovuti alla vicinanza ideologica con un determinato schieramento, e considerazioni sull'operato o sul profilo della singola persona. Mi chiedo, però, quanto questi dati siano generalizzabili: in Italia, per esempio, è sempre vero che l'elettore dello schieramento più progressista è mentalmente più portato alla flessibilità dell'elettore conservatore? E cosa è progressista e cosa conservatore (dal punto di vista strettamente neuropsicologico, per carità)? Chi vota la Binetti, per esempio, è mentalmente più flessibile di chi vota la Santanché? E quanto queste considerazioni sono utili, per esempio, a stabilire il potenziale bacino di voto di un partito? Infine, dal momento che gli elettori progressisti sono anche quelli più aperti ai cambiamenti, teoricamente, mi vien da dire, dovrebbero essere meno \"fedeli\", ma se ricordo bene le analisi degli esperti dopo le ultime elezioni, in genere accade il contrario. Infine, quale sarà mai il pattern cognitivo dei centristi? Io, purtroppo, non potrò partecipare al convegno, perché in quei giorni non sono in Italia, ma se qualcuno dei bloggers ci va e ci racconta, potrebbe essere interessante. Visto che negli ultimi tempi i sondaggi e gli exit poll fanno cilecca, si sa mai che a qualcuno venga in mente di mettere, all'uscita dei seggi, una bella risonanza invece del metal detector!"},
{"title": "Chi chiede di morire è per forza depresso?", "text": "La morte per eutanasia, a poche ore di distanza, dello scrittore fiammingo Hugo Claus e della signora francese Chantal Sebire ha riacceso, sui giornali italiani, il dibattito su questo tema. Non sono mancati, ovviamente, i commenti degli \"esperti\", dagli oncologi ai preti. Tra i temi ricorrenti, quello per cui il suicidio assistito o il suicidio volontario di un malato terminale o incurabile sono una manifestazione clinica di depressione, per cui basterebbe curare questa per evitare il problema. La questione mi sta particolarmente a cuore, non solo perché sono da sempre una sostentrice della libertà di scelta individuale, ma anche per ragioni personali che potete leggere in questo post sul blog di mio marito. E così sono andata a rivedere la letteratura scientifica in materia, che ha cercato di rispondere alla domanda, apparentemente banale ma tutt'altro che semplice: chi vuole morire è per forza di cose depresso? Si può desiderare l'autoannientamento, contraddicendo così una delle leggi fondamentali della biologia, essendo completamente sani di mente e presenti a se stessi? Ci pensavo ancora recentemente ricordando il meraviglioso film di Denys Arcand \"Le invasioni barbariche\", in cui un professore universitario malato di cancro mette fine ai suoi giorni in serenità, circondato dall'affetto del figlio e degli amici: certamente non una rappresentazione della depressione. Per contrasto, un altro film, \"Lo scafandro e la farfalla\" di Julian Schnabel, che racconta la tremenda vicenda del giornalista Jean-Dominique Bauby affetto da sindrome locked-in, mi ha fatto pensare che non sempre chi si trova in situazioni apparentemente senza speranza e intollerabili attende la morte come liberazione. Che cosa succede nella psiche di chi chiede di essere aiutato a morire? Saperlo è importante anche per gli aspetti legali ed etici del dibattito sull'eutanasia e sul suicidio assistito.  Che la depressione possa giocare un ruolo, pare incontestabile: in uno studio uscito su JAMA nel 2000 su quasi mille pazienti terminali o gravemente malati, risulta che oltre il 60 per cento è sostenitore dell'eutanasia, ma solo l'11 per cento circa l'ha seriamente presa in considerazione per se stesso. La presenza di sintomi depressivi aumenta la richiesta di aiuto a morire, e anche la possibilità di cambiare idea nel tempo. Chi non è depresso ma vuole morire, generalmente non torna indietro sulla propria decisione. Lo conferma anche uno studio prospettico di coorte uscito nel 2005 sul Journal of Clinical Oncology e condotto in Olanda su malati di cancro, uno dei pochi Paesi in cui sono consentiti sia l'eutanasia sia il suicidio assistito; studio che insiste, tra l'altro, sulla necessità di curare la depressione nei pazienti terminali come parte integrante delle terapie palliative, dal momento che risulterebbe depresso il 44 per cento di coloro che chiedono di morire. Anche studi effettuati in Oregon su malati di sclerosi laterale amiotrofica o SLA (Ganzini L et al. NEJM 1998; 339: 967-973) e in Australia (Kissane DW et al. Lancet 1998; 352: 1097-1102), ambedue Paesi dove l'eutanasia è o era legale, forniscono dati analoghi, seppure in percentuali minori. Tutto chiaro, allora? Non proprio, perché uno studio uscito su Neurology nel 2005 mi ha fatto molto pensare: una ricerca retrospettiva su malati di SLA ha dimostrato che il 20 per cento circa aveva chiesto di morire e che in quasi tutti i casi risultavano sintomi di depressione alle scale di valutazione usate per diagnosticare questa malattia. Se però si escludono dalle scale stesse le domande che riguardano direttamente la cosiddetta \"ideazione suicidaria\" (cioè se si è mai pensato al suicidio, se si desidera morire), i pazienti che chiedono l'eutanasia non risultano più depressi degli altri. In pratica, siamo in pieno nel classico gatto che si morde la coda: voglio morire perché sono depresso (oltre che inguaribilmente malato), ma sono anche depresso perché voglio morire, e questo è considerato, nelle scale di misurazione della depressione, un sintomo inequivocabile di questa patologia psichiatrica. Anche familiari e infermieri di pazienti deceduti a seguito di suicidio assistito in Oregon affermano che la depressione non è la molla principale, bensì il desiderio di controllare i tempi e le modalità della propria fine, per andarsene con dignità e ancora in possesso delle proprie facoltà mentali, come è accaduto a Claus. I problema, adesso, è degli psichiatri e degli psicologi, chiamati a valutare la depressione nei pazienti terminali con strumenti diversi da quelli utilizzati normalmente: un compito arduo, tanto che la depressione appare la prima causa di rifiuto del suicidio assistito in Olanda, dove è legale solo se approvato da un'apposita commissione medica. E anche gli psichiatri dell'Oregon, interrogati sull'American Journal of Psychiatry nel 1996 , due anni dopo la legalizzazione del suicidio assistito, si dicevano spaventati dalla sfida e incompetenti a valutare. Per quel che mi riguarda, penso che casi come quelli di Piergiorgio Welby o di Giovanni Nuvoli (così come quello di Chantal Sebire), in cui la volontà di gestire la propria morte viene espressa con tanta determinazione e perseguita per un periodo così lungo e senza tentennamenti, costituiscano una sorta di \"diagnosi per manifesta evidenza\" dell'assenza di depressione. È una sensazione e non una valutazione scientifica. Ma gli strumenti che usano gli psichiatri sono, in questo caso specifico, sufficientemente \"scientifici\"?"},
{"title": "L'inganno di San Tommaso", "text": "Come San Tommaso, se lo vedo, ci credo. È questa, in sintesi, la morale che si trae da uno studio curioso uscito sull'ultimo numero di Cognition e inventato da due psicologi statunitensi. In sostanza hanno mostrato ai loro studenti di psicologia i risultati di alcune ricerche fittizie sul cervello, raccontate in modo giornalistico e corredate o meno dalla classica immagine ottenibile con la risonanza magnetica funzionale, cioè una foto simile a questa:  Le ricerche erano in effetti un po' assurde: in un caso si sosteneva che, poiché vedere la TV e studiare la matematica attivano le stesse aree cerebrali a livello temporale, la visione di programmi televisivi poteva aiutare ad acquisire confidenza con i numeri. In alcuni casi all'immagine del cervello veniva sostituita la cosiddetta mappa topografica dell'attivazione cerebrale, ovvero un'immagine simile a questa, decisamente meno realistica:  Gli studenti dovevano quindi dare un voto alla scientificità della scoperta su una scala da 1 a 4: le sintesi corredate dell'immagine realistica hanno ottenuto un punteggio più elevato, benché i testi presentassero evidenti lacune in termini concettuali e non riportassero dati numerici. Gli autori dello studio affermano che il fascino esercitato sul grande pubblico (ma anche sugli addetti ai lavori, visto che le cavie erano studenti di psicologia) dalle immagini del cervello ottenute con la fRM costituiscono la base di una sorta di \"neuro-realismo\": un giudizio acritico di realtà legato al semplice fatto dell'esistenza di un'immagine intuitivamente riconoscibile del fenomeno. Gli stessi neuroscienziati si chiedono se ciò sia un bene, perché questo induce una visione riduzionistica del funzionamento del cervello, una sorta di identificazione totale della mente con l'organo e dei processi cognitivi con i fenomeni chimico-fisici che avvengono all'interno delle cellule cerebrali e che sono \"fotografate\" dal neuroimaging. A riprova di ciò, i testi accompagnati dalla semplice immagine topografica, che non è inserita nei contorni ricnoscibili di un cervello umano, non trasmettono lo stesso senso di scientificità. Non solo: il voto di qualità sull'articolo (in pratica il giudizio sulle capacità di comunicazione del giornalista) è più elevato se c'è un bel cervellino acceso, anche se il testo è assolutamente identico, e questo dovrebbe far meditare a lungo noi giornalisti scientifici sul ruolo che gioca l'iconografia nella comprensione di ciò che scriviamo. Sempre secondo gli autori, la possibilità offerta dalle tecniche moderne di neuroimaging di vedere un sistema al lavoro, tipica delle scienze \"esatte\" come la fisica e la chimica, è alla base del crescente interesse del grande pubblico per le scienze cognitive. Non solo: ha dato anche una sorta di aura di scientificità alla psicologia, considerata fino a poco tempo fa più una sorta di \"filosofia\" che di scienza. \"La scoperta che le immagini del cervello in azione influenzano la credibilità delle scienze cognitive ha anche delle implicazioni etiche\" dicono nello studio. \"Alcuni sostengono che i neuroscienziati dovrebbero diffondere di più le loro scoperte, altri, invece, sono frustrati dalla eccessiva semplificazione dei dati effettuata dai media a causa della presenza di immagini cerebrali\". In pratica, è colpa della foto se poi il pubblico crede che vi sia un'area dell'amore, una dell'odio, una della sessualità e persino una per la percezione del divino. Il cervello subisce lo stesso processo di \"sezionamento\" e di semplificazione della relazione di causa-effetto avvenuto nell'ambito della genetica a causa dei titoloni giornalistici stile \"Scoperto il gene della felicità\". Gli autori sono pur sempre americani, e quindi concludono: \"Poiché la percezione della scienza da parte del pubblico può giocare un ruolo importante nelle decisioni di finanziamento e nella direzione da imprimere alla ricerca scientifica, la fascinazione per il cervello può avere un impatto positivo sull'opinione che il pubblico ha delle scienze cognitive\". Come a dire: un cervellino acceso in più farà anche qualche danno epistemologico, ma fa anche mettere mano al portafoglio."},
{"title": "Il libro che mi ha cambiato la vita", "text": "Sull'ultimo numero di New Scientist diversi scienziati raccontano qual è il libro che ha cambiato loro la vita.  Si scopre così che se per Oliver Sacks è stato il molto prevedibile \"L'uomo che non dimenticava nulla\" del neuropsicologo russo Alexander Luria, per Vilayanur Ramachandran è stato \"The art of soluble\" del Premio Nobel per la fisiologia Peter Medawar. \"Ho capito che la scienza era una grande, romantica avventura\" dice il neuroscienziato indiano. I libri possono davvero cambiare la vita, nelle grandi e nelle piccole cose: la primatologa Jane Goodall, per esempio, ha scelto \"Liberazione animale\" del filosofo Peter Singer, uno dei pionieri dei diritti degli animali, perché \"appena lo lessi divenni istantaneamente vegetariana\". Il fisico teorico e divulgatore Michio Kaku ha tutta la mia simpatia, avendo scelto \"La trilogia della Fondazione\" di Asimov, mentre il suo collega Sean Carroll preferisce \"Uno, due, tre...infinito\", un superclassico (è del 1947) del fisico russo George Gamow. \"Sono una di quelle persone fortunate che sapeva già a 10 anni che cosa voleva fare nella vita\" dice Carroll. \"Cosa ne sapeva un bambino di fisica o cosmologia? Sapeva, perché aveva letto il libro giusto\". C'è chi su un libro letto da bambina ha fondato tutta la propria vita di scienziata. È  la psicologa dello sviluppo Alison Gopnik, la cui teoria dell'apprendimento infantile deriva dal suo amore per \"Alice nel paese delle meraviglie\": nell'intervista sostiene di aver abbandonato una carriera di filosofa chomskiana per studiare le basi dell'apprendimento \"perché ero colpita dalla cecità degli adulti, specialmente dalla loro incapacità di riconoscere che i bambini sono molto più intelligenti di loro\". Il gioco, da vera bulimica della lettura quale sono, mi piace molto: qualche giorno fa, parlando di neuroni specchio in una trasmissione scientifica della TV svizzera mi è venuto spontaneo ricordare che l'uomo è l'unico essere vivente in grado di provare emozioni non solo vedendo un suo simile che le prova, ma anche leggendo, cioè sentendo raccontare (o solo evocare) le emozioni con le parole. Allora ve lo rilancio e comicio io stessa. Il libro che mi ha cambiato la vita è stato \"La peste\" di Albert Camus. Dovevo avere 14 o 15 anni, quando lo lessi, e anch'io, fin da piccola, sapevo cosa volevo fare: \"il dottore\". Ma questo libro mi ha illuminata: c'era dentro tutto, la vita, la morte, la lotta contro la malattia, la paura, la caducità dell'uomo, l'altruismo e l'egoismo. Per anni il dottor Rieux è stato il mio personale riferimento etico nel mondo della medicina. Da adulta ho capito altre cose di questo grande libro, e in particolare la sua potenza metaforica, ma da adolescente, pur restando in superficie, mi ha davvero cambiato la vita, dando una consistenza emotiva al mio amore per la medicina (e forse anche per il genere umano)."},
{"title": "Basaglia e la porta chiusa a chiave", "text": "Ieri, 13 maggio, era un anniversario speciale: quello dei 30 anni della legge Basaglia e della formale chiusura dei manicomi in Italia (quella reale è avvenuta a spizzichi e bocconi nei 15 anni successivi). Per uno di quei casi del destino, ieri sera ero in un reparto psichiatrico di un grande ospedale milanese per andare a trovare un amico. Per la prima volta ho varcato la soglia chiusa a chiave di questo \"mondo a parte\" senza la protezione del camice, ma nella veste di comune mortale, di persona emotivamente coinvolta che si guarda intorno e che percepisce il dolore e la fatica che accompagna la malattia mentale, anche sul volto di una persona cara. Di fronte a noi che chiacchieravamo c'era un giovane che fumava, con l'occhio appannato dai farmaci, la voce impastata e la madre vicino: una madre dallo sguardo stanco, che si è scusata perché il figlio ci ha attaccato bottone, stringendoci la mano e presentandosi. Mi è venuta in mente la lettera della madre di uno schizofrenico pubblicata qualche giorno fa nella rubrica di Concita De Gregorio su D di Repubblica. Il testo è disponibile on line solo in un formato non linkabile, per cui ve lo allego come immagine in fondo a questo post. L'autrice ha fondato un'associazione di familiari che, contrariamente ad altre, si batte per una revisione della legge 180 e per rendere possibile la cura coatta del paziente recalcitrante. Il succo è: la libertà del malato psichiatrico è diventata la prigione della sua famiglia. È una lettera molto sentita e sofferta, che non mi permetterei mai di contestare. Ma ieri, in quel reparto chiuso a chiave - dove per uscire bisogna chiedere all'infermiere di venire ad aprire la porta - consapevole del fatto che per la psichiatria italiana era comunque una giornata \"speciale\", non ho potuto fare a meno di pensare a Basaglia e alla profonda umanità del suo pensiero: anche quei malati lì intorno -  il ragazzo con la mutanda sporca di indicibili macchie, la donna logorroica che non smetteva di parlarci, la ragazza dallo sguardo catatonico seduta a cinque centimetri dalla televisione accesa con il volume al massimo - sono persone, sono pazienti che vanno rispettati in quanto tali e che hanno gli stessi diritti di tutti gli altri . Vanno rispettati soprattutto nei loro desideri e nelle loro aspirazioni e aiutati a ritornare alla vita civile, in mezzo a noi. Un bel pensiero, che lì dentro, però, si è manifestato in tutta la sua difficoltà e anche, per certi versi, nella sua carica utopica. Anche loro hanno diritto di dire ciò che pensano della loro malattia, delle cure che vengono loro somministrate e del destino che viene loro riservato. In quel reparto eravamo chiusi tutti insieme, malati e sani, e la chiave l'avevano i medici e gli infermieri. Mi pareva una metafora molto concreta di quanto diceva Basaglia, e anche, in un certo senso, di quanto racconta la lettera su D. E ho pensato anche che è dovere della società civile (ovvero di ciascuno per il proprio ruolo) far sì che i malati di mente possano essere liberi senza che ciò significhi la prigione dei loro familiari. "},
{"title": "Depressi e improduttivi", "text": "Esiste un filone molto agguerrito della ricerca psichiatrica che mi ha sempre lasciata piuttosto perplessa: quello che analizza le ragioni per cui non riusciamo a essere tutti sempre felici, attenti e, possibilmente, produttivi per la società. L'ultimo studio che mi è capitato tra le mani è uscito la settimana scorsa su JAMA, quindi su una rivista medica tutt'altro che marginale. Un americano su sei, dicono gli esperti, soffre di depressione almeno una volta nella sua esistenza e spesso ciò accade durante la vita lavorativa, con una perdita di produttività di qualche miliardino di dollari l'anno. \"Le grandi compagnie ne ricavano un danno importante e potrebbero avere interesse a migliorare la salute mentale dei loro impiegati\" afferma lo studio. Ecco quindi che qualcuno ha fatto la prova: i lavoratori di 16 \"big companies\" statunitensi sono stati sottoposti a screening per la depressione. Metà di quelli risultati depressi sono stati trattati con farmaci o psicoterapie via telefono (sic!). Risultato: i soggetti trattati hanno lavorato ben DUE ORE IN PIU' A SETTIMANA rispetto a quelli non trattati. E gli scienziati hanno fatto un po' di conti: l'intervento ha fatto guadagnare alla compagnia, per ogni dipendente, circa 1.800 dollari l'anno ed è costato una miseria, ovvero dai 100 ai 400 dollari l'anno. Ora, senza voler assolutamente negare l'importanza di una patologia come la depressione, che merita attenzione e assistenza, mi chiedo: possibile che un disturbo che colpisce una persona su sei nel corso della vita debba per forza essere considerata una patologia e non una \"variante della norma\"? Su questo tema c'è peraltro fior di letteratura scientifica, perché niente è più difficile da distinguere della malattia psichiatrica dal disagio psicologico che fa parte della natura umana: tra i due non c'è soluzione di continuità e il confine lo mette la medicina. Quando poi l'obiettivo della terapia (o almeno la misura della sua efficacia) è far guadagnare due ore a settimana di produttività (e soprattutto quando questa diventa il parametro della felicità), allora mi viene in mente la celebre pagina del Piccolo Principe, quando lo scienziato gli prospetta l'idea di una pillola contro la sete che fa risparmiare quasi un'ora di tempo e lui risponde: \"Se avessi 53 minuti da spendere, camminerei adagio adagio verso una fontana...\"."},
{"title": "Io penso positivo...", "text": "Sono un po' di corsa per via dei vari impegni di lavoro, ma non posso esimermi dal segnalarvi una ricerca uscita sul numero odierno di Nature e ripresa anche da alcuni quotidiani italiani, tra cui Repubblica. Un gruppo di neuroscienziati dell'Università di New York ha scoperto che siamo progettati per essere ottimisti. Infatti tutti si aspettano di vivere una vita mediamente serena, di non divorziare e di avere successo sul lavoro. Nessuno pensa in partenza di essere destinato a litigare con la moglie o a finire sotto i ponti. Meno male, altrimenti il genere umano non avrebbe la forza di andare avanti: anche questo è uno dei frutti della selezione darwiniana. Dal momento che, a quanto pare, siamo l'unica specie animale capace di immaginare un futuro (gli esperimenti dicono che persino i primati hanno una limitatissima capacità di proiettarsi avanti nel tempo), questo deve per forza essere roseo.  La parte più interessante dello studio, però, è quella di neuroimaging: se si chiede a qualcuno di immaginare qualcosa di bello per il proprio futuro, il cervello attiva l'amigdala e la corteccia cingolata anteriore. Queste due aree sono le stesse che sembrano compromesse nella depressione e, nel caso dell'amigdala, che sono responsabili dei sentimenti di ansia e paura (l'opposto dell'ottimismo, quindi). Sono anche alcune delle aree cerebrali filogeneticamente più vecchie, cioè quelle che gestiscono i nostri \"istinti primordiali\". Tutto ciò mi pare estremamente affascinante: siamo forse emersi dalla massa del mondo animale perché pensiamo postivo? E i pessimisti cosa sono: un hardware fallato? Mi perdoni Jovanotti se mi permetto di storpiarlo: \"io sono vivo perché penso positivo\"!"},
{"title": "Il mito di Orfeo", "text": "C'è un libro uscito negli Stati Uniti circa 15 giorni fa e che non vedo l'ora di avere tra le mani (Poste Italiane permettendo...). È l'ultima fatica di Oliver Sacks e si intitola \"Musicophilia - Tales of music and the brain\".  Lo so, tanti puristi delle neuroscienze storcono il naso quando si parla di Sacks, perché viene accusato di \"romanzare\" gli studi sul cervello, contribuendo, seppure involontariamente, al rafforzamento nei profani della teoria localista (ovvero che ogni area del cervello si occupi di una precisa funzione e solo di quella) e che di fatto è già stata smentita dalla scienza. Io però non dimentico quanto abbia contribuito, con la sua prosa elegante e il suo vero amore per i pazienti, a far capire che, nel corpo umano, il cervello è di gran lunga l'organo più affascinante (le battute salaci sono proscritte da questo blog). In questo caso si uniscono nello stesso libro due miei grandi amori: la neurologia e la musica. Sacks è un noto melomane e approfitta di alcuni aneddoti succosi per fare il punto su quanto sappiamo della relazione tra musica e cervello umano. C'è già un illustre precedente, seppure un po' vecchiotto: nel 1986 Macdonald Critchley e R.A. Henson pubblicarono \"La musica e il cervello - Studi sulla neurologia della musica\", edito in Italia da Piccin, una sorta di manuale su quanto si sapeva vent'anni fa su come il nostro cervello percepiva lo stimolo musicale. La cosa che trovo più affascinante in questo filone di studi è la scoperta che la musica viene percepita solo in parte come qualsiasi altro suono, dal momento che attiva anche aree deputate alla percezione spaziale, aree emotive, aree della programmazione motoria (il ritmo che non riesce a farvi restare fermi sulla sedia...). Ogni componente della musica (altezza del suono, ritmo, sovrapposizione di più linee melodiche) può essere percettivamente scisso dagli altri, come dimostrano alcuni pazienti con lesioni cerebrali: Sacks, per esempio, racconta il caso di un paziente che, dopo un ictus, perde la facoltà di sentire il suono di un quartetto d'archi come un tutt'uno, ma percepisce le singole parti \"come fasci di luce laser tra loro indipendenti\". Non solo: la musica fa parte di alcune forme fisiologiche e patologiche di sinestesia, può comparire \"a tradimento\" nelle epilessie musicogene, nelle allucinazioni uditive... Il cervello umano sembra possedere, secondo alcuni studi, un sistema di percezione \"tarato\" sulla musica tonale: quanto questo sia una dato attendibile è molto controverso, dato che la musica tonale è un'elaborazione relativamente recente e prevalentemente occidentale. Però è incontestabile che la memoria musicale, così come viene misurata nei test, si basa su una gerarchia di probabilità transizionali della scala musicale, ovvero sulla probabilità che a un tipo di accordo (per esempio un accordo di tonica) ne segua un altro con una certa frequenza (per esempio un accordo di sottodominante o di dominante). Ovviamente se cambiano le probabilità transizionali cambia quello che in musica viene chiamato \"modo musicale\" e si può dire che ogni cultura musicale ha i suoi modi, in senso proprio e figurato! Un bell'esempio di \"relativismo cerebrale\", che meriterebbe maggiore attenzione da parte degli studiosi, se non altro per evitare di estendere a tutto il genere umano scoperte sul funzionamento cerebrale che sono proprie dei nostri cervellini \"occidentali\". Non vi dico quante discussioni con amici musicisti su questo tema, dal momento che se fosse davvero così potremmo dire che la musica completamente atonale (come una parte di quella contemporanea) non è musica, almeno per il nostro cervello, dal momento che attiva altre aree cerebrali (come quelle che usiamo per capire la matematica) rispetto, per esempio, alla mia amatissima musica barocca... Alcuni esperimenti hanno per esempio dimostrato che il tempo di apprendimento di melodie che appartengono al sistema tonale nel quale si è cresciuti è molto più rapido del tempo di apprendimento di una sequenza di note che non sono ordinate in modo da stabilire una tonalità, così come è più facile ricordare una parola che un insieme di sillabe senza senso.Siamo anche dotati di una sorprendente memoria musicale: se qualcuno si mette a cantare una melodia di qualsiasi genere, anche se non ha alcuna preparazione musicale utilizzerà probabilmente una tonalità molto vicina a quella originale (magari riportandola nell'ottava più consona alla sua capacità vocale), così come simile sarà il ritmo. Questo dimostrerebbe scientificamente quanto afferma da anni il direttore del mio coro, ovvero stonati non si nasce ma lo si diventa per \"incuria\". La musica aiuta la funzione motoria, come ben sanno alcuni fisioterapisti che la utilizzano per dare il giusto \"ritmo\" al movimento dei Parkinsoniani. Vedere, come mi è capitato, un gruppo di persone devastate dal tremore e impossibilitate a muoversi a causa della parcellizzazione del movimento riacquistare fluidità e stabilità grazie a una sinfonia di Beethoven è un'esperienza incredibile. Il tema si presta ovviamente a infinite discussioni, considerando che finora ho tralasciato l'aspetto emotivo della musica, nonché quello del suo ruolo nella vita dell'uomo (anche Sacks si chiede perché l'evoluzione abbia mantenuto e selezionato una capacità così complessa e apparentemente inutile come la percezione musicale). Se andate su Amazon alla pagina riservata a Musicophilia potete anche vedere quattro video in cui Sacks racconta alcuni dei casi raccolti nel libro, tra cui un incredibile caso di amnesia globale in un cantante e direttore di coro che, dopo un'encefalite erpetica, perde qualsiasi ricordo di sé e della sua vita, ma non la capacità di cantare tutte le arie da lui conosciute e di dirigere il proprio gruppo. Se poi la questione della relazione tra la musica e il cervello vi interessa particolarmente, vi consiglio anche di leggere un altro volume appena uscito, \"This is your brain on music - The science of a human obsession\" di Daniel J. Levitin, docente di psicologia della comunicazione elettronica  e direttore del Laboratorio di percezione musicale della McGill University. Un testo più tecnico, ma apparentemente molto interessante. Saprò dirvene di più appena le solite Poste Italiane avranno depositato sulla mia scrivania il pacco in arrivo dagli States..."},
{"title": "Fuori dal corpo con un elettrodo", "text": "Dopo aver assistito silente alla deriva \"informatica\" del mio post musicale (discussione molto interessante, ma ammetto che ad un certo punto mi sono persa...), stasera mi è finito sotto gli occhi uno studio piuttosto curioso pubblicato sul New England Journal of Medicine. Un gruppo di medici è riuscito a \"fotografare\" la sorgente cerebrale della sensazione di extracorporeità, quella che viene chiamata tecnicamente con la sigla OBE, ovvero \"out of body experience\". Tutti sanno che alcune persone che si sono trovate in punto di morte riferiscono di essere letteralmente uscite dal proprio corpo per osservare la scena dall'alto, di solito a una cinquantina di centimetri sopra il letto. A questo tipo di sensazione, riferita come molto realistica, si associano spesso percezioni di estremo benessere, desideri di perdersi nell'incoscienza, grandi luci tranquillizzanti e così via. Intorno a questo fenomeno è fiorita una quantità di letteratura folcloristica nonché una discreta mitologia di stampo New age (ho fatto una rapida ricerca con \"out of body experience\" su Google e mi sono persa tra 'manifestazioni dell'energia astrale', 'massimo livello dell'esperienza trascendentale' ecc ecc). Ebbene, la realtà è molto più prosaica. Per liberare l'anima dal corpo basta l'attivazione anomala di due aree dell'emisfero destro: la giunzione tra il giro angolare e il giro sovramarginale e la corteccia temporale superiore. La prima area ha probabilmente il ruolo di integrare i segnali sensitivi per facilitare il corretto orientamento del capo e del corpo nello spazio. La corteccia temporale è invece coinvolta nella percezione del sé. Di fatto si sapeva già che la giunzione temporoparietale era un'area critica per questo tipo di sensazione, che può comparire anche in alcune forme di epilessia e di emicrania con aura. Il colpaccio, dal punto di vista scientifico, è stato trovare un paziente che \"usciva dal proprio corpo\" più volte al giorno per via di un elettrodo extradurale posizionato più o meno nell'area incriminata per curare una grave forma di vertigine. È bastato infilare questa persona al momento giusto sotto una risonanza, et voilà: per quanto ci è dato sapere, non è l'avvicinarsi di Signora Morte a farci volare lievi, a liberarci dal dolore, a illuminarci d'immenso, ma una semplice, banale, alterazione della circolazione cerebrale che manda in tilt la nostra giunzione temporoparietale. "},
{"title": "Umorismo in rete", "text": "Su uno degli ultimi numeri di New Scientist, Mark Buchanan racconta le teorie del fisico russo Igor Suslov, dell'Istituto Kapitza per la fisica di Mosca, che sta cercando di disegnare un modello computazionale del senso dell'umorismo. Pare che, da studente, costui si dilettasse di teatro ma non trovasse mai il tempo di scrivere le battute, così si è messo a studiare un sistema per farle scrivere al suo computer. La premessa è che generalmente la psicologia ritiene che l'umorismo sia appannaggio della mente umana, perché è una diretta conseguenza della cosiddetta \"teoria della mente\" (ovvero la capacità di riflettersi nel pensiero dell'altro). Infatti in genere le persone affette da autismo o da altre malattie psichiche con precise basi biologiche hanno difficoltà a comprendere le battute o ad apprezzare, per esempio, i giochi di parole, perché rimangono \"agganciate\" all'interpretazione più immediata. Tornando all'intelligenza artificiale, si è sempre pensato che i computer sono poco spiritosi e che probabilmente tali resteranno (se ricordo bene anche i robot nel ciclo di Asimov non erano capaci di fare e capire le battute). Suslov sostiene che la nostra abilità nel produrre humor dipende dall'imprevedibilità: in pratica fa ridere ciò che il nostro cervello percepisce come imprevedibile (per esempio, una parola con un doppio senso, che inizialmente viene compresa nella sua accezzione più comune poi, una frazione di secondo dopo, nel suo significato umoristico). Partendo dall'esempio del gioco di parole, che di fatto è un \"inganno\" per la mente, Suslov sostiene che alla radice dello spirito umoristico c'è la difficoltà, per il cervello umano, di interpretare i dati in entrata. Per fare ciò il cervello deve collegare i dati in ingresso con un pattern interpretativo congruente con l'esperienza, un processo che è per lo più inconscio. Solo quando la mente si ferma su un'interpretazione plausibile (in genere la più comune, perché così si guadagna in rapidità) questa diventa cosciente. Il cervello continua a lavorare in background (un po' come lo streaming di un video dalla rete: si comincia a guardarlo mentre il computer macina...) in modo da essere pronto a correggere il tiro. Non c'è modo di evitare totalmente gli errori interpretativi, connaturati alla natura algoritmica del processo. In questo quadro, la risata sarebbe il sistema emotivo selezionato dal cervello per attivare il meccanismo di correzione e fare arrivare alla coscienza la risposta giusta (e quindi sarebbe un elemento evolutivamente utile). Questo spiegherebbe anche perché il ritmo è un elemento imprescindibile della comicità (nulla è più penoso di chi non sa fare battute perché è così lento che capiamo in anticipo dove vuole arrivare, e così si ammazza l'effetto sorpresa).  Il fisico russo non ha ancora prodotto un computer capace di ridere (benché io sia convinta che il mio Apple, la cui natura superiore è ben nota, si sia divertito più volte alle mie spalle) ma un modello computazionale che mima il processo descritto (per gli addetti ai lavori, ecco il  link su Arxiv). Il prossimo passo è quello di costruire una rete neurale con un sistema sesoriale che capti gli stimoli dall'esterno e li invii al sistema di memoria in grado di riconoscerne lo schema. Quando ciò accade, la rete invia il risultato a un terzo network, che rappresenta la \"coscienza\" del computer, a sua volta collegato a un sottonetwork di tipo motorio, che mima la corteccia motoria, espellendo fisicamente dalla rete la risposta sbagliata per lasciare posto a quella giusta e che accompagna il tutto con un rumore buffo (per la descrizione della rete andate sempre su Arxiv). Che dire? Sono da sempre affascinata dalle reti neurali, fin da quando, negli anni Ottanta, ne ho vista una con 40 nodi in un laboratorio di neuroscienze dell'Università di Padova che era in grado di discriminare il colore bianco dal nero. L'idea di spiegare il funzionamento del nostro cervello con la matematica è fantastica, anche perché spesso questi modelli computazionali arrivano vicini vicini alla natura umana, ma non riescono a mimarla del tutto. Sarà perché siamo molto più imprevedibili di quanto riesca ad essere una teoria computazionale..."}
]